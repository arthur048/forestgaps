# Quick Start - Benchmarking ForestGaps

Guide de dÃ©marrage rapide pour lancer ton premier benchmark.

## âœ… PrÃ©-requis

1. **Docker lancÃ©** avec TensorBoard :
```bash
cd docker/
docker-compose up -d tensorboard
```

2. **DonnÃ©es prÃ©sentes** :
```bash
# VÃ©rifier les donnÃ©es d'entraÃ®nement
ls -lh data/*.tif | wc -l  # Devrait afficher ~16 fichiers

# VÃ©rifier les donnÃ©es externes
ls -lh data/data_external_test/*.tif
```

3. **AccÃ¨s TensorBoard** : http://localhost:6006

## ğŸš€ Lancer ton premier benchmark

### Option 1 : Test rapide (5-10 minutes)

```bash
# Entrer dans le container
cd docker/
docker-compose run --rm forestgaps bash

# Dans le container
python scripts/benchmark_quick_test.py \
  --experiment-name "mon_premier_test" \
  --epochs 5 \
  --models "unet,unet_film"
```

**Ce que Ã§a fait :**
- Compare 2 modÃ¨les (U-Net et U-Net FiLM)
- 5 Ã©poques seulement
- 20 tuiles d'entraÃ®nement
- Seuils : 5m et 10m
- **DurÃ©e : ~5-10 minutes**

### Option 2 : Benchmark complet (plusieurs heures)

```bash
# Dans le container
python scripts/benchmark_full.py \
  --experiment-name "comparison_all_models" \
  --epochs 50 \
  --models "unet,unet_film,deeplabv3_plus,deeplabv3_plus_threshold" \
  --thresholds "2.0,5.0,10.0,15.0"
```

**Ce que Ã§a fait :**
- Compare 4 modÃ¨les
- 50 Ã©poques
- Toutes les donnÃ©es
- 4 seuils de hauteur
- **DurÃ©e : ~4-8 heures** (selon GPU)

## ğŸ“Š Suivre l'entraÃ®nement

### TensorBoard (temps rÃ©el)
```
1. Ouvrir http://localhost:6006
2. SÃ©lectionner l'expÃ©rience en cours
3. Voir les mÃ©triques en direct
```

### Logs Docker
```bash
# Logs en temps rÃ©el
docker-compose logs -f forestgaps

# Chercher les erreurs
docker-compose logs forestgaps | grep -i error
```

## ğŸ“ Trouver les rÃ©sultats

AprÃ¨s le benchmark, tout est organisÃ© dans :

```
outputs/benchmarks/YYYYMMDD_HHMMSS_<experiment_name>/
â”œâ”€â”€ benchmark_results.json          # RÃ©sultats agrÃ©gÃ©s
â”œâ”€â”€ best_model.pt                   # Meilleur modÃ¨le
â”œâ”€â”€ config.yaml                     # Configuration utilisÃ©e
â”œâ”€â”€ models/                         # DÃ©tails par modÃ¨le
â”‚   â”œâ”€â”€ UNet_Base/
â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt            # â­ Utiliser ce modÃ¨le
â”‚   â”‚   â”‚   â””â”€â”€ last.pt
â”‚   â”‚   â””â”€â”€ metrics.json
â”‚   â”œâ”€â”€ UNet_FiLM/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ visualizations/                 # Graphiques PNG
â”‚   â”œâ”€â”€ metric_comparison_iou.png
â”‚   â”œâ”€â”€ training_curves_iou.png
â”‚   â””â”€â”€ radar_chart.png
â””â”€â”€ reports/                        # Rapports
    â”œâ”€â”€ benchmark_report.html       # ğŸ“„ Ouvrir en premier
    â”œâ”€â”€ benchmark_report.md
    â””â”€â”€ benchmark_report.txt
```

## ğŸ” Analyser les rÃ©sultats

### 1. Rapport HTML (recommandÃ©)
```bash
# Trouver le dernier benchmark
ls -lhtr outputs/benchmarks/ | tail -1

# Ouvrir le rapport
firefox outputs/benchmarks/<experiment_id>/reports/benchmark_report.html
```

### 2. RÃ©sultats JSON
```bash
# Voir le rÃ©sumÃ©
cat outputs/benchmarks/<experiment_id>/benchmark_results.json | jq '.summary'

# Meilleurs modÃ¨les
cat outputs/benchmarks/<experiment_id>/benchmark_results.json | jq '.best_models'
```

### 3. TensorBoard (analyse approfondie)
```
http://localhost:6006
- Comparer les courbes d'entraÃ®nement
- Voir les distributions de poids
- Analyser la convergence
```

## ğŸ¯ Ã‰valuer sur donnÃ©es externes

AprÃ¨s le benchmark, teste le meilleur modÃ¨le sur les donnÃ©es SODEFOR :

```bash
# Dans le container
python -m forestgaps.evaluation.external \
  --model outputs/benchmarks/<experiment_id>/best_model.pt \
  --dsm data/data_external_test/SODEFOR_Mini2_DSM.tif \
  --chm data/data_external_test/SODEFOR_Mini2_CHM.tif \
  --output outputs/external_eval/<experiment_id> \
  --visualize
```

## ğŸ› ProblÃ¨mes courants

### "No module named 'forestgaps'"
```bash
# Dans le container, installer en mode dev
pip install -e .
```

### "Out of memory"
```bash
# RÃ©duire le batch size
python scripts/benchmark_quick_test.py --batch-size 2
```

### "CUDA out of memory"
```bash
# VÃ©rifier l'utilisation GPU
docker-compose exec forestgaps nvidia-smi

# Si un autre process utilise le GPU, le tuer ou rÃ©duire batch_size
```

### TensorBoard ne s'affiche pas
```bash
# RedÃ©marrer le service
docker-compose restart tensorboard

# VÃ©rifier qu'il tourne
docker-compose ps tensorboard
```

## ğŸ“ Commandes utiles

```bash
# Lister tous les benchmarks
ls -lhtr outputs/benchmarks/

# Voir la structure d'un benchmark
tree outputs/benchmarks/<experiment_id>/ -L 2

# Copier le meilleur modÃ¨le en production
cp outputs/benchmarks/<experiment_id>/best_model.pt models/production/unet_film_v1.pt

# Nettoyer les vieux logs (garder les 3 derniers)
cd logs/benchmarks && ls -t | tail -n +4 | xargs rm -rf

# Archiver une expÃ©rience
tar -czf archive_<experiment_id>.tar.gz outputs/benchmarks/<experiment_id>
```

## ğŸ’¡ Conseils

1. **Commencer par un test rapide** pour valider le setup
2. **Surveiller TensorBoard** pendant l'entraÃ®nement
3. **Nommer clairement les expÃ©riences** (experiment-name descriptif)
4. **Sauvegarder les bons modÃ¨les** dans `models/production/`
5. **Archiver les expÃ©riences importantes** (tar.gz)

## ğŸ“š Aller plus loin

- Lire [BENCHMARKING_GUIDE.md](BENCHMARKING_GUIDE.md) pour l'organisation complÃ¨te
- Consulter [forestgaps/benchmarking/README.md](forestgaps/benchmarking/README.md) pour l'API
- Voir les exemples dans `examples/`

## âš¡ Commande ultime (tout-en-un)

```bash
# Lancer Docker + TensorBoard + Benchmark rapide
cd docker/ && \
docker-compose up -d tensorboard && \
docker-compose run --rm forestgaps python scripts/benchmark_quick_test.py \
  --experiment-name "test_$(date +%Y%m%d)" && \
echo "âœ… RÃ©sultats dans : outputs/benchmarks/" && \
echo "ğŸ“Š TensorBoard : http://localhost:6006"
```

Bonne chance ! ğŸš€
