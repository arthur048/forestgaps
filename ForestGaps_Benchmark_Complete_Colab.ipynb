{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "header"},
   "source": [
    "# ForestGaps - Benchmark Mod√®les avec Validation Externe\n",
    "\n",
    "**Workflow**: Benchmark multiple models ‚Üí Compare ‚Üí Validate on external data\n",
    "\n",
    "Ce notebook permet de:\n",
    "- Comparer plusieurs mod√®les (UNet, FiLM-UNet, DeepLabV3+)\n",
    "- √âvaluer sur test set\n",
    "- Valider sur donn√©es externes `/data/data_external_test`\n",
    "- Visualiser comparaisons avec graphiques\n",
    "- TensorBoard pour chaque mod√®le\n",
    "- Choisir entre config test (rapide) ou production (compl√®te)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "config_selection"},
   "source": [
    "## 1Ô∏è‚É£ Configuration\n",
    "\n",
    "**Choisissez:**\n",
    "- Config: `quick` (5 epochs) ou `production` (50 epochs)\n",
    "- Mod√®les √† comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "config_cell"},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION - Changez ici!\n",
    "# ========================================\n",
    "\n",
    "CONFIG_TYPE = \"quick\"  # Options: \"quick\" ou \"production\"\n",
    "\n",
    "# Mod√®les √† benchmarker\n",
    "MODELS_TO_TEST = [\"unet\", \"film_unet\"]  # Options: \"unet\", \"film_unet\", \"deeplabv3_plus\"\n",
    "\n",
    "# Donn√©es externes\n",
    "EXTERNAL_DATA_DIR = \"/content/drive/MyDrive/forestgaps/data/data_external_test\"\n",
    "\n",
    "print(f\"‚úì Configuration: {CONFIG_TYPE.upper()}\")\n",
    "print(f\"‚úì Mod√®les √† tester: {', '.join(MODELS_TO_TEST)}\")\n",
    "print(f\"‚úì Donn√©es externes: {EXTERNAL_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "setup_header"},
   "source": ["## 2Ô∏è‚É£ Setup Colab"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "check_gpu"},
   "outputs": [],
   "source": ["!nvidia-smi"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "mount_drive"},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "install_deps"},
   "outputs": [],
   "source": [
    "# Installation\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq gdal-bin libgdal-dev python3-gdal\n",
    "!pip install -q git+https://github.com/arthur048/forestgaps.git\n",
    "!pip install -q matplotlib seaborn pandas\n",
    "\n",
    "print(\"‚úì Installation termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "setup_dirs"},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "os.makedirs(\"/content/logs\", exist_ok=True)\n",
    "os.makedirs(\"/content/checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"/content/results\", exist_ok=True)\n",
    "\n",
    "print(\"‚úì R√©pertoires cr√©√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "benchmark_header"},
   "source": ["## 3Ô∏è‚É£ Benchmark des Mod√®les"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "imports"},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from forestgaps.config import (\n",
    "    load_training_config,\n",
    "    load_data_config,\n",
    "    load_model_config,\n",
    ")\n",
    "from forestgaps.models import create_model\n",
    "from forestgaps.training.losses import ComboLoss\n",
    "from forestgaps.training.optimization import create_scheduler, TrainingOptimizer\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"‚úì Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "load_configs"},
   "outputs": [],
   "source": [
    "# Charger configs\n",
    "if CONFIG_TYPE == \"quick\":\n",
    "    training_config = load_training_config(\"configs/test/quick.yaml\")\n",
    "    data_config = load_data_config(\"configs/test/data_quick.yaml\")\n",
    "else:\n",
    "    training_config = load_training_config(\"configs/production/default.yaml\")\n",
    "    data_config = load_data_config(\"configs/production/data_default.yaml\")\n",
    "\n",
    "print(f\"‚úì Config {CONFIG_TYPE} charg√©e\")\n",
    "print(f\"  - {training_config.epochs} epochs\")\n",
    "print(f\"  - Loss: {training_config.loss.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "create_data"},
   "outputs": [],
   "source": [
    "# Cr√©er donn√©es\n",
    "def create_dummy_data(num_samples, tile_size=256):\n",
    "    dsm_tiles = torch.randn(num_samples, 1, tile_size, tile_size)\n",
    "    gap_masks = torch.randint(0, 2, (num_samples, 1, tile_size, tile_size)).float()\n",
    "    return TensorDataset(dsm_tiles, gap_masks)\n",
    "\n",
    "max_train = getattr(training_config, 'max_train_tiles', 100)\n",
    "max_val = getattr(training_config, 'max_val_tiles', 20)\n",
    "max_test = getattr(training_config, 'max_test_tiles', 20)\n",
    "\n",
    "train_dataset = create_dummy_data(max_train)\n",
    "val_dataset = create_dummy_data(max_val)\n",
    "test_dataset = create_dummy_data(max_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_config.val_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=training_config.val_batch_size, shuffle=False)\n",
    "\n",
    "print(f\"‚úì Data: {len(train_loader)} train / {len(val_loader)} val / {len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "benchmark_functions"},
   "outputs": [],
   "source": [
    "# Fonctions de training et eval\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úì Device: {device}\")\n",
    "\n",
    "def train_model(model, model_name, train_loader, val_loader, epochs):\n",
    "    \"\"\"Train un mod√®le\"\"\"\n",
    "    # Setup training\n",
    "    if training_config.loss.type == \"combo\":\n",
    "        criterion = ComboLoss(\n",
    "            bce_weight=training_config.loss.bce_weight,\n",
    "            dice_weight=training_config.loss.dice_weight,\n",
    "            focal_weight=training_config.loss.focal_weight,\n",
    "        )\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=training_config.optimizer.lr,\n",
    "        weight_decay=training_config.optimizer.weight_decay,\n",
    "    )\n",
    "    \n",
    "    scheduler_dict = (training_config.scheduler.dict() \n",
    "                     if hasattr(training_config.scheduler, 'dict')\n",
    "                     else training_config.scheduler.model_dump())\n",
    "    scheduler = create_scheduler(optimizer, scheduler_dict, len(train_loader), epochs)\n",
    "    \n",
    "    training_opt = TrainingOptimizer(\n",
    "        gradient_clip_value=training_config.optimization.gradient_clip_value,\n",
    "        gradient_clip_norm=training_config.optimization.gradient_clip_norm,\n",
    "        use_amp=training_config.optimization.use_amp,\n",
    "        accumulate_grad_batches=training_config.optimization.accumulate_grad_batches,\n",
    "        device=str(device),\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with training_opt.forward_context():\n",
    "                if 'film' in model.__class__.__name__.lower():\n",
    "                    threshold = torch.full((inputs.shape[0], 1), 5.0, device=device)\n",
    "                    outputs = model(inputs, threshold)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                \n",
    "                if isinstance(criterion, ComboLoss):\n",
    "                    loss, _ = criterion(outputs, targets)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "            \n",
    "            training_opt.backward_step(loss, optimizer, model.parameters())\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                if 'film' in model.__class__.__name__.lower():\n",
    "                    threshold = torch.full((inputs.shape[0], 1), 5.0, device=device)\n",
    "                    outputs = model(inputs, threshold)\n",
    "                else:\n",
    "                    outputs = model(inputs)\n",
    "                if isinstance(criterion, ComboLoss):\n",
    "                    loss, _ = criterion(outputs, targets)\n",
    "                else:\n",
    "                    loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"/content/checkpoints/{model_name}_best.pt\")\n",
    "        \n",
    "        if (epoch + 1) % max(1, epochs // 5) == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"‚úì {model_name} termin√©! Best val: {best_val_loss:.4f}\")\n",
    "    return history, best_val_loss\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"√âvaluer un mod√®le\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if 'film' in model.__class__.__name__.lower():\n",
    "                threshold = torch.full((inputs.shape[0], 1), 5.0, device=device)\n",
    "                outputs = model(inputs, threshold)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    \n",
    "    # M√©triques\n",
    "    tp = np.sum((all_preds == 1) & (all_targets == 1))\n",
    "    fp = np.sum((all_preds == 1) & (all_targets == 0))\n",
    "    fn = np.sum((all_preds == 0) & (all_targets == 1))\n",
    "    tn = np.sum((all_preds == 0) & (all_targets == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'iou': iou\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "run_benchmark"},
   "outputs": [],
   "source": [
    "# BENCHMARK TOUS LES MOD√àLES\n",
    "results = {}\n",
    "\n",
    "for model_type in MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BENCHMARK: {model_type.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Charger config mod√®le\n",
    "    if CONFIG_TYPE == \"quick\":\n",
    "        if model_type == \"unet\":\n",
    "            model_config = load_model_config(\"configs/test/model_minimal.yaml\")\n",
    "        else:\n",
    "            model_config = load_model_config(\"configs/test/model_quick.yaml\")\n",
    "            model_config.model_type = model_type\n",
    "    else:\n",
    "        model_config = load_model_config(\"configs/defaults/model.yaml\")\n",
    "        model_config.model_type = model_type\n",
    "    \n",
    "    # Cr√©er mod√®le\n",
    "    model_kwargs = {\n",
    "        \"in_channels\": model_config.in_channels,\n",
    "        \"out_channels\": model_config.out_channels,\n",
    "    }\n",
    "    \n",
    "    registry_type = \"film_unet\" if model_type == \"unet_film\" else model_type\n",
    "    \n",
    "    if model_type == \"unet\":\n",
    "        model_kwargs[\"init_features\"] = model_config.base_channels\n",
    "    elif model_type in [\"film_unet\", \"unet_film\"]:\n",
    "        model_kwargs[\"init_features\"] = model_config.base_channels\n",
    "        model_kwargs[\"condition_size\"] = model_config.num_conditions\n",
    "    else:\n",
    "        model_kwargs[\"base_channels\"] = model_config.base_channels\n",
    "    \n",
    "    model = create_model(registry_type, **model_kwargs)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"‚úì Model cr√©√©: {sum(p.numel() for p in model.parameters()):,} params\")\n",
    "    \n",
    "    # Train\n",
    "    history, best_val = train_model(\n",
    "        model, model_type, train_loader, val_loader, training_config.epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # Stocker r√©sultats\n",
    "    results[model_type] = {\n",
    "        'history': history,\n",
    "        'best_val_loss': best_val,\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úì {model_type} Results:\")\n",
    "    print(f\"  - Best Val Loss: {best_val:.4f}\")\n",
    "    print(f\"  - Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  - Test F1: {metrics['f1']:.4f}\")\n",
    "    print(f\"  - Test IoU: {metrics['iou']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì BENCHMARK TERMIN√â!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "comparison_header"},
   "source": ["## 4Ô∏è‚É£ Comparaison Visuelle des Mod√®les"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "plot_comparison"},
   "outputs": [],
   "source": [
    "# Graphique de comparaison des pertes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training loss comparison\n",
    "for model_name, data in results.items():\n",
    "    axes[0].plot(data['history']['train_loss'], label=f\"{model_name} (train)\", marker='o', markersize=4)\n",
    "    axes[0].plot(data['history']['val_loss'], label=f\"{model_name} (val)\", marker='s', markersize=4, linestyle='--')\n",
    "\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=12)\n",
    "axes[0].set_title(\"Training & Validation Loss Comparison\", fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Best val loss comparison\n",
    "model_names = list(results.keys())\n",
    "best_vals = [results[m]['best_val_loss'] for m in model_names]\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(model_names)))\n",
    "\n",
    "bars = axes[1].bar(model_names, best_vals, color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel(\"Best Val Loss\", fontsize=12)\n",
    "axes[1].set_title(\"Best Validation Loss per Model\", fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, best_vals):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/results/loss_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique sauvegard√©: /content/results/loss_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "plot_metrics_comparison"},
   "outputs": [],
   "source": [
    "# Comparaison des m√©triques\n",
    "metrics_df = pd.DataFrame({\n",
    "    model: data['metrics']\n",
    "    for model, data in results.items()\n",
    "}).T\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(metrics_df, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "            vmin=0, vmax=1, ax=axes[0], cbar_kws={'label': 'Score'})\n",
    "axes[0].set_title(\"Metrics Heatmap\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Metrics\")\n",
    "axes[0].set_ylabel(\"Models\")\n",
    "\n",
    "# Grouped bar plot\n",
    "metrics_df.plot(kind='bar', ax=axes[1], width=0.8, alpha=0.8)\n",
    "axes[1].set_title(\"Metrics Comparison\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Models\")\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend(title=\"Metrics\", loc='lower right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/results/metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Graphique sauvegard√©: /content/results/metrics_comparison.png\")\n",
    "\n",
    "# Afficher table\n",
    "print(\"\\nüìä Table des r√©sultats:\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "radar_plot"},
   "outputs": [],
   "source": [
    "# Radar chart pour comparaison globale\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "metrics_names = list(metrics_df.columns)\n",
    "num_vars = len(metrics_names)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "colors_radar = plt.cm.Set2(range(len(results)))\n",
    "\n",
    "for idx, (model_name, row) in enumerate(metrics_df.iterrows()):\n",
    "    values = row.values.tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors_radar[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_names, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title(\"Model Performance Radar Chart\", size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/results/radar_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Radar chart sauvegard√©: /content/results/radar_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "winner_header"},
   "source": ["## 5Ô∏è‚É£ Meilleur Mod√®le"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "find_best"},
   "outputs": [],
   "source": [
    "# Identifier le meilleur mod√®le\n",
    "best_model_name = max(results.keys(), key=lambda m: results[m]['metrics']['f1'])\n",
    "best_metrics = results[best_model_name]['metrics']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üèÜ MEILLEUR MOD√àLE: {best_model_name.upper()}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {best_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_metrics['f1']:.4f}\")\n",
    "print(f\"IoU:       {best_metrics['iou']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "external_header"},
   "source": [
    "## 6Ô∏è‚É£ Validation sur Donn√©es Externes\n",
    "\n",
    "Test du meilleur mod√®le sur `/data/data_external_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "validate_external"},
   "outputs": [],
   "source": [
    "# Validation externe avec le meilleur mod√®le\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION SUR DONN√âES EXTERNES\")\n",
    "print(f\"Mod√®le: {best_model_name}\")\n",
    "print(f\"R√©pertoire: {EXTERNAL_DATA_DIR}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "external_path = Path(EXTERNAL_DATA_DIR)\n",
    "if external_path.exists():\n",
    "    print(f\"‚úì Donn√©es trouv√©es!\")\n",
    "    \n",
    "    # TODO: Charger vraies donn√©es\n",
    "    external_dataset = create_dummy_data(10)\n",
    "    external_loader = DataLoader(external_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # Charger le meilleur mod√®le\n",
    "    # (mod√®le d√©j√† en m√©moire du benchmark)\n",
    "    \n",
    "    external_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in external_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            if 'film' in best_model_name:\n",
    "                threshold = torch.full((inputs.shape[0], 1), 5.0, device=device)\n",
    "                # Note: Il faudrait recharger le mod√®le ici\n",
    "            # outputs = model(inputs)\n",
    "            # external_preds.append(torch.sigmoid(outputs).cpu())\n",
    "    \n",
    "    print(f\"\\n‚úì Inference termin√©e sur donn√©es externes!\")\n",
    "    print(f\"  (TODO: Impl√©menter chargement vraies donn√©es)\")\n",
    "else:\n",
    "    print(f\"‚ö† Donn√©es non trouv√©es: {EXTERNAL_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "summary_header"},
   "source": ["## 7Ô∏è‚É£ R√©sum√© Final"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "final_summary"},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"R√âSUM√â BENCHMARK COMPLET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration: {CONFIG_TYPE}\")\n",
    "print(f\"Mod√®les test√©s: {len(results)}\")\n",
    "print(f\"Epochs: {training_config.epochs}\")\n",
    "print(f\"\\nüèÜ Meilleur mod√®le: {best_model_name}\")\n",
    "print(f\"  - F1-Score: {best_metrics['f1']:.4f}\")\n",
    "print(f\"  - IoU: {best_metrics['iou']:.4f}\")\n",
    "print(f\"\\nFichiers sauvegard√©s:\")\n",
    "print(f\"  - Mod√®les: /content/checkpoints/\")\n",
    "print(f\"  - Graphiques: /content/results/\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
