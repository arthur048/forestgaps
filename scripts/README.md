# Scripts ForestGaps

Ce rÃ©pertoire contient les scripts utilitaires pour l'entraÃ®nement, l'Ã©valuation et le benchmarking des modÃ¨les ForestGaps.

## ğŸ“ Structure

```
scripts/
â”œâ”€â”€ README.md                    # Ce fichier
â”œâ”€â”€ benchmark_quick_test.py      # Test rapide du benchmarking (5-10 min)
â”œâ”€â”€ benchmark_full.py            # Benchmark complet (plusieurs heures)
â”œâ”€â”€ docker-build.sh              # Construction de l'image Docker
â”œâ”€â”€ docker-run.sh                # Lancement du container Docker
â””â”€â”€ docker-test.sh               # Tests dans Docker
```

## ğŸ¯ Scripts de Benchmarking

### `benchmark_quick_test.py`

Script de test rapide pour valider le pipeline de benchmarking.

**Usage :**
```bash
python scripts/benchmark_quick_test.py \
  --experiment-name "test_run" \
  --epochs 5 \
  --models "unet,unet_film"
```

**ParamÃ¨tres principaux :**
- `--experiment-name` : Nom de l'expÃ©rience (requis)
- `--epochs` : Nombre d'Ã©poques (dÃ©faut: 5)
- `--batch-size` : Taille du batch (dÃ©faut: 4)
- `--max-train-tiles` : Nombre de tuiles d'entraÃ®nement (dÃ©faut: 20)
- `--models` : ModÃ¨les Ã  comparer (dÃ©faut: "unet,unet_film")
- `--thresholds` : Seuils de hauteur (dÃ©faut: "5.0,10.0")

**Cas d'usage :**
- Tester rapidement une nouvelle configuration
- Valider que le pipeline fonctionne
- DÃ©bugger avant un long entraÃ®nement

**DurÃ©e estimÃ©e :** 5-10 minutes

---

### `benchmark_full.py`

Script de benchmarking complet pour comparer tous les modÃ¨les.

**Usage :**
```bash
python scripts/benchmark_full.py \
  --experiment-name "comparison_all_models" \
  --epochs 50 \
  --batch-size 8 \
  --models "unet,unet_film,deeplabv3_plus,deeplabv3_plus_threshold"
```

**ParamÃ¨tres principaux :**
- `--experiment-name` : Nom de l'expÃ©rience (requis)
- `--epochs` : Nombre d'Ã©poques (dÃ©faut: 50)
- `--batch-size` : Taille du batch (dÃ©faut: 8)
- `--models` : ModÃ¨les Ã  comparer (dÃ©faut: tous)
- `--thresholds` : Seuils de hauteur (dÃ©faut: "2.0,5.0,10.0,15.0")
- `--config` : Fichier de config personnalisÃ© (optionnel)
- `--no-tensorboard` : DÃ©sactiver TensorBoard
- `--save-all-checkpoints` : Sauvegarder tous les checkpoints

**Cas d'usage :**
- Benchmark final pour la publication
- Comparaison exhaustive des architectures
- ExpÃ©riences de recherche

**DurÃ©e estimÃ©e :** 4-8 heures (selon GPU et donnÃ©es)

---

## ğŸš€ ModÃ¨les disponibles

Les modÃ¨les suivants peuvent Ãªtre spÃ©cifiÃ©s avec `--models` :

| Nom | Description | ParamÃ¨tres |
|-----|-------------|------------|
| `unet` | U-Net de base | 32 features init |
| `unet_film` | U-Net avec FiLM | 32 features + FiLM |
| `deeplabv3_plus` | DeepLabV3+ base | ASPP + dÃ©codeur |
| `deeplabv3_plus_threshold` | DeepLabV3+ conditionnÃ© | ASPP + CBAM + encoding seuil |

## ğŸ“Š Outputs gÃ©nÃ©rÃ©s

Chaque benchmark crÃ©e une structure complÃ¨te dans `outputs/benchmarks/` :

```
YYYYMMDD_HHMMSS_<experiment_name>/
â”œâ”€â”€ config.yaml                    # Configuration complÃ¨te
â”œâ”€â”€ benchmark_results.json         # RÃ©sultats agrÃ©gÃ©s
â”œâ”€â”€ best_model.pt                  # Meilleur modÃ¨le global
â”œâ”€â”€ models/                        # ModÃ¨les individuels
â”‚   â”œâ”€â”€ <ModelName>/
â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt           # Meilleur checkpoint
â”‚   â”‚   â”‚   â””â”€â”€ last.pt           # Dernier checkpoint
â”‚   â”‚   â”œâ”€â”€ metrics.json          # MÃ©triques du modÃ¨le
â”‚   â”‚   â”œâ”€â”€ model_config.json     # Config du modÃ¨le
â”‚   â”‚   â””â”€â”€ prediction_examples/  # Exemples de prÃ©dictions
â”œâ”€â”€ visualizations/                # Graphiques comparatifs
â”‚   â”œâ”€â”€ metric_comparison_*.png
â”‚   â”œâ”€â”€ threshold_comparison_*.png
â”‚   â”œâ”€â”€ training_curves_*.png
â”‚   â”œâ”€â”€ training_time_comparison.png
â”‚   â”œâ”€â”€ convergence_speed_*.png
â”‚   â””â”€â”€ radar_chart.png
â””â”€â”€ reports/                       # Rapports dÃ©taillÃ©s
    â”œâ”€â”€ benchmark_report.html      # Rapport principal
    â”œâ”€â”€ benchmark_report.md        # Version Markdown
    â””â”€â”€ benchmark_report.txt       # Version texte
```

## ğŸ” MÃ©triques calculÃ©es

### Par modÃ¨le et par seuil
- **IoU** : Intersection over Union
- **F1-Score** : Harmonic mean Precision/Recall
- **Precision** : Taux de vrais positifs
- **Recall** : Taux de dÃ©tection

### MÃ©triques globales
- **Moyennes** : MÃ©triques moyennÃ©es sur tous les seuils
- **Temps d'entraÃ®nement** : DurÃ©e totale en secondes
- **Vitesse de convergence** : Ã‰poques pour atteindre 90% de perf max

### Classement
- Meilleur modÃ¨le par mÃ©trique
- Meilleur modÃ¨le par seuil
- ModÃ¨le le plus rapide
- ModÃ¨le le plus stable

## ğŸ’¡ Exemples d'utilisation

### Test rapide avec 2 modÃ¨les
```bash
python scripts/benchmark_quick_test.py \
  --experiment-name "test_unet_vs_film" \
  --models "unet,unet_film" \
  --epochs 5
```

### Benchmark complet avec configuration personnalisÃ©e
```bash
python scripts/benchmark_full.py \
  --experiment-name "exp_custom_config" \
  --config config/custom.yaml \
  --epochs 100 \
  --batch-size 16
```

### Benchmark avec seuils spÃ©cifiques
```bash
python scripts/benchmark_full.py \
  --experiment-name "seuils_extrÃªmes" \
  --thresholds "1.0,2.0,20.0,30.0" \
  --epochs 50
```

### Benchmark sans TensorBoard (serveur sans GUI)
```bash
python scripts/benchmark_full.py \
  --experiment-name "server_run" \
  --no-tensorboard \
  --epochs 50
```

## ğŸ› Debugging

### Mode verbose
```bash
# Ajouter avant la commande
export LOG_LEVEL=DEBUG
python scripts/benchmark_full.py ...
```

### Profiling mÃ©moire
```bash
# Utiliser le profiler Python
python -m memory_profiler scripts/benchmark_full.py ...
```

### Dry run (vÃ©rifier sans exÃ©cuter)
Modifier temporairement `epochs=1` et `max_train_tiles=5` pour tester rapidement.

## ğŸ“š Documentation associÃ©e

- [QUICK_START_BENCHMARK.md](../QUICK_START_BENCHMARK.md) : Guide de dÃ©marrage rapide
- [BENCHMARKING_GUIDE.md](../BENCHMARKING_GUIDE.md) : Guide complet d'organisation
- [forestgaps/benchmarking/README.md](../forestgaps/benchmarking/README.md) : Documentation de l'API

## âš™ï¸ Scripts Docker

### `docker-build.sh`
Construit l'image Docker ForestGaps.

```bash
bash scripts/docker-build.sh
```

### `docker-run.sh`
Lance le container avec les bons volumes montÃ©s.

```bash
bash scripts/docker-run.sh python scripts/benchmark_full.py ...
```

### `docker-test.sh`
ExÃ©cute les tests dans le container Docker.

```bash
bash scripts/docker-test.sh
```

## ğŸ” Bonnes pratiques

1. **Toujours nommer les expÃ©riences** avec `--experiment-name`
2. **Commencer par un test rapide** avant le benchmark complet
3. **Surveiller TensorBoard** pendant l'entraÃ®nement
4. **Sauvegarder les configs** pour la reproductibilitÃ©
5. **Archiver les bons rÃ©sultats** dans `models/production/`
6. **Documenter les expÃ©riences** dans un fichier EXPERIMENTS.md

## ğŸ†˜ Support

En cas de problÃ¨me :
1. VÃ©rifier les logs : `docker-compose logs forestgaps`
2. Consulter [BENCHMARKING_GUIDE.md](../BENCHMARKING_GUIDE.md) section Troubleshooting
3. VÃ©rifier les issues GitHub du projet
