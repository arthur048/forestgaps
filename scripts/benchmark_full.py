#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script de benchmarking complet pour les mod√®les ForestGaps.

Ce script ex√©cute un benchmark complet avec tous les mod√®les
et configurations sur l'ensemble des donn√©es.

Usage:
    python scripts/benchmark_full.py --experiment-name "comparison_all_models"
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from forestgaps.config import load_default_config
from forestgaps.environment import setup_environment
from forestgaps.data.loaders import create_data_loaders
from forestgaps.benchmarking import ModelComparison

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("benchmark_full")


def parse_args():
    """Parse les arguments de la ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Benchmark complet des mod√®les ForestGaps",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(
        "--experiment-name",
        type=str,
        required=True,
        help="Nom de l'exp√©rience (sera pr√©fix√© par timestamp)"
    )

    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="Nombre d'√©poques pour l'entra√Ænement"
    )

    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Taille du batch"
    )

    parser.add_argument(
        "--models",
        type=str,
        default="unet,unet_film,deeplabv3_plus,deeplabv3_plus_threshold",
        help="Mod√®les √† comparer (s√©par√©s par des virgules)"
    )

    parser.add_argument(
        "--thresholds",
        type=str,
        default="2.0,5.0,10.0,15.0",
        help="Seuils de hauteur √† √©valuer (s√©par√©s par des virgules)"
    )

    parser.add_argument(
        "--output-base-dir",
        type=str,
        default="outputs/benchmarks",
        help="R√©pertoire de base pour les outputs"
    )

    parser.add_argument(
        "--log-base-dir",
        type=str,
        default="logs/benchmarks",
        help="R√©pertoire de base pour les logs"
    )

    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Chemin vers un fichier de configuration personnalis√© (optionnel)"
    )

    parser.add_argument(
        "--no-tensorboard",
        action="store_true",
        help="D√©sactiver les logs TensorBoard"
    )

    parser.add_argument(
        "--save-all-checkpoints",
        action="store_true",
        help="Sauvegarder tous les checkpoints (pas seulement le meilleur)"
    )

    return parser.parse_args()


def create_experiment_dirs(base_output_dir: str, base_log_dir: str, experiment_name: str):
    """
    Cr√©e les r√©pertoires pour l'exp√©rience avec timestamp.

    Args:
        base_output_dir: R√©pertoire de base pour outputs
        base_log_dir: R√©pertoire de base pour logs
        experiment_name: Nom de l'exp√©rience

    Returns:
        Tuple (output_dir, log_dir, experiment_id)
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_id = f"{timestamp}_{experiment_name}"

    output_dir = Path(base_output_dir) / experiment_id
    log_dir = Path(base_log_dir) / experiment_id

    output_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)

    # Cr√©er les sous-r√©pertoires
    (output_dir / "models").mkdir(exist_ok=True)
    (output_dir / "visualizations").mkdir(exist_ok=True)
    (output_dir / "reports").mkdir(exist_ok=True)

    logger.info(f"Exp√©rience ID: {experiment_id}")
    logger.info(f"Output dir: {output_dir}")
    logger.info(f"Log dir: {log_dir}")

    return output_dir, log_dir, experiment_id


def get_all_model_configs() -> list:
    """
    Retourne toutes les configurations de mod√®les disponibles.

    Returns:
        Liste des configurations de mod√®les
    """
    return [
        # U-Net de base
        {
            "name": "unet",
            "display_name": "UNet_Base",
            "params": {
                "in_channels": 1,
                "out_channels": 1,
                "init_features": 32,
                "dropout_rate": 0.2
            }
        },

        # U-Net avec FiLM (Feature-wise Linear Modulation)
        {
            "name": "unet_film",
            "display_name": "UNet_FiLM",
            "params": {
                "in_channels": 1,
                "out_channels": 1,
                "init_features": 32,
                "dropout_rate": 0.2
            }
        },

        # DeepLabV3+ de base
        {
            "name": "deeplabv3_plus",
            "display_name": "DeepLabV3+_Base",
            "params": {
                "in_channels": 1,
                "out_channels": 1,
                "encoder_channels": [64, 128, 256, 512],
                "aspp_channels": 256,
                "decoder_channels": 256,
                "dropout_rate": 0.2,
                "use_cbam": False
            }
        },

        # DeepLabV3+ avec conditionnement par seuil
        {
            "name": "deeplabv3_plus_threshold",
            "display_name": "DeepLabV3+_Threshold",
            "params": {
                "in_channels": 1,
                "out_channels": 1,
                "encoder_channels": [64, 128, 256, 512],
                "aspp_channels": 256,
                "decoder_channels": 256,
                "threshold_encoding_dim": 128,
                "dropout_rate": 0.2,
                "use_cbam": True,
                "use_pos_encoding": True
            }
        }
    ]


def filter_model_configs(all_configs: list, selected_names: list) -> list:
    """
    Filtre les configurations de mod√®les selon les noms s√©lectionn√©s.

    Args:
        all_configs: Toutes les configurations disponibles
        selected_names: Noms des mod√®les s√©lectionn√©s

    Returns:
        Liste filtr√©e des configurations
    """
    filtered = []
    for name in selected_names:
        matching = [c for c in all_configs if c['name'] == name]
        if matching:
            filtered.extend(matching)
        else:
            logger.warning(f"Mod√®le inconnu: {name}, ignor√©")

    return filtered


def print_banner(text: str, char: str = "="):
    """Affiche un bandeau format√©."""
    logger.info(char * 80)
    logger.info(text)
    logger.info(char * 80)


def main():
    """Fonction principale."""
    args = parse_args()

    print_banner("BENCHMARKING COMPLET - ForestGaps")

    # Configuration de l'environnement
    logger.info("\n[1/7] Configuration de l'environnement...")
    env = setup_environment()
    logger.info(f"Environnement: {env.__class__.__name__}")
    logger.info(f"Device: {env.get_device()}")

    # Cr√©er les r√©pertoires d'exp√©rience
    logger.info("\n[2/7] Cr√©ation des r√©pertoires d'exp√©rience...")
    output_dir, log_dir, experiment_id = create_experiment_dirs(
        args.output_base_dir,
        args.log_base_dir,
        args.experiment_name
    )

    # Charger la configuration
    logger.info("\n[3/7] Chargement de la configuration...")
    if args.config:
        from forestgaps.config import load_config_from_file
        config = load_config_from_file(args.config)
        logger.info(f"Configuration charg√©e depuis: {args.config}")
    else:
        config = load_default_config()
        logger.info("Configuration par d√©faut charg√©e")

    # Ajuster les param√®tres d'entra√Ænement
    config.training.epochs = args.epochs
    config.training.batch_size = args.batch_size

    logger.info(f"Epochs: {config.training.epochs}")
    logger.info(f"Batch size: {config.training.batch_size}")
    logger.info(f"Learning rate: {config.training.learning_rate}")
    logger.info(f"Optimizer: {config.training.optimizer}")

    # Sauvegarder la configuration
    config_path = output_dir / "config.yaml"
    logger.info(f"Configuration sauvegard√©e: {config_path}")

    # Cr√©er les DataLoaders
    logger.info("\n[4/7] Cr√©ation des DataLoaders...")
    try:
        data_loaders = create_data_loaders(config)
        logger.info(f"Train loader: {len(data_loaders['train'])} batches")
        logger.info(f"Val loader: {len(data_loaders['val'])} batches")
        logger.info(f"Test loader: {len(data_loaders['test'])} batches")
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation des DataLoaders: {e}")
        logger.error("V√©rifiez que les donn√©es sont pr√©sentes dans 'data/'")
        return 1

    # Pr√©parer les configurations des mod√®les
    logger.info("\n[5/7] Pr√©paration des mod√®les...")
    all_configs = get_all_model_configs()
    model_names = [m.strip() for m in args.models.split(',')]
    model_configs = filter_model_configs(all_configs, model_names)

    if not model_configs:
        logger.error("Aucune configuration de mod√®le valide")
        return 1

    logger.info(f"Mod√®les √† comparer ({len(model_configs)}):")
    for mc in model_configs:
        logger.info(f"  - {mc['display_name']} ({mc['name']})")

    # Parser les seuils
    thresholds = [float(t.strip()) for t in args.thresholds.split(',')]
    logger.info(f"Seuils de hauteur: {thresholds} m√®tres")

    # Informations sur le temps estim√©
    logger.info("\n[6/7] Estimation du temps de calcul...")
    n_models = len(model_configs)
    n_epochs = args.epochs
    estimated_time_per_epoch = 2  # minutes (approximatif)
    total_estimated_minutes = n_models * n_epochs * estimated_time_per_epoch
    logger.info(f"Temps estim√©: ~{total_estimated_minutes} minutes (~{total_estimated_minutes/60:.1f}h)")
    logger.info("Note: Ceci est une estimation approximative")

    # Confirmer avant de lancer
    logger.info("\n" + "-"*80)
    logger.info("Configuration du benchmark:")
    logger.info(f"  - Exp√©rience: {experiment_id}")
    logger.info(f"  - Mod√®les: {n_models}")
    logger.info(f"  - Epochs: {n_epochs}")
    logger.info(f"  - Batch size: {args.batch_size}")
    logger.info(f"  - Seuils: {len(thresholds)}")
    logger.info(f"  - TensorBoard: {'D√©sactiv√©' if args.no_tensorboard else 'Activ√© (http://localhost:6006)'}")
    logger.info("-"*80)

    # Cr√©er et ex√©cuter le benchmark
    logger.info("\n[7/7] Ex√©cution du benchmark...")
    print_banner("D√©but de l'entra√Ænement", char="-")

    benchmark = ModelComparison(
        model_configs=model_configs,
        base_config=config,
        train_loader=data_loaders['train'],
        val_loader=data_loaders['val'],
        test_loader=data_loaders['test'],
        output_dir=output_dir,
        threshold_values=thresholds
    )

    try:
        results = benchmark.run()

        print_banner("BENCHMARK TERMIN√â AVEC SUCC√àS !")

        # Afficher les r√©sultats principaux
        logger.info("\nR√©sultats principaux:")
        logger.info("-"*80)

        # Meilleurs mod√®les par m√©trique
        logger.info("\nMeilleurs mod√®les:")
        for metric in ['iou', 'f1', 'precision', 'recall']:
            best_model = benchmark.get_best_model(metric=metric)
            if best_model:
                name = best_model.get('display_name', best_model.get('name'))
                logger.info(f"  - {metric.upper():12s}: {name}")

        # Mod√®le le plus rapide
        best_models = results.get('best_models', {})
        if 'training_time' in best_models:
            logger.info(f"  - {'TIME':12s}: {best_models['training_time']}")

        # Sauvegarder le meilleur mod√®le
        logger.info("\nSauvegarde du meilleur mod√®le...")
        best_model_path = output_dir / "best_model.pt"
        saved_path = benchmark.save_best_model(best_model_path)
        if saved_path:
            logger.info(f"Meilleur mod√®le: {saved_path}")

        # R√©sum√© des chemins
        logger.info("\n" + "="*80)
        logger.info("R√âSULTATS DISPONIBLES DANS:")
        logger.info("="*80)
        logger.info(f"üìÅ Outputs:      {output_dir}")
        logger.info(f"üìä Logs:         {log_dir}")
        logger.info(f"üìà TensorBoard:  http://localhost:6006")
        logger.info(f"üìÑ Rapport HTML: {output_dir}/reports/benchmark_report.html")
        logger.info(f"üìã R√©sultats:    {output_dir}/benchmark_results.json")
        logger.info(f"üèÜ Meilleur:     {best_model_path}")
        logger.info("="*80)

        # Conseils pour la suite
        logger.info("\nPROCHAINES √âTAPES:")
        logger.info("1. Visualiser dans TensorBoard: http://localhost:6006")
        logger.info("2. Consulter le rapport HTML pour l'analyse d√©taill√©e")
        logger.info("3. √âvaluer le meilleur mod√®le sur donn√©es externes:")
        logger.info(f"   python scripts/evaluate_external.py --model {best_model_path}")
        logger.info("")

        return 0

    except KeyboardInterrupt:
        logger.warning("\nBenchmark interrompu par l'utilisateur")
        logger.info("Les r√©sultats partiels sont disponibles dans:")
        logger.info(f"  - {output_dir}")
        return 130

    except Exception as e:
        logger.error(f"\nErreur lors du benchmark: {e}", exc_info=True)
        logger.error("\nV√©rifiez:")
        logger.error("  1. Les donn√©es sont pr√©sentes dans data/")
        logger.error("  2. Le GPU est disponible (nvidia-smi)")
        logger.error("  3. La m√©moire est suffisante")
        logger.error("  4. Les logs pour plus de d√©tails")
        return 1


if __name__ == "__main__":
    sys.exit(main())
