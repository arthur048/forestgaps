{"cells":[{"cell_type":"markdown","metadata":{"id":"2B0MFk3hhgYG"},"source":["# ImplÃ©mentation du modÃ¨le U-Net avec PyTorch pour la dÃ©tection des trouÃ©es forestiÃ¨res\n","\n","Ce script dÃ©finit l'architecture du modÃ¨le U-Net en utilisant PyTorch pour la\n","dÃ©tection des trouÃ©es forestiÃ¨res Ã  partir de modÃ¨les numÃ©riques de surface (DSM).\n","\n","**Auteur :** VANDER LINDEN Arthur\n","**Date :** 28-02-2025\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y-uqXW6P0qqG"},"source":["# PARTIE 1: CONFIGURATION"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1741116909061,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"aTuTGLBQ8QDd","outputId":"a236d9f0-56fe-4f14-d4c5-75e41768b643"},"outputs":[{"output_type":"stream","name":"stdout","text":["Session keep-alive activÃ©. Votre session Colab restera active.\n"]}],"source":["import time\n","import threading\n","from google.colab import output\n","\n","def keep_alive():\n","    while True:\n","        output.eval_js('google.colab.kernel.invokeFunction(\"notebook\", \"ExecuteTime\", {})')\n","        time.sleep(60)  # Attendre 60 secondes avant de renvoyer une requÃªte\n","\n","# DÃ©marrer dans un thread sÃ©parÃ© pour ne pas bloquer l'exÃ©cution\n","keep_alive_thread = threading.Thread(target=keep_alive)\n","keep_alive_thread.daemon = True  # Le thread s'arrÃªtera quand le programme principal se termine\n","keep_alive_thread.start()\n","\n","print(\"Session keep-alive activÃ©. Votre session Colab restera active.\")"]},{"cell_type":"markdown","metadata":{"id":"t59ExEvQuZQr"},"source":["## SECTION 1: IMPORTATIONS ET CONFIGURATION"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81815,"status":"ok","timestamp":1741116990878,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"w5R5IaD2hMZy","outputId":"94ac68c0-f15e-4a38-88c1-1c21c64cf390"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.1.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n","Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n","Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.7)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n","Utilisation de: cuda\n","PyTorch version: 2.5.1+cu124\n","GPU disponible: NVIDIA L4\n","MÃ©moire GPU totale: 23.80 GB\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# =====================================================================\n","# SECTION 1: IMPORTATIONS ET CONFIGURATION\n","# =====================================================================\n","\n","!pip install rasterio geopandas\n","\n","import os\n","import glob\n","import time\n","import copy\n","import sys\n","import json\n","import pickle\n","import random\n","import datetime\n","import warnings\n","import traceback\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from pathlib import Path\n","from scipy import ndimage\n","import cv2\n","\n","# Importation des bibliothÃ¨ques pour le traitement de donnÃ©es gÃ©ospatiales\n","import rasterio\n","from rasterio.windows import Window\n","import geopandas as gpd\n","\n","# Importations PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as TF\n","\n","from torch.amp import GradScaler, autocast\n","\n","# Ignorer les avertissements de FutureWarning de torch.load\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.serialization\")\n","\n","# Configuration PyTorch\n","# Fixer le seed pour la reproductibilitÃ©\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# DÃ©terminer le pÃ©riphÃ©rique Ã  utiliser (CPU ou GPU)\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Utilisation de: {DEVICE}\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","\n","# Optimisation des opÃ©rations CUDA\n","def optimize_cuda_operations():\n","    \"\"\"\n","    Configure les opÃ©rations CUDA pour de meilleures performances.\n","    Ã€ appeler au dÃ©but du script.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        # Activer l'auto-tuning pour trouver les algorithmes CUDA les plus rapides\n","        torch.backends.cudnn.benchmark = True\n","\n","        # Si votre batch size et tailles d'entrÃ©e sont fixes, ceci peut amÃ©liorer les performances\n","        # torch.backends.cudnn.deterministic = False\n","\n","        # Permet Ã  PyTorch d'allouer plus de mÃ©moire si nÃ©cessaire\n","        torch.cuda.empty_cache()\n","\n","        # Configurer les allocateurs de mÃ©moire CUDA pour de meilleures performances\n","        # avec les GPUs rÃ©cents\n","        # torch.cuda.set_per_process_memory_fraction(0.95)  # Utiliser 95% de la mÃ©moire GPU\n","\n","        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n","        print(f\"MÃ©moire GPU totale: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","    else:\n","        print(\"CUDA n'est pas disponible. Utilisation du CPU.\")\n","\n","# Appeler la fonction d'optimisation CUDA\n","optimize_cuda_operations()\n","\n","# Montage de Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yKYjybMMucsA"},"source":["## SECTION 2: DÃ‰FINITIONS DES CHEMINS ET CONFIGURATION\n"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1741116990884,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"6ZDkr2uRigRs"},"outputs":[],"source":["# =====================================================================\n","# SECTION 2: DÃ‰FINITIONS DES CHEMINS ET CONFIGURATION\n","# =====================================================================\n","\n","class Config:\n","    \"\"\"Classe de configuration centralisÃ©e pour le projet\"\"\"\n","\n","    def __init__(self):\n","        # RÃ©pertoire de base\n","        self.BASE_DIR = '/content/drive/MyDrive/ForestGaps_DeepLearning_Workflow'\n","\n","        # Sous-rÃ©pertoires principaux\n","        self.DATA_DIR = os.path.join(self.BASE_DIR, 'data')\n","        self.DATA_EXTERNAL_TEST_DIR = os.path.join(self.BASE_DIR, 'data_external_test')\n","        self.PROCESSED_DIR = os.path.join(self.BASE_DIR, 'processed')\n","        self.TILES_DIR = os.path.join(self.PROCESSED_DIR, 'tiles')\n","\n","        # Sous-rÃ©pertoires pour les tuiles\n","        self.TRAIN_TILES_DIR = os.path.join(self.TILES_DIR, 'train')\n","        self.VAL_TILES_DIR = os.path.join(self.TILES_DIR, 'val')\n","        self.TEST_TILES_DIR = os.path.join(self.TILES_DIR, 'test')\n","\n","        # Dossiers spÃ©cifiques au modÃ¨le U-Net\n","        self.MODELS_DIR = os.path.join(self.BASE_DIR, 'models')\n","        self.UNET_DIR = os.path.join(self.MODELS_DIR, 'unet')\n","\n","        # Sous-rÃ©pertoires pour les rÃ©sultats et les checkpoints\n","        self.CHECKPOINTS_DIR = os.path.join(self.UNET_DIR, 'checkpoints')\n","        self.LOGS_DIR = os.path.join(self.UNET_DIR, 'logs')\n","        self.RESULTS_DIR = os.path.join(self.UNET_DIR, 'results')\n","        self.VISUALIZATIONS_DIR = os.path.join(self.UNET_DIR, 'visualizations')\n","\n","        # Type de modÃ¨le ('basic', 'film', 'cbam', 'droppath', 'film_cbam', 'all')\n","        self.MODEL_TYPE = 'film_cbam'\n","\n","        # ParamÃ¨tres spÃ©cifiques aux modÃ¨les avancÃ©s\n","        self.IN_CHANNELS = 1\n","        self.DROP_PATH_RATE = 0.1  # Taux de DropPath (pour 'droppath' et 'all')\n","\n","        # CrÃ©ation des rÃ©pertoires s'ils n'existent pas\n","        for dir_path in [\n","            self.CHECKPOINTS_DIR, self.LOGS_DIR,\n","            self.RESULTS_DIR, self.VISUALIZATIONS_DIR\n","        ]:\n","            os.makedirs(dir_path, exist_ok=True)\n","\n","        # ParamÃ¨tres du modÃ¨le et de l'entraÃ®nement\n","        self.TILE_SIZE = 256  # Taille des tuiles en pixels\n","        self.BATCH_SIZE = 64  # Taille des batchs pour l'entraÃ®nement\n","        self.THRESHOLDS = [10, 15, 20, 25, 30]  # Seuils de hauteur en mÃ¨tres\n","        self.EPOCHS = 50  # Nombre maximal d'Ã©poques\n","        self.LEARNING_RATE = 0.001  # Taux d'apprentissage initial\n","        self.DROPOUT_RATE = 0.2  # Taux de dropout pour la rÃ©gularisation\n","        self.TEST_SPLIT = 0.15  # Proportion de donnÃ©es pour le test\n","        self.VAL_SPLIT = 0.15  # Proportion de donnÃ©es pour la validation\n","\n","        # ParamÃ¨tres pour l'optimisation de l'entraÃ®nement\n","        self.NUM_WORKERS = 8  # Nombre de workers pour le DataLoader\n","        self.PIN_MEMORY = True  # Utiliser pin_memory pour accÃ©lÃ©rer le transfert vers GPU\n","        self.PREFETCH_FACTOR = 50\n","\n","        # ParamÃ¨tres pour le mixup et l'augmentation des donnÃ©es\n","        self.AUGMENTATION = True  # Activer l'augmentation des donnÃ©es\n","        self.MIXUP_ALPHA = 0.2  # ParamÃ¨tre alpha pour mixup (0 = dÃ©sactivÃ©)\n","\n","        # Optimisations pour le modÃ¨le\n","        self.USE_AMP = True  # Utiliser la prÃ©cision mixte automatique\n","        self.USE_GRADIENT_CHECKPOINTING = False  # Ã‰conomiser de la mÃ©moire GPU\n","\n","    def save_config(self, filepath=None):\n","        \"\"\"Sauvegarde la configuration actuelle dans un fichier JSON\"\"\"\n","        if filepath is None:\n","            filepath = os.path.join(self.LOGS_DIR, 'config.json')\n","\n","        # CrÃ©ation d'un dictionnaire de configuration\n","        config_dict = {k: v for k, v in self.__dict__.items() if not k.startswith('__') and not callable(v)}\n","\n","        # Conversion des chemins en chaÃ®nes de caractÃ¨res\n","        for k, v in config_dict.items():\n","            if isinstance(v, Path):\n","                config_dict[k] = str(v)\n","\n","        # Sauvegarde dans un fichier JSON\n","        with open(filepath, 'w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","        print(f\"Configuration sauvegardÃ©e dans {filepath}\")\n","\n","    def load_config(self, filepath):\n","        \"\"\"Charge une configuration Ã  partir d'un fichier JSON\"\"\"\n","        with open(filepath, 'r') as f:\n","            config_dict = json.load(f)\n","\n","        # Mise Ã  jour des attributs\n","        for k, v in config_dict.items():\n","            setattr(self, k, v)\n","\n","        print(f\"Configuration chargÃ©e depuis {filepath}\")\n","\n","# Initialisation de la configuration\n","config = Config()"]},{"cell_type":"markdown","metadata":{"id":"B0kAtyf3uh6T"},"source":["## SECTION 3: DÃ‰FINITION DES MÃ‰TRIQUES ET UTILITAIRES"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1741116990915,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"R5eCF44Qil0l"},"outputs":[],"source":["# =====================================================================\n","# SECTION 3: DÃ‰FINITION DES MÃ‰TRIQUES ET UTILITAIRES\n","# =====================================================================\n","\n","class SegmentationMetrics:\n","    \"\"\"\n","    Classe pour calculer diverses mÃ©triques d'Ã©valuation pour la segmentation sÃ©mantique.\n","    Inclut: Accuracy, Precision, Recall, F1-score, IoU, mIoU, Kappa, etc.\n","    \"\"\"\n","    def __init__(self, device=None):\n","        \"\"\"\n","        Initialise l'objet de mÃ©triques.\n","\n","        Args:\n","            device: PÃ©riphÃ©rique (CPU ou GPU) pour les calculs\n","        \"\"\"\n","        self.device = device\n","        self.reset()\n","\n","    def reset(self):\n","        \"\"\"RÃ©initialise tous les compteurs.\"\"\"\n","        self.tp = 0  # True Positives\n","        self.fp = 0  # False Positives\n","        self.tn = 0  # True Negatives\n","        self.fn = 0  # False Negatives\n","\n","        # Pour les moyennes par image\n","        self.precision_sum = 0.0\n","        self.recall_sum = 0.0\n","        self.f1_sum = 0.0\n","        self.iou_sum = 0.0\n","        self.image_count = 0\n","\n","        # Pour les mÃ©triques par seuil\n","        self.metrics_by_threshold = {}\n","\n","    def update(self, pred, target, threshold=0.5):\n","        \"\"\"\n","        Met Ã  jour les compteurs avec les nouvelles prÃ©dictions.\n","\n","        Args:\n","            pred: PrÃ©dictions du modÃ¨le (aprÃ¨s sigmoid) (B, 1, H, W)\n","            target: VÃ©ritÃ© terrain (0 ou 1) (B, 1, H, W)\n","            threshold: Seuil pour binariser les prÃ©dictions\n","        \"\"\"\n","        # S'assurer que les tenseurs sont au bon format\n","        if pred.dim() == 3:\n","            pred = pred.unsqueeze(1)\n","        if target.dim() == 3:\n","            target = target.unsqueeze(1)\n","\n","        # Binariser les prÃ©dictions\n","        pred_binary = (torch.sigmoid(pred) > threshold).float()\n","\n","        # Calculer les mÃ©triques pour le batch actuel\n","        batch_tp = (pred_binary * target).sum().item()\n","        batch_fp = (pred_binary * (1 - target)).sum().item()\n","        batch_tn = ((1 - pred_binary) * (1 - target)).sum().item()\n","        batch_fn = ((1 - pred_binary) * target).sum().item()\n","\n","        # Mettre Ã  jour les compteurs globaux\n","        self.tp += batch_tp\n","        self.fp += batch_fp\n","        self.tn += batch_tn\n","        self.fn += batch_fn\n","\n","        # Calculer les mÃ©triques pour chaque image du batch\n","        for i in range(pred.size(0)):\n","            img_pred = pred_binary[i]\n","            img_target = target[i]\n","\n","            img_tp = (img_pred * img_target).sum().item()\n","            img_fp = (img_pred * (1 - img_target)).sum().item()\n","            img_tn = ((1 - img_pred) * (1 - img_target)).sum().item()\n","            img_fn = ((1 - img_pred) * img_target).sum().item()\n","\n","            # Calculer precision, recall, F1, IoU pour cette image\n","            img_precision = img_tp / (img_tp + img_fp + 1e-7)\n","            img_recall = img_tp / (img_tp + img_fn + 1e-7)\n","            img_f1 = 2 * img_precision * img_recall / (img_precision + img_recall + 1e-7)\n","            img_iou = img_tp / (img_tp + img_fp + img_fn + 1e-7)\n","\n","            # Ajouter aux sommes pour calculer les moyennes plus tard\n","            self.precision_sum += img_precision\n","            self.recall_sum += img_recall\n","            self.f1_sum += img_f1\n","            self.iou_sum += img_iou\n","            self.image_count += 1\n","\n","    def update_by_threshold(self, pred, target, threshold_value):\n","        \"\"\"\n","        Met Ã  jour les mÃ©triques spÃ©cifiques Ã  un seuil de hauteur.\n","\n","        Args:\n","            pred: PrÃ©dictions du modÃ¨le\n","            target: VÃ©ritÃ© terrain\n","            threshold_value: Valeur du seuil de hauteur (en mÃ¨tres)\n","        \"\"\"\n","        # Initialiser les compteurs pour ce seuil s'ils n'existent pas\n","        if threshold_value not in self.metrics_by_threshold:\n","            self.metrics_by_threshold[threshold_value] = {\n","                'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0,\n","                'precision_sum': 0, 'recall_sum': 0,\n","                'f1_sum': 0, 'iou_sum': 0, 'image_count': 0\n","            }\n","\n","        # Binariser les prÃ©dictions\n","        pred_binary = (torch.sigmoid(pred) > 0.5).float()\n","\n","        # Calculer les mÃ©triques\n","        batch_tp = (pred_binary * target).sum().item()\n","        batch_fp = (pred_binary * (1 - target)).sum().item()\n","        batch_tn = ((1 - pred_binary) * (1 - target)).sum().item()\n","        batch_fn = ((1 - pred_binary) * target).sum().item()\n","\n","        # Mettre Ã  jour les compteurs pour ce seuil\n","        metrics = self.metrics_by_threshold[threshold_value]\n","        metrics['tp'] += batch_tp\n","        metrics['fp'] += batch_fp\n","        metrics['tn'] += batch_tn\n","        metrics['fn'] += batch_fn\n","\n","        # Calculer les mÃ©triques pour chaque image du batch\n","        for i in range(pred.size(0)):\n","            img_pred = pred_binary[i]\n","            img_target = target[i]\n","\n","            img_tp = (img_pred * img_target).sum().item()\n","            img_fp = (img_pred * (1 - img_target)).sum().item()\n","            img_tn = ((1 - img_pred) * (1 - img_target)).sum().item()\n","            img_fn = ((1 - img_pred) * img_target).sum().item()\n","\n","            # Calculer precision, recall, F1, IoU pour cette image\n","            img_precision = img_tp / (img_tp + img_fp + 1e-7)\n","            img_recall = img_tp / (img_tp + img_fn + 1e-7)\n","            img_f1 = 2 * img_precision * img_recall / (img_precision + img_recall + 1e-7)\n","            img_iou = img_tp / (img_tp + img_fp + img_fn + 1e-7)\n","\n","            # Ajouter aux sommes\n","            metrics['precision_sum'] += img_precision\n","            metrics['recall_sum'] += img_recall\n","            metrics['f1_sum'] += img_f1\n","            metrics['iou_sum'] += img_iou\n","            metrics['image_count'] += 1\n","\n","    def compute(self):\n","        \"\"\"\n","        Calcule toutes les mÃ©triques Ã  partir des compteurs.\n","\n","        Returns:\n","            Dictionnaire de mÃ©triques\n","        \"\"\"\n","        # Ã‰viter division par zÃ©ro\n","        smooth = 1e-7\n","\n","        # MÃ©triques globales\n","        accuracy = (self.tp + self.tn) / (self.tp + self.fp + self.tn + self.fn + smooth)\n","        precision = self.tp / (self.tp + self.fp + smooth)\n","        recall = self.tp / (self.tp + self.fn + smooth)\n","        f1_score = 2 * precision * recall / (precision + recall + smooth)\n","        iou = self.tp / (self.tp + self.fp + self.fn + smooth)\n","\n","        # Balanced Accuracy (moyenne de la sensibilitÃ© et spÃ©cificitÃ©)\n","        sensitivity = recall  # TPR = TP / (TP + FN)\n","        specificity = self.tn / (self.tn + self.fp + smooth)\n","        balanced_accuracy = (sensitivity + specificity) / 2\n","\n","        # Coefficient Kappa\n","        p_o = accuracy\n","        p_e = (((self.tp + self.fn) * (self.tp + self.fp)) +\n","               ((self.tn + self.fp) * (self.tn + self.fn))) / ((self.tp + self.fp + self.tn + self.fn) ** 2 + smooth)\n","        kappa = (p_o - p_e) / (1 - p_e + smooth)\n","\n","        # Moyennes par image\n","        precision_mean = self.precision_sum / (self.image_count + smooth)\n","        recall_mean = self.recall_sum / (self.image_count + smooth)\n","        f1_mean = self.f1_sum / (self.image_count + smooth)\n","        iou_mean = self.iou_sum / (self.image_count + smooth)\n","\n","        # MÃ©triques par seuil\n","        threshold_metrics = {}\n","        for threshold, metrics in self.metrics_by_threshold.items():\n","            tp, fp, tn, fn = metrics['tp'], metrics['fp'], metrics['tn'], metrics['fn']\n","            total = tp + fp + tn + fn + smooth\n","\n","            t_accuracy = (tp + tn) / total\n","            t_precision = tp / (tp + fp + smooth)\n","            t_recall = tp / (tp + fn + smooth)\n","            t_f1_score = 2 * t_precision * t_recall / (t_precision + t_recall + smooth)\n","            t_iou = tp / (tp + fp + fn + smooth)\n","\n","            # Moyennes par image pour ce seuil\n","            img_count = metrics['image_count'] + smooth\n","            t_precision_mean = metrics['precision_sum'] / img_count\n","            t_recall_mean = metrics['recall_sum'] / img_count\n","            t_f1_mean = metrics['f1_sum'] / img_count\n","            t_iou_mean = metrics['iou_sum'] / img_count\n","\n","            threshold_metrics[threshold] = {\n","                'accuracy': t_accuracy,\n","                'precision': t_precision,\n","                'recall': t_recall,\n","                'f1_score': t_f1_score,\n","                'iou': t_iou,\n","                'precision_mean': t_precision_mean,\n","                'recall_mean': t_recall_mean,\n","                'f1_mean': t_f1_mean,\n","                'iou_mean': t_iou_mean\n","            }\n","\n","        # RÃ©sultats finaux\n","        return {\n","            # MÃ©triques globales\n","            'accuracy': accuracy,\n","            'balanced_accuracy': balanced_accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1_score': f1_score,\n","            'iou': iou,\n","            'kappa': kappa,\n","\n","            # Moyennes par image\n","            'precision_mean': precision_mean,\n","            'recall_mean': recall_mean,\n","            'f1_mean': f1_mean,\n","            'iou_mean': iou_mean,\n","\n","            # MÃ©triques par seuil\n","            'threshold_metrics': threshold_metrics\n","        }\n","\n","    def compute_confusion_matrix(self):\n","        \"\"\"\n","        Construit la matrice de confusion.\n","\n","        Returns:\n","            Dictionnaire avec la matrice de confusion et les mÃ©triques dÃ©rivÃ©es\n","        \"\"\"\n","        confusion_matrix = {\n","            'tp': self.tp,\n","            'fp': self.fp,\n","            'tn': self.tn,\n","            'fn': self.fn\n","        }\n","\n","        # Totaliser les pixels\n","        total_pixels = self.tp + self.fp + self.tn + self.fn\n","\n","        # Calculer les pourcentages\n","        confusion_percentages = {\n","            'tp_percent': self.tp / total_pixels * 100 if total_pixels > 0 else 0,\n","            'fp_percent': self.fp / total_pixels * 100 if total_pixels > 0 else 0,\n","            'tn_percent': self.tn / total_pixels * 100 if total_pixels > 0 else 0,\n","            'fn_percent': self.fn / total_pixels * 100 if total_pixels > 0 else 0\n","        }\n","\n","        return {\n","            'matrix': confusion_matrix,\n","            'percentages': confusion_percentages,\n","            'total_pixels': total_pixels\n","        }\n","\n","\n","def add_metrics_to_tensorboard(writer, metrics, epoch, prefix=''):\n","    \"\"\"\n","    Ajoute toutes les mÃ©triques Ã  TensorBoard.\n","\n","    Args:\n","        writer: SummaryWriter de TensorBoard\n","        metrics: Dictionnaire de mÃ©triques retournÃ© par SegmentationMetrics.compute()\n","        epoch: Ã‰poque courante\n","        prefix: PrÃ©fixe pour les noms des mÃ©triques (ex: 'val/')\n","    \"\"\"\n","    # MÃ©triques globales\n","    for metric_name in ['accuracy', 'balanced_accuracy', 'precision', 'recall',\n","                        'f1_score', 'iou', 'kappa']:\n","        if metric_name in metrics:\n","            writer.add_scalar(f'{prefix}{metric_name}', metrics[metric_name], epoch)\n","\n","    # Moyennes par image\n","    for metric_name in ['precision_mean', 'recall_mean', 'f1_mean', 'iou_mean']:\n","        if metric_name in metrics:\n","            writer.add_scalar(f'{prefix}{metric_name}', metrics[metric_name], epoch)\n","\n","    # MÃ©triques par seuil\n","    if 'threshold_metrics' in metrics:\n","        for threshold, t_metrics in metrics['threshold_metrics'].items():\n","            for t_metric_name, t_metric_value in t_metrics.items():\n","                writer.add_scalar(\n","                    f'{prefix}threshold_{threshold}/{t_metric_name}',\n","                    t_metric_value,\n","                    epoch\n","                )\n","\n","def iou_metric(pred, target, smooth=1e-6):\n","    \"\"\"\n","    Calcule l'Intersection over Union (IoU) entre deux masques binaires.\n","    Version robuste qui gÃ¨re diffÃ©rentes dimensions de tenseurs.\n","\n","    Args:\n","        pred: Tensor des prÃ©dictions (B, 1, H, W) ou (1, H, W)\n","        target: Tensor des cibles (B, 1, H, W) ou (1, H, W)\n","        smooth: Valeur pour Ã©viter la division par zÃ©ro\n","\n","    Returns:\n","        Valeur IoU moyenne sur le batch\n","    \"\"\"\n","    # VÃ©rifier les dimensions et les adapter si nÃ©cessaire\n","    if pred.dim() == 3:\n","        pred = pred.unsqueeze(0)  # Ajouter dimension batch\n","    if target.dim() == 3:\n","        target = target.unsqueeze(0)  # Ajouter dimension batch\n","\n","    # Assurons-nous que les deux tenseurs ont la mÃªme forme\n","    if pred.shape != target.shape:\n","        print(f\"Avertissement: Les formes des tenseurs ne correspondent pas: pred {pred.shape}, target {target.shape}\")\n","\n","        # Tentative d'adaptation des dimensions\n","        if pred.dim() == 4 and target.dim() == 4:\n","            if pred.shape[0] != target.shape[0]:\n","                # Si les tailles de batch diffÃ¨rent, utiliser la plus petite\n","                min_batch_size = min(pred.shape[0], target.shape[0])\n","                pred = pred[:min_batch_size]\n","                target = target[:min_batch_size]\n","\n","    # Binariser les prÃ©dictions\n","    pred_binary = (torch.sigmoid(pred) > 0.5).float()\n","\n","    # Calculer l'intersection et l'union en fonction du nombre de dimensions\n","    if pred.dim() == 4:\n","        # Pour tenseurs 4D (avec batch)\n","        # DÃ©terminer les dimensions sur lesquelles sommer\n","        sum_dims = tuple(range(1, pred.dim()))\n","\n","        intersection = (pred_binary * target).sum(dim=sum_dims)\n","        union = pred_binary.sum(dim=sum_dims) + target.sum(dim=sum_dims) - intersection\n","    else:\n","        # Pour tenseurs 3D (sans batch)\n","        intersection = (pred_binary * target).sum()\n","        union = pred_binary.sum() + target.sum() - intersection\n","\n","    # Calculer IoU pour chaque Ã©lÃ©ment du batch\n","    iou = (intersection + smooth) / (union + smooth)\n","\n","    # Moyenne sur le batch\n","    return iou.mean()\n","\n","class CombinedFocalDiceLoss(nn.Module):\n","    \"\"\"\n","    Fonction de perte combinant BCE-Focal Loss et Dice Loss, adaptative au seuil de hauteur.\n","    - BCE-Focal: Donne moins de poids aux exemples faciles Ã  classifier\n","    - Dice Loss: Se concentre sur le recouvrement entre prÃ©dictions et masques\n","    - Adaptation au seuil: Ajuste la pondÃ©ration selon le dÃ©sÃ©quilibre des classes par seuil\n","    \"\"\"\n","    def __init__(self, alpha=0.5, gamma=2.0, smooth=1e-6, threshold_weights=None):\n","        \"\"\"\n","        Args:\n","            alpha: Ã‰quilibre entre BCE et Dice (0.5 = poids Ã©gal)\n","            gamma: Facteur focal pour rÃ©duire l'impact des exemples faciles\n","            smooth: Valeur pour Ã©viter division par zÃ©ro\n","            threshold_weights: Dictionnaire {seuil normalisÃ©: poids} (calculÃ© automatiquement si None)\n","        \"\"\"\n","        super(CombinedFocalDiceLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.smooth = smooth\n","        self.threshold_weights = threshold_weights\n","\n","    def forward(self, pred, target, threshold=None):\n","        # VÃ©rifier les entrÃ©es\n","        if torch.isnan(pred).any():\n","            print(\"NaN dÃ©tectÃ© dans les prÃ©dictions\")\n","            return torch.tensor(1.0, requires_grad=True, device=pred.device)\n","\n","        # Application de sigmoid avec protection numÃ©rique renforcÃ©e\n","        pred_sigmoid = torch.sigmoid(pred)\n","        epsilon = 1e-7\n","        pred_sigmoid = torch.clamp(pred_sigmoid, epsilon, 1.0 - epsilon)\n","\n","        # Calcul de BCE standard avec meilleure stabilitÃ© numÃ©rique\n","        bce = -target * torch.log(pred_sigmoid) - (1 - target) * torch.log(1 - pred_sigmoid)\n","\n","        # Protection renforcÃ©e contre les valeurs extrÃªmes\n","        bce = torch.clamp(bce, 0, 50)  # Limiter les valeurs extrÃªmes\n","\n","        # Terme focal\n","        pt = target * pred_sigmoid + (1 - target) * (1 - pred_sigmoid)\n","        focal_weight = (1 - pt) ** self.gamma\n","\n","        # Utiliser une normalisation plus stable\n","        focal_bce = focal_weight * bce\n","        focal_bce_mean = torch.mean(focal_bce)\n","\n","        # VÃ©rification avant de multiplier par threshold_weight\n","        if torch.isnan(focal_bce_mean) or torch.isinf(focal_bce_mean):\n","            print(\"NaN ou Inf dÃ©tectÃ© dans focal_bce_mean, utilisation du dice_loss uniquement\")\n","            focal_bce_mean = 0.0  # Utiliser 0 pour ignorer cette composante\n","        else:\n","            # Appliquer le poids du seuil si nÃ©cessaire\n","            if threshold is not None and self.threshold_weights is not None:\n","                t_value = threshold[0].item()\n","                closest_t = min(self.threshold_weights.keys(), key=lambda x: abs(x - t_value))\n","                threshold_weight = self.threshold_weights[closest_t]\n","                # Limiter les poids extrÃªmes qui pourraient causer des NaN\n","                threshold_weight = min(max(threshold_weight, 0.1), 10.0)\n","                focal_bce_mean = focal_bce_mean * threshold_weight\n","\n","        # Calcul de Dice Loss avec protection numÃ©rique renforcÃ©e\n","        pred_flat = pred_sigmoid.view(-1)\n","        target_flat = target.view(-1)\n","\n","        intersection = (pred_flat * target_flat).sum() + self.smooth\n","        union = pred_flat.sum() + target_flat.sum() + self.smooth * 2\n","\n","        dice_loss = 1 - (2. * intersection) / union\n","\n","        # Appliquer threshold_weight au dice_loss\n","        if threshold is not None and self.threshold_weights is not None:\n","            t_value = threshold[0].item()\n","            closest_t = min(self.threshold_weights.keys(), key=lambda x: abs(x - t_value))\n","            threshold_weight = self.threshold_weights[closest_t]\n","            threshold_weight = min(max(threshold_weight, 0.1), 10.0)\n","            dice_loss = dice_loss * threshold_weight\n","\n","        # Combinaison pondÃ©rÃ©e avec vÃ©rification\n","        combined_loss = self.alpha * focal_bce_mean + (1 - self.alpha) * dice_loss\n","\n","        if torch.isnan(combined_loss):\n","            print(f\"NaN dÃ©tectÃ© dans la perte - focal_bce: {focal_bce_mean.item() if not isinstance(focal_bce_mean, float) else focal_bce_mean}, dice_loss: {dice_loss.item()}\")\n","            # Utiliser seulement dice_loss si combined_loss est NaN\n","            return dice_loss\n","\n","        return combined_loss\n","\n","def create_threshold_weights(config, threshold_stats=None):\n","    \"\"\"\n","    Calcule les poids pour chaque seuil en fonction du dÃ©sÃ©quilibre des classes.\n","\n","    Args:\n","        config: Configuration avec les seuils\n","        threshold_stats: Optionnel - statistiques {seuil: {pixels_trouees, total_pixels}}\n","                        Si None, utilise des valeurs approximatives basÃ©es sur les donnÃ©es\n","\n","    Returns:\n","        Dictionnaire {seuil_normalisÃ©: poids}\n","    \"\"\"\n","    max_threshold = max(config.THRESHOLDS)\n","    weights = {}\n","\n","    # Si aucune statistique n'est fournie, utiliser des valeurs approximatives\n","    # basÃ©es sur les donnÃ©es prÃ©sentÃ©es dans le prompt\n","    if threshold_stats is None:\n","        threshold_stats = {\n","            10: {'pixels_trouees': 0.15, 'total_pixels': 1.0},  # ~15% trouÃ©es\n","            15: {'pixels_trouees': 0.30, 'total_pixels': 1.0},  # ~30% trouÃ©es\n","            20: {'pixels_trouees': 0.50, 'total_pixels': 1.0},  # ~50% trouÃ©es\n","            25: {'pixels_trouees': 0.70, 'total_pixels': 1.0},  # ~70% trouÃ©es\n","            30: {'pixels_trouees': 0.85, 'total_pixels': 1.0}   # ~85% trouÃ©es\n","        }\n","\n","    for threshold, stats in threshold_stats.items():\n","        if isinstance(stats, dict) and 'pixels_trouees' in stats and 'total_pixels' in stats:\n","            ratio_trouees = stats['pixels_trouees'] / stats['total_pixels']\n","        else:\n","            # Cas oÃ¹ stats est directement le ratio\n","            ratio_trouees = stats\n","\n","        # Calcul du poids pour Ã©quilibrer les classes\n","        # Plus une classe est rare, plus son poids est Ã©levÃ©\n","        if ratio_trouees > 0.5:\n","            # Favoriser les pixels non-trouÃ©es (minoritaires)\n","            weight = ratio_trouees / (1 - ratio_trouees + 1e-6)\n","        else:\n","            # Favoriser les pixels trouÃ©es (minoritaires)\n","            weight = (1 - ratio_trouees) / (ratio_trouees + 1e-6)\n","\n","        # Normaliser pour Ã©viter des poids extrÃªmes\n","        weight = min(weight, 10.0)\n","\n","        # Stocker avec le seuil normalisÃ©\n","        weights[threshold / max_threshold] = weight\n","\n","    return weights\n","\n","class LossTracker:\n","    \"\"\"\n","    Classe pour suivre et enregistrer les pertes et mÃ©triques pendant l'entraÃ®nement.\n","    Version amÃ©liorÃ©e avec intÃ©gration TensorBoard et gestion de mÃ©triques multiples.\n","    \"\"\"\n","    def __init__(self, log_dir=None):\n","        # MÃ©triques de base\n","        self.train_losses = []\n","        self.val_losses = []\n","        self.train_iou = []\n","        self.val_iou = []\n","        self.lr_history = []\n","\n","        # MÃ©triques supplÃ©mentaires (initialisÃ©es dynamiquement)\n","        self.metric_names = ['iou']  # IoU est toujours prÃ©sent\n","\n","        # Meilleures performances\n","        self.best_val_loss = float('inf')\n","        self.best_val_iou = 0.0\n","        self.best_metrics = {}  # Pour stocker les meilleures valeurs de chaque mÃ©trique\n","\n","        # Compteur pour l'early stopping\n","        self.epochs_without_improvement = 0\n","        self.log_file = None\n","\n","        # Initialisation du writer TensorBoard\n","        self.writer = SummaryWriter(log_dir=log_dir if log_dir else os.path.join(config.LOGS_DIR, 'tensorboard'))\n","        print(f\"TensorBoard initialisÃ© dans: {self.writer.log_dir}. Pour visualiser, exÃ©cutez: tensorboard --logdir={log_dir}\")\n","\n","        # Dictionnaire pour stocker les mÃ©triques par seuil\n","        self.threshold_metrics = {}\n","\n","        # Historique des mÃ©triques complÃ¨tes\n","        self.metrics_history = {\n","            'train': {},\n","            'val': {}\n","        }\n","\n","    def set_log_file(self, filepath):\n","        \"\"\"DÃ©finit le fichier de log textuel\"\"\"\n","        self.log_file = filepath\n","\n","        # CrÃ©er l'en-tÃªte du fichier de log avec toutes les mÃ©triques\n","        header = \"Epoch,Train_Loss,Val_Loss\"\n","\n","        # Ajouter toutes les mÃ©triques au header\n","        for metric in self.metric_names:\n","            header += f\",Train_{metric},Val_{metric}\"\n","\n","        header += \",Learning_Rate\\n\"\n","\n","        with open(self.log_file, 'w') as f:\n","            f.write(header)\n","\n","    def add_metric(self, metric_name):\n","        \"\"\"\n","        Ajoute une nouvelle mÃ©trique Ã  suivre.\n","\n","        Args:\n","            metric_name: Nom de la mÃ©trique\n","        \"\"\"\n","        if metric_name not in self.metric_names:\n","            self.metric_names.append(metric_name)\n","\n","            # Initialiser les listes pour cette mÃ©trique\n","            setattr(self, f'train_{metric_name}', [])\n","            setattr(self, f'val_{metric_name}', [])\n","\n","            # Initialiser la meilleure valeur (supposer que plus grand est meilleur)\n","            self.best_metrics[metric_name] = 0.0\n","\n","            # Initialiser l'historique\n","            self.metrics_history['train'][metric_name] = []\n","            self.metrics_history['val'][metric_name] = []\n","\n","    def update(self, train_metrics, val_metrics, lr):\n","        \"\"\"\n","        Met Ã  jour les mÃ©triques aprÃ¨s une Ã©poque.\n","\n","        Args:\n","            train_metrics: Dictionnaire de mÃ©triques d'entraÃ®nement\n","            val_metrics: Dictionnaire de mÃ©triques de validation\n","            lr: Taux d'apprentissage actuel\n","\n","        Returns:\n","            Boolean indiquant si une amÃ©lioration a Ã©tÃ© observÃ©e\n","        \"\"\"\n","        # Mettre Ã  jour les pertes (toujours prÃ©sentes)\n","        self.train_losses.append(train_metrics['loss'])\n","        self.val_losses.append(val_metrics['loss'])\n","        self.lr_history.append(lr)\n","\n","        # Mettre Ã  jour les autres mÃ©triques\n","        for metric_name in self.metric_names:\n","            if metric_name in train_metrics and metric_name in val_metrics:\n","                train_list = getattr(self, f'train_{metric_name}')\n","                val_list = getattr(self, f'val_{metric_name}')\n","\n","                train_list.append(train_metrics[metric_name])\n","                val_list.append(val_metrics[metric_name])\n","\n","                # Mettre Ã  jour l'historique\n","                self.metrics_history['train'].setdefault(metric_name, []).append(train_metrics[metric_name])\n","                self.metrics_history['val'].setdefault(metric_name, []).append(val_metrics[metric_name])\n","\n","        # Mettre Ã  jour les mÃ©triques par seuil\n","        if 'threshold_metrics' in val_metrics:\n","            self.threshold_metrics = val_metrics['threshold_metrics']\n","\n","        # Ã‰poque actuelle (0-indexed, donc +1 pour l'affichage)\n","        epoch = len(self.train_losses)\n","\n","        # Enregistrer dans TensorBoard\n","        self.writer.add_scalar('Loss/train', train_metrics['loss'], epoch)\n","        self.writer.add_scalar('Loss/val', val_metrics['loss'], epoch)\n","\n","        for metric_name in self.metric_names:\n","            if metric_name in train_metrics and metric_name in val_metrics:\n","                self.writer.add_scalar(f'{metric_name.capitalize()}/train', train_metrics[metric_name], epoch)\n","                self.writer.add_scalar(f'{metric_name.capitalize()}/val', val_metrics[metric_name], epoch)\n","\n","                # Enregistrer la diffÃ©rence entre entraÃ®nement et validation\n","                if train_metrics[metric_name] is not None and val_metrics[metric_name] is not None:\n","                    self.writer.add_scalar(\n","                        f'{metric_name.capitalize()}/train_val_gap',\n","                        train_metrics[metric_name] - val_metrics[metric_name],\n","                        epoch\n","                    )\n","\n","        self.writer.add_scalar('LearningRate', lr, epoch)\n","\n","        # VÃ©rifier si c'est la meilleure performance\n","        improved = False\n","\n","        # VÃ©rifier la perte\n","        if val_metrics['loss'] < self.best_val_loss:\n","            self.writer.add_scalar('BestMetrics/val_loss', val_metrics['loss'], epoch)\n","            self.best_val_loss = val_metrics['loss']\n","            improved = True\n","\n","        # VÃ©rifier les autres mÃ©triques\n","        for metric_name in self.metric_names:\n","            if metric_name in val_metrics:\n","                val_value = val_metrics[metric_name]\n","                best_value = self.best_metrics.get(metric_name, 0.0)\n","\n","                # Supposer que plus grand est meilleur\n","                if val_value > best_value:\n","                    self.writer.add_scalar(f'BestMetrics/val_{metric_name}', val_value, epoch)\n","                    self.best_metrics[metric_name] = val_value\n","\n","                    # Cas particulier pour l'IoU\n","                    if metric_name == 'iou':\n","                        self.best_val_iou = val_value\n","\n","                    improved = True\n","\n","        # Mettre Ã  jour le compteur pour early stopping\n","        if improved:\n","            self.epochs_without_improvement = 0\n","        else:\n","            self.epochs_without_improvement += 1\n","\n","        # Ajouter l'Ã©criture dans le fichier de log\n","        if self.log_file:\n","            with open(self.log_file, 'a') as f:\n","                log_line = f\"{epoch},{train_metrics['loss']:.6f},{val_metrics['loss']:.6f}\"\n","\n","                for metric_name in self.metric_names:\n","                    if metric_name in train_metrics and metric_name in val_metrics:\n","                        log_line += f\",{train_metrics[metric_name]:.6f},{val_metrics[metric_name]:.6f}\"\n","                    else:\n","                        log_line += \",N/A,N/A\"\n","\n","                log_line += f\",{lr:.6f}\\n\"\n","                f.write(log_line)\n","\n","        # Enregistrer les mÃ©triques par seuil\n","        if 'threshold_metrics' in val_metrics:\n","            for threshold, t_metrics in val_metrics['threshold_metrics'].items():\n","                for metric_name, value in t_metrics.items():\n","                    self.writer.add_scalar(\n","                        f'Threshold_{threshold}/{metric_name}',\n","                        value,\n","                        epoch\n","                    )\n","\n","        return improved\n","\n","    def save(self, filepath):\n","        \"\"\"Sauvegarde les mÃ©triques dans un fichier\"\"\"\n","        metrics = {\n","            'train_losses': self.train_losses,\n","            'val_losses': self.val_losses,\n","            'lr_history': self.lr_history,\n","            'best_val_loss': self.best_val_loss,\n","            'metrics_history': self.metrics_history,\n","            'best_metrics': self.best_metrics,\n","            'threshold_metrics': self.threshold_metrics,\n","            'metric_names': self.metric_names\n","        }\n","\n","        # Ajouter toutes les listes de mÃ©triques\n","        for metric_name in self.metric_names:\n","            train_key = f'train_{metric_name}'\n","            val_key = f'val_{metric_name}'\n","\n","            if hasattr(self, train_key) and hasattr(self, val_key):\n","                metrics[train_key] = getattr(self, train_key)\n","                metrics[val_key] = getattr(self, val_key)\n","\n","        with open(filepath, 'wb') as f:\n","            pickle.dump(metrics, f)\n","\n","        # Fermer proprement le writer TensorBoard\n","        self.writer.close()\n","\n","        print(f\"MÃ©triques sauvegardÃ©es dans {filepath}\")\n","\n","    def load(self, filepath):\n","        \"\"\"Charge les mÃ©triques depuis un fichier\"\"\"\n","        with open(filepath, 'rb') as f:\n","            metrics = pickle.load(f)\n","\n","        # Charger les mÃ©triques de base\n","        self.train_losses = metrics['train_losses']\n","        self.val_losses = metrics['val_losses']\n","        self.lr_history = metrics['lr_history']\n","        self.best_val_loss = metrics['best_val_loss']\n","\n","        # Charger les mÃ©triques additionnelles\n","        if 'metric_names' in metrics:\n","            self.metric_names = metrics['metric_names']\n","\n","        if 'metrics_history' in metrics:\n","            self.metrics_history = metrics['metrics_history']\n","\n","        if 'best_metrics' in metrics:\n","            self.best_metrics = metrics['best_metrics']\n","\n","        if 'threshold_metrics' in metrics:\n","            self.threshold_metrics = metrics['threshold_metrics']\n","\n","        # Charger toutes les listes de mÃ©triques\n","        for metric_name in self.metric_names:\n","            train_key = f'train_{metric_name}'\n","            val_key = f'val_{metric_name}'\n","\n","            if train_key in metrics and val_key in metrics:\n","                setattr(self, train_key, metrics[train_key])\n","                setattr(self, val_key, metrics[val_key])\n","\n","                # Cas particulier pour l'IoU\n","                if metric_name == 'iou' and 'best_val_iou' in metrics:\n","                    self.best_val_iou = metrics['best_val_iou']\n","                elif metric_name == 'iou' and 'iou' in self.best_metrics:\n","                    self.best_val_iou = self.best_metrics['iou']\n","\n","        # Re-enregistrer les mÃ©triques chargÃ©es dans TensorBoard\n","        for epoch, (tl, vl, lr) in enumerate(zip(\n","            self.train_losses, self.val_losses, self.lr_history\n","        ), 1):\n","            self.writer.add_scalar('Loss/train', tl, epoch)\n","            self.writer.add_scalar('Loss/val', vl, epoch)\n","            self.writer.add_scalar('LearningRate', lr, epoch)\n","\n","            for metric_name in self.metric_names:\n","                train_key = f'train_{metric_name}'\n","                val_key = f'val_{metric_name}'\n","\n","                if hasattr(self, train_key) and hasattr(self, val_key):\n","                    train_values = getattr(self, train_key)\n","                    val_values = getattr(self, val_key)\n","\n","                    if epoch-1 < len(train_values) and epoch-1 < len(val_values):\n","                        self.writer.add_scalar(f'{metric_name.capitalize()}/train', train_values[epoch-1], epoch)\n","                        self.writer.add_scalar(f'{metric_name.capitalize()}/val', val_values[epoch-1], epoch)\n","\n","        print(f\"MÃ©triques chargÃ©es depuis {filepath}\")\n","\n","    def plot(self, save_path=None):\n","        \"\"\"\n","        MÃ©thode pour visualiser l'Ã©volution des mÃ©triques.\n","        \"\"\"\n","        plt.figure(figsize=(15, 10))\n","\n","        # Graphique des pertes\n","        plt.subplot(2, 2, 1)\n","        plt.plot(self.train_losses, label='Train')\n","        plt.plot(self.val_losses, label='Validation')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.title('Evolution de la fonction de perte')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        # Graphique de l'IoU (et autres mÃ©triques si disponibles)\n","        available_metrics = [m for m in self.metric_names if hasattr(self, f'train_{m}')]\n","\n","        if available_metrics:\n","            metric_name = available_metrics[0]  # Prendre la premiÃ¨re mÃ©trique disponible (habituellement IoU)\n","            train_values = getattr(self, f'train_{metric_name}')\n","            val_values = getattr(self, f'val_{metric_name}')\n","\n","            plt.subplot(2, 2, 2)\n","            plt.plot(train_values, label='Train')\n","            plt.plot(val_values, label='Validation')\n","            plt.xlabel('Epoch')\n","            plt.ylabel(metric_name.upper())\n","            plt.title(f'Evolution de la mÃ©trique {metric_name.upper()}')\n","            plt.legend()\n","            plt.grid(True)\n","\n","        # Graphique du taux d'apprentissage\n","        plt.subplot(2, 2, 3)\n","        plt.plot(self.lr_history)\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Learning Rate')\n","        plt.title('Evolution du taux d\\'apprentissage')\n","        plt.grid(True)\n","        plt.yscale('log')\n","\n","        # RÃ©sumÃ© des meilleures performances\n","        plt.subplot(2, 2, 4)\n","        plt.axis('off')\n","        plt.text(0.1, 0.9, f\"Meilleure perte de validation: {self.best_val_loss:.6f}\")\n","\n","        y_pos = 0.8\n","        for metric_name, best_value in self.best_metrics.items():\n","            plt.text(0.1, y_pos, f\"Meilleur {metric_name.upper()} de validation: {best_value:.6f}\")\n","            y_pos -= 0.1\n","\n","        plt.text(0.1, 0.2, f\"Nombre d'Ã©poques: {len(self.train_losses)}\")\n","        plt.text(0.1, 0.1, f\"Pour des visualisations dÃ©taillÃ©es, utilisez TensorBoard\")\n","\n","        plt.tight_layout()\n","\n","        if save_path:\n","            plt.savefig(save_path, dpi=300)\n","            print(f\"Graphiques sauvegardÃ©s dans {save_path}\")\n","\n","        plt.show()\n","\n","        print(\"\\nPour des visualisations plus dÃ©taillÃ©es et interactives, utilisez TensorBoard:\")\n","        print(f\"tensorboard --logdir={self.writer.log_dir}\")\n","\n","        # Si nous avons plusieurs mÃ©triques, recommander l'utilisation de visualize_metrics_evolution\n","        if len(self.metric_names) > 1:\n","            print(\"\\nPour visualiser toutes les mÃ©triques, utilisez la fonction visualize_metrics_evolution(tracker)\")\n","\n","class TrainingControl:\n","    \"\"\"\n","    Classe pour contrÃ´ler l'exÃ©cution de l'entraÃ®nement (pause, arrÃªt, etc.).\n","    \"\"\"\n","    def __init__(self):\n","        self.stop_requested = False\n","        self.pause_requested = False\n","        self.is_paused = False\n","\n","    def request_stop(self):\n","        self.stop_requested = True\n","\n","    def request_pause(self):\n","        self.pause_requested = True\n","\n","    def resume(self):\n","        self.pause_requested = False\n","        self.is_paused = False\n","\n","    def check_and_handle_pause(self):\n","        if self.pause_requested and not self.is_paused:\n","            self.is_paused = True\n","            print(\"EntraÃ®nement en pause. Cliquez sur 'Reprendre' pour continuer.\")\n","            while self.pause_requested and not self.stop_requested:\n","                import time\n","                time.sleep(0.5)\n","            self.is_paused = False\n","            print(\"Reprise de l'entraÃ®nement...\")\n","\n","# CrÃ©er une instance globale\n","training_control = TrainingControl()"]},{"cell_type":"markdown","metadata":{"id":"6pGP5_cduoDp"},"source":["## SECTION 4: DÃ‰FINITION DU DATASET ET DU DATALOADER"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1741116990931,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"YdggBFiyisgx"},"outputs":[],"source":["# =====================================================================\n","# SECTION 4: DÃ‰FINITION DU DATASET ET DU DATALOADER\n","# =====================================================================\n","\n","class ForestGapTransforms:\n","    \"\"\"\n","    Classe amÃ©liorÃ©e pour les transformations des donnÃ©es DSM avec:\n","    - Plus d'angles de rotation\n","    - Support pour elastic deformation (externe)\n","    - Random crop and zoom\n","    \"\"\"\n","    def __init__(self, prob=0.7, is_train=True, advanced_aug_prob=0.3, enable_elastic=False):\n","        \"\"\"\n","        Initialise les transformations avec des probabilitÃ©s adaptÃ©es.\n","\n","        Args:\n","            prob: ProbabilitÃ© d'appliquer chaque augmentation de base\n","            is_train: Si True, applique des augmentations alÃ©atoires\n","            advanced_aug_prob: ProbabilitÃ© d'appliquer les augmentations avancÃ©es\n","            enable_elastic: Si True, active la dÃ©formation Ã©lastique\n","        \"\"\"\n","        self.prob = prob  # AugmentÃ© Ã  0.7 pour plus d'exposition aux augmentations\n","        self.is_train = is_train\n","        self.advanced_aug_prob = advanced_aug_prob\n","        self.enable_elastic = enable_elastic\n","\n","        # Plus d'angles pour les rotations (multiples de 15Â° pour limiter les distorsions)\n","        self.rotation_angles = [15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180,\n","                               195, 210, 225, 240, 255, 270, 285, 300, 315, 330, 345]\n","\n","        # Facteurs de zoom pour random crop and zoom\n","        self.zoom_factors = [0.8, 0.9, 1.0, 1.1, 1.2]  # 0.8 = zoom in, 1.2 = zoom out\n","\n","    def _random_crop_zoom(self, image, mask, zoom_factor):\n","        \"\"\"\n","        Effectue un crop alÃ©atoire puis redimensionne Ã  la taille originale.\n","        \"\"\"\n","        h, w = image.shape\n","\n","        # Calculer la taille du crop\n","        crop_height = int(h * zoom_factor)\n","        crop_width = int(w * zoom_factor)\n","\n","        # S'assurer que les dimensions sont valides\n","        crop_height = max(min(crop_height, h), h//2)\n","        crop_width = max(min(crop_width, w), w//2)\n","\n","        # Calculer les offsets pour le crop\n","        top = random.randint(0, h - crop_height)\n","        left = random.randint(0, w - crop_width)\n","\n","        # Extraire le crop\n","        image_crop = image[top:top + crop_height, left:left + crop_width]\n","        mask_crop = mask[top:top + crop_height, left:left + crop_width]\n","\n","        # Redimensionner Ã  la taille originale\n","        image_resized = cv2.resize(image_crop, (w, h), interpolation=cv2.INTER_LINEAR)\n","        mask_resized = cv2.resize(mask_crop, (w, h), interpolation=cv2.INTER_NEAREST)\n","\n","        return image_resized, mask_resized\n","\n","    def __call__(self, dsm, mask):\n","        \"\"\"\n","        Applique les transformations aux images DSM et aux masques.\n","        \"\"\"\n","        # Normalisation des donnÃ©es DSM\n","        dsm_valid = ~np.isnan(dsm)\n","        if np.any(dsm_valid):\n","            dsm_min = np.nanmin(dsm)\n","            dsm_max = np.nanmax(dsm)\n","            dsm_range = dsm_max - dsm_min\n","            if dsm_range > 0:\n","                dsm = np.where(dsm_valid, (dsm - dsm_min) / dsm_range, 0)\n","            else:\n","                dsm = np.zeros_like(dsm)\n","\n","        # PrÃ©parer le masque\n","        mask_valid = (mask != 255)\n","        mask_binary = np.where(mask_valid, (mask > 0).astype(np.float32), 0)\n","\n","        # Appliquer les augmentations avancÃ©es avant la conversion en tenseurs\n","        if self.is_train:\n","            # 1. Elastic dÃ©formation (globale) - Ã  appliquer AVANT la conversion en tenseurs\n","            if self.enable_elastic and random.random() < self.advanced_aug_prob:\n","                try:\n","                    # Appel Ã  la fonction globale elastic_transform\n","                    dsm, mask_binary = elastic_transform(dsm, mask_binary)\n","                except Exception as e:\n","                    print(f\"Attention: Erreur lors de la transformation Ã©lastique: {str(e)}\")\n","\n","            # 2. Random crop and zoom\n","            if random.random() < self.advanced_aug_prob:\n","                zoom_factor = random.choice(self.zoom_factors)\n","                dsm, mask_binary = self._random_crop_zoom(dsm, mask_binary, zoom_factor)\n","\n","        # Conversion en tenseurs PyTorch (aprÃ¨s toutes les augmentations numpy)\n","        dsm_tensor = torch.from_numpy(dsm).float().unsqueeze(0)\n","        mask_tensor = torch.from_numpy(mask_binary).float().unsqueeze(0)\n","\n","        # Appliquer les augmentations PyTorch seulement pendant l'entraÃ®nement\n","        if self.is_train:\n","            # 3. Rotation avec plus d'angles\n","            if random.random() < self.prob:\n","                angle = random.choice(self.rotation_angles)\n","                dsm_tensor = TF.rotate(dsm_tensor, angle)\n","                mask_tensor = TF.rotate(mask_tensor, angle)\n","\n","            # 4. Miroir horizontal\n","            if random.random() < self.prob:\n","                dsm_tensor = TF.hflip(dsm_tensor)\n","                mask_tensor = TF.hflip(mask_tensor)\n","\n","            # 5. Miroir vertical\n","            if random.random() < self.prob:\n","                dsm_tensor = TF.vflip(dsm_tensor)\n","                mask_tensor = TF.vflip(mask_tensor)\n","\n","        return dsm_tensor, mask_tensor\n","\n","def elastic_transform(image, mask, alpha=50, sigma=5):\n","    \"\"\"\n","    Applique une dÃ©formation Ã©lastique aux images.\n","    Version globale pour compatibilitÃ© avec les worker processes.\n","\n","    Args:\n","        image: Image d'entrÃ©e (numpy array 2D)\n","        mask: Masque d'entrÃ©e (numpy array 2D)\n","        alpha: IntensitÃ© de la dÃ©formation\n","        sigma: Lissage de la dÃ©formation\n","\n","    Returns:\n","        Tuple (image dÃ©formÃ©e, masque dÃ©formÃ©)\n","    \"\"\"\n","    shape = image.shape\n","\n","    # GÃ©nÃ©rer la grille de dÃ©placement\n","    dx = np.random.rand(shape[0], shape[1]) * 2 - 1\n","    dy = np.random.rand(shape[0], shape[1]) * 2 - 1\n","\n","    # Appliquer un filtre gaussien pour lisser les dÃ©placements\n","    dx = ndimage.gaussian_filter(dx, sigma) * alpha\n","    dy = ndimage.gaussian_filter(dy, sigma) * alpha\n","\n","    # CrÃ©er la grille de coordonnÃ©es\n","    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n","\n","    # Appliquer la dÃ©formation\n","    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n","\n","    # Appliquer la transformation aux deux images\n","    warped_image = ndimage.map_coordinates(image, indices, order=1).reshape(shape)\n","    warped_mask = ndimage.map_coordinates(mask, indices, order=0).reshape(shape)\n","\n","    return warped_image, warped_mask\n","\n","class GapDataset(Dataset):\n","    \"\"\"\n","    Dataset pour les donnÃ©es de trouÃ©es forestiÃ¨res.\n","    \"\"\"\n","    def __init__(self, tile_info, thresholds, transform=None):\n","        \"\"\"\n","        Args:\n","            tile_info: Liste d'informations sur les tuiles\n","            thresholds: Liste des seuils de hauteur (en mÃ¨tres)\n","            transform: Transformations Ã  appliquer\n","        \"\"\"\n","        self.tile_info = tile_info\n","        self.thresholds = thresholds\n","        self.transform = transform\n","\n","        # CrÃ©er tous les indices (tuile_idx, seuil)\n","        self.indices = []\n","        for i in range(len(tile_info)):\n","            for threshold in thresholds:\n","                self.indices.append((i, threshold))\n","\n","    def __len__(self):\n","        \"\"\"Retourne la taille du dataset\"\"\"\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retourne un Ã©lÃ©ment Ã  l'indice spÃ©cifiÃ©.\n","\n","        Args:\n","            idx: Indice dans la liste\n","\n","        Returns:\n","            Tuple (dsm_tensor, threshold_tensor, mask_tensor)\n","        \"\"\"\n","        # RÃ©cupÃ©rer l'indice et le seuil\n","        info_idx, threshold = self.indices[idx]\n","        info = self.tile_info[info_idx]\n","\n","        # Charger la tuile DSM\n","        dsm_tile = np.load(info['dsm_path'])\n","\n","        # Charger la tuile de masque\n","        mask_tile = np.load(info['mask_paths'][threshold])\n","\n","        # Appliquer les transformations\n","        if self.transform:\n","            dsm_tensor, mask_tensor = self.transform(dsm_tile, mask_tile)\n","        else:\n","            # Normalisation simple si pas de transform\n","            dsm_valid = ~np.isnan(dsm_tile)\n","            if np.any(dsm_valid):\n","                dsm_min = np.nanmin(dsm_tile)\n","                dsm_max = np.nanmax(dsm_tile)\n","                dsm_range = dsm_max - dsm_min\n","                if dsm_range > 0:\n","                    dsm_tile = np.where(dsm_valid, (dsm_tile - dsm_min) / dsm_range, 0)\n","                else:\n","                    dsm_tile = np.where(dsm_valid, 0, 0)\n","\n","            # Conversion en tenseurs\n","            dsm_tensor = torch.from_numpy(dsm_tile).float().unsqueeze(0)\n","            mask_tensor = torch.from_numpy(mask_tile).float().unsqueeze(0)\n","\n","        # Normaliser le seuil\n","        threshold_tensor = torch.tensor([threshold / max(self.thresholds)], dtype=torch.float32)\n","\n","        return dsm_tensor, threshold_tensor, mask_tensor\n","\n","def calculate_sampling_weights(tile_info, thresholds, gap_ratios):\n","    \"\"\"\n","    Calcule les poids d'Ã©chantillonnage pour chaque combinaison (tuile, seuil).\n","\n","    Args:\n","        tile_info: Liste d'informations sur les tuiles\n","        thresholds: Liste des seuils de hauteur\n","        gap_ratios: Dictionnaire des ratios de trouÃ©es par seuil\n","\n","    Returns:\n","        Liste des poids d'Ã©chantillonnage\n","    \"\"\"\n","    # Initialiser la liste des poids\n","    weights = []\n","\n","    # Calculer les poids pour chaque seuil (une seule fois)\n","    threshold_weights = {}\n","    for threshold in thresholds:\n","        ratio = gap_ratios.get(threshold, 0.5)  # Valeur par dÃ©faut si non disponible\n","\n","        # Calculer le facteur de pondÃ©ration\n","        if ratio > 0.5:\n","            # Si plus de 50% des pixels sont des trouÃ©es, donner plus de poids aux non-trouÃ©es\n","            weight = ratio / (1 - ratio + 1e-6)\n","        else:\n","            # Sinon, donner plus de poids aux trouÃ©es\n","            weight = (1 - ratio) / (ratio + 1e-6)\n","\n","        # Limiter les poids extrÃªmes\n","        threshold_weights[threshold] = min(weight, 5.0)\n","\n","    # Attribuer un poids Ã  chaque combinaison (tuile, seuil)\n","    for i in range(len(tile_info)):\n","        for threshold in thresholds:\n","            weights.append(threshold_weights[threshold])\n","\n","    return weights\n","\n","def create_data_loaders(config, train_tile_info, val_tile_info, test_tile_info=None):\n","    \"\"\"\n","    CrÃ©e les DataLoaders pour l'entraÃ®nement, la validation et le test.\n","\n","    Args:\n","        config: Configuration du projet\n","        train_tile_info: Informations sur les tuiles d'entraÃ®nement\n","        val_tile_info: Informations sur les tuiles de validation\n","        test_tile_info: Informations sur les tuiles de test (optionnel)\n","\n","    Returns:\n","        Tuple (train_loader, val_loader, test_loader)\n","    \"\"\"\n","    # Charger les ratios de trouÃ©es\n","    gap_ratios_path = os.path.join(config.PROCESSED_DIR, 'gap_ratios.json')\n","    gap_ratios = {}\n","\n","    if os.path.exists(gap_ratios_path):\n","        try:\n","            with open(gap_ratios_path, 'r') as f:\n","                gap_ratios_data = json.load(f)\n","                gap_ratios = {int(k): float(v) for k, v in gap_ratios_data.items()}\n","            print(f\"Ratios de trouÃ©es chargÃ©s: {gap_ratios}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Erreur lors du chargement des ratios de trouÃ©es: {str(e)}\")\n","            # Valeurs par dÃ©faut approximatives\n","            gap_ratios = {10: 0.15, 15: 0.30, 20: 0.50, 25: 0.70, 30: 0.85}\n","    else:\n","        # Valeurs par dÃ©faut approximatives\n","        gap_ratios = {10: 0.15, 15: 0.30, 20: 0.50, 25: 0.70, 30: 0.85}\n","        print(\"Utilisation de ratios de trouÃ©es par dÃ©faut.\")\n","\n","    # CrÃ©er les datasets\n","    train_dataset = GapDataset(\n","        train_tile_info,\n","        config.THRESHOLDS,\n","        transform=ForestGapTransforms(\n","            is_train=True,\n","            prob=0.7,\n","            advanced_aug_prob=0.3,\n","            enable_elastic=False  # DÃ©sactivÃ© par dÃ©faut pour Ã©viter les problÃ¨mes\n","        )\n","    )\n","\n","    val_dataset = GapDataset(\n","        val_tile_info,\n","        config.THRESHOLDS,\n","        transform=ForestGapTransforms(is_train=False)  # DÃ©sactive l'augmentation pour la validation\n","    )\n","\n","    # Calculer les poids d'Ã©chantillonnage pour l'entraÃ®nement\n","    train_weights = calculate_sampling_weights(train_tile_info, config.THRESHOLDS, gap_ratios)\n","\n","    # CrÃ©er le sampler pondÃ©rÃ© pour l'entraÃ®nement\n","    train_sampler = torch.utils.data.WeightedRandomSampler(\n","        weights=train_weights,\n","        num_samples=len(train_dataset),\n","        replacement=True  # Permettre le remplacement\n","    )\n","\n","    # Configurer les paramÃ¨tres du DataLoader\n","    persistent_workers = config.NUM_WORKERS > 0\n","    prefetch_factor = config.PREFETCH_FACTOR if config.NUM_WORKERS > 0 else None\n","\n","    # CrÃ©er le DataLoader d'entraÃ®nement avec le sampler\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=config.BATCH_SIZE,\n","        sampler=train_sampler,  # Utiliser le sampler au lieu de shuffle\n","        num_workers=config.NUM_WORKERS,\n","        pin_memory=config.PIN_MEMORY,\n","        persistent_workers=persistent_workers,\n","        prefetch_factor=prefetch_factor,\n","        drop_last=True\n","    )\n","\n","    # DataLoader de validation (sans sampler)\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=config.BATCH_SIZE,\n","        shuffle=False,\n","        num_workers=config.NUM_WORKERS,\n","        pin_memory=config.PIN_MEMORY,\n","        persistent_workers=persistent_workers,\n","        prefetch_factor=prefetch_factor\n","    )\n","\n","    # CrÃ©er le DataLoader de test si des donnÃ©es de test sont fournies\n","    test_loader = None\n","    if test_tile_info is not None:\n","        test_dataset = GapDataset(\n","            test_tile_info,\n","            config.THRESHOLDS,\n","            transform=ForestGapTransforms(is_train=False)\n","        )\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=config.BATCH_SIZE,\n","            shuffle=False,\n","            num_workers=config.NUM_WORKERS,\n","            pin_memory=config.PIN_MEMORY,\n","            persistent_workers=persistent_workers,\n","            prefetch_factor=prefetch_factor\n","        )\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"markdown","metadata":{"id":"y-WI2ho2uwF9"},"source":["## SECTION 5: DÃ‰FINITION DES BLOCS D'ARCHITECTURE"]},{"cell_type":"markdown","metadata":{"id":"nRf99uJr9C9y"},"source":["### Achitecture base : position, attention et blocs rÃ©siduels"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1741116990953,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"PTOqTXqAi1cU"},"outputs":[],"source":["# =====================================================================\n","# SECTION 5: DÃ‰FINITION DES BLOCS D'ARCHITECTURE\n","# =====================================================================\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Module d'encodage de position qui ajoute des informations spatiales aux features.\n","    \"\"\"\n","    def __init__(self):\n","        super(PositionalEncoding, self).__init__()\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor d'entrÃ©e (B, C, H, W)\n","\n","        Returns:\n","            Tensor avec informations de position (B, C+4, H, W)\n","        \"\"\"\n","        # Obtenir les dimensions\n","        batch_size, _, h, w = x.size()\n","\n","        # CrÃ©er des grilles de coordonnÃ©es normalisÃ©es\n","        y_range = torch.linspace(0, 1, h, device=x.device)\n","        x_range = torch.linspace(0, 1, w, device=x.device)\n","\n","        # CrÃ©er des grilles 2D\n","        y_grid, x_grid = torch.meshgrid(y_range, x_range, indexing='ij')\n","\n","        # Distance au centre (0.5, 0.5)\n","        y_center = torch.abs(y_grid - 0.5)\n","        x_center = torch.abs(x_grid - 0.5)\n","\n","        # RÃ©pÃ©ter pour chaque Ã©lÃ©ment du batch\n","        y_grid = y_grid.expand(batch_size, 1, -1, -1)\n","        x_grid = x_grid.expand(batch_size, 1, -1, -1)\n","        y_center = y_center.expand(batch_size, 1, -1, -1)\n","        x_center = x_center.expand(batch_size, 1, -1, -1)\n","\n","        # ConcatÃ©ner avec les features existantes\n","        pos_encoding = torch.cat([y_grid, x_grid, y_center, x_center], dim=1)\n","\n","        return torch.cat([x, pos_encoding], dim=1)\n","\n","class ResidualBlock(nn.Module):\n","    \"\"\"\n","    Bloc rÃ©siduel pour permettre un meilleur flux de gradient.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0):\n","        super(ResidualBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Connexion rÃ©siduelle (shortcut)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        # Dropout pour la rÃ©gularisation\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        # Chemin principal\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Appliquer dropout si activÃ©\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        # Ajouter la connexion rÃ©siduelle\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","class SpatialAttentionBlock(nn.Module):\n","    \"\"\"\n","    Bloc d'attention spatiale qui aide le modÃ¨le Ã  se concentrer sur les rÃ©gions importantes.\n","    \"\"\"\n","    def __init__(self, enc_channels, dec_channels, out_channels):\n","        super(SpatialAttentionBlock, self).__init__()\n","\n","        # RÃ©duire le nombre de canaux\n","        self.theta_x = nn.Conv2d(enc_channels, out_channels, kernel_size=1, padding=0)\n","        self.phi_g = nn.Conv2d(dec_channels, out_channels, kernel_size=1, padding=0)\n","\n","        # Couche pour gÃ©nÃ©rer les coefficients d'attention\n","        self.psi = nn.Sequential(\n","            nn.Conv2d(out_channels, 1, kernel_size=1, padding=0),\n","            nn.Sigmoid()\n","        )\n","\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x, g):\n","        \"\"\"\n","        Args:\n","            x: Features de l'encodeur (B, C1, H, W)\n","            g: Features du dÃ©codeur (B, C2, H, W)\n","\n","        Returns:\n","            Features avec attention appliquÃ©e\n","        \"\"\"\n","        # RÃ©duire le nombre de canaux\n","        theta_x = self.theta_x(x)\n","        phi_g = self.phi_g(g)\n","\n","        # Ajouter les features et activer\n","        f = self.relu(theta_x + phi_g)\n","\n","        # GÃ©nÃ©rer les coefficients d'attention\n","        attention = self.psi(f)\n","\n","        # Appliquer l'attention aux features d'entrÃ©e\n","        return x * attention\n","\n","class UNet(nn.Module):\n","    \"\"\"\n","    ModÃ¨le U-Net amÃ©liorÃ© avec encodage de position, blocs rÃ©siduels et attention spatiale.\n","    \"\"\"\n","    def __init__(self, in_channels=1, dropout_rate=0.2, use_checkpointing=False):\n","        super(UNet, self).__init__()\n","        self.use_checkpointing = use_checkpointing\n","\n","        # Encodage de position (ajoute 4 canaux)\n","        self.pos_encoding = PositionalEncoding()\n","\n","        # Encodeur avec blocs rÃ©siduels\n","        self.enc1 = ResidualBlock(in_channels + 4, 64, dropout_rate=dropout_rate)  # +4 pour l'encodage positionnel\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc2 = ResidualBlock(64, 128, dropout_rate=dropout_rate)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc3 = ResidualBlock(128, 256, dropout_rate=dropout_rate)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bottleneck (fond du U)\n","        self.bottleneck = ResidualBlock(256, 512, dropout_rate=dropout_rate)\n","\n","        # Module de traitement du seuil de hauteur\n","        self.threshold_encoder = nn.Sequential(\n","            nn.Linear(1, 32),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(32, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, 128),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # DÃ©codeur avec attention spatiale\n","        self.upconv3 = nn.ConvTranspose2d(512 + 128, 256, kernel_size=2, stride=2)  # +128 pour le seuil encodÃ©\n","        self.att3 = SpatialAttentionBlock(256, 256, 256)\n","        self.dec3 = ResidualBlock(2 * 256, 256, dropout_rate=dropout_rate)\n","\n","        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.att2 = SpatialAttentionBlock(128, 128, 128)\n","        self.dec2 = ResidualBlock(2 * 128, 128, dropout_rate=dropout_rate)\n","\n","        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.att1 = SpatialAttentionBlock(64, 64, 64)\n","        self.dec1 = ResidualBlock(2 * 64, 64, dropout_rate=dropout_rate)\n","\n","        # Couche de sortie pour la segmentation binaire (sans sigmoid)\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, dsm, threshold):\n","        \"\"\"\n","        Args:\n","            dsm: Tensor DSM (B, 1, H, W)\n","            threshold: Tensor seuil (B, 1)\n","\n","        Returns:\n","            Masque de segmentation prÃ©dit (B, 1, H, W)\n","        \"\"\"\n","        if self.use_checkpointing and self.training:\n","            from torch.utils.checkpoint import checkpoint\n","\n","            # Ajouter l'encodage de position\n","            x = self.pos_encoding(dsm)\n","\n","            # Encodeur avec checkpointing\n","            e1 = self.enc1(x)\n","            p1 = self.pool1(e1)\n","\n","            # Utiliser le checkpointing pour les parties lourdes\n","            e2 = checkpoint(self.enc2, p1, use_reentrant=False)\n","            p2 = self.pool2(e2)\n","\n","            e3 = checkpoint(self.enc3, p2)\n","            p3 = self.pool3(e3)\n","\n","            # Bottleneck avec checkpointing\n","            b = checkpoint(self.bottleneck, p3)\n","\n","            # Traitement du seuil (lÃ©ger, pas besoin de checkpoint)\n","            t = self.threshold_encoder(threshold)  # (B, 128)\n","\n","            # Redimensionner le seuil encodÃ©\n","            _, _, H, W = b.size()\n","            t = t.unsqueeze(-1).unsqueeze(-1)  # (B, 128, 1, 1)\n","            t = t.expand(-1, -1, H, W)  # (B, 128, H, W)\n","\n","            # ConcatÃ©ner le seuil avec le bottleneck\n","            b_with_threshold = torch.cat([b, t], dim=1)\n","\n","            # DÃ©codeur avec checkpointing pour les blocs lourds\n","            d3 = self.upconv3(b_with_threshold)\n","            a3 = self.att3(e3, d3)\n","            d3 = torch.cat([a3, d3], dim=1)\n","            d3 = checkpoint(self.dec3, d3)\n","\n","            d2 = self.upconv2(d3)\n","            a2 = self.att2(e2, d2)\n","            d2 = torch.cat([a2, d2], dim=1)\n","            d2 = checkpoint(self.dec2, d2)\n","\n","            d1 = self.upconv1(d2)\n","            a1 = self.att1(e1, d1)\n","            d1 = torch.cat([a1, d1], dim=1)\n","            d1 = checkpoint(self.dec1, d1)\n","\n","            # Couche finale (lÃ©gÃ¨re, pas besoin de checkpoint)\n","            out = self.final_conv(d1)\n","\n","        else:\n","            # Version standard sans checkpointing\n","            # Ajouter l'encodage de position\n","            x = self.pos_encoding(dsm)\n","\n","            # Encodeur\n","            e1 = self.enc1(x)\n","            p1 = self.pool1(e1)\n","\n","            e2 = self.enc2(p1)\n","            p2 = self.pool2(e2)\n","\n","            e3 = self.enc3(p2)\n","            p3 = self.pool3(e3)\n","\n","            # Bottleneck\n","            b = self.bottleneck(p3)\n","\n","            # Traitement du seuil\n","            t = self.threshold_encoder(threshold)  # (B, 128)\n","\n","            # Redimensionner le seuil encodÃ© pour l'ajouter aux features\n","            _, _, H, W = b.size()\n","            t = t.unsqueeze(-1).unsqueeze(-1)  # (B, 128, 1, 1)\n","            t = t.expand(-1, -1, H, W)  # (B, 128, H, W)\n","\n","            # ConcatÃ©ner le seuil avec le bottleneck\n","            b_with_threshold = torch.cat([b, t], dim=1)\n","\n","            # DÃ©codeur avec attention\n","            d3 = self.upconv3(b_with_threshold)\n","            a3 = self.att3(e3, d3)\n","            d3 = torch.cat([a3, d3], dim=1)\n","            d3 = self.dec3(d3)\n","\n","            d2 = self.upconv2(d3)\n","            a2 = self.att2(e2, d2)\n","            d2 = torch.cat([a2, d2], dim=1)\n","            d2 = self.dec2(d2)\n","\n","            d1 = self.upconv1(d2)\n","            a1 = self.att1(e1, d1)\n","            d1 = torch.cat([a1, d1], dim=1)\n","            d1 = self.dec1(d1)\n","\n","            # Couche finale pour la segmentation\n","            out = self.final_conv(d1)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"A5xYdOol9ZgI"},"source":["### Architecture avec implÃ©mentation de FiLM (Feature-wise Linear Modulation)"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1741116990979,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"AqzqXnEY9oGz","outputId":"eca905d7-0195-4a84-f274-7dbd66a1a54f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Ancien module de traitement du seuil (remplacÃ© par FiLM)\\nself.threshold_encoder = nn.Sequential(\\n    nn.Linear(1, 32),\\n    nn.ReLU(inplace=True),\\n    nn.Linear(32, 64),\\n    nn.ReLU(inplace=True),\\n    nn.Linear(64, 128),\\n    nn.ReLU(inplace=True)\\n)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":88}],"source":["class FiLMGenerator(nn.Module):\n","    \"\"\"\n","    GÃ©nÃ¨re les paramÃ¨tres de modulation FiLM (gamma, beta) Ã  partir d'une entrÃ©e conditionnelle.\n","    Dans notre cas, l'entrÃ©e est le seuil de hauteur.\n","    \"\"\"\n","    def __init__(self, input_dim=1, hidden_dim=64, output_dim=128):\n","        \"\"\"\n","        Args:\n","            input_dim: Dimension d'entrÃ©e (1 pour le seuil)\n","            hidden_dim: Dimension des couches cachÃ©es\n","            output_dim: Dimension de sortie (num_channels*2 pour gamma et beta)\n","        \"\"\"\n","        super(FiLMGenerator, self).__init__()\n","\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        # RÃ©seau MLP pour gÃ©nÃ©rer les paramÃ¨tres FiLM\n","        self.mlp = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden_dim, hidden_dim*2),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden_dim*2, output_dim*2)  # *2 pour gamma et beta\n","        )\n","\n","    def forward(self, condition):\n","        \"\"\"\n","        Args:\n","            condition: Tenseur de condition (B, input_dim)\n","\n","        Returns:\n","            Tuple (gamma, beta) de paramÃ¨tres FiLM\n","        \"\"\"\n","        # GÃ©nÃ©rer les paramÃ¨tres FiLM\n","        film_params = self.mlp(condition)\n","\n","        # SÃ©parer en gamma et beta\n","        gamma, beta = torch.split(film_params, self.output_dim, dim=1)\n","\n","        # Reshape pour le broadcasting\n","        gamma = gamma.view(-1, self.output_dim, 1, 1)\n","        beta = beta.view(-1, self.output_dim, 1, 1)\n","\n","        return gamma, beta\n","\n","\n","class FiLMModulation(nn.Module):\n","    \"\"\"\n","    Applique une modulation FiLM aux caractÃ©ristiques d'entrÃ©e.\n","    \"\"\"\n","    def __init__(self, num_channels):\n","        \"\"\"\n","        Args:\n","            num_channels: Nombre de canaux dans les caractÃ©ristiques Ã  moduler\n","        \"\"\"\n","        super(FiLMModulation, self).__init__()\n","        self.num_channels = num_channels\n","\n","    def forward(self, features, gamma, beta):\n","        \"\"\"\n","        Args:\n","            features: CaractÃ©ristiques Ã  moduler (B, C, H, W)\n","            gamma: Coefficient de multiplication (B, C, 1, 1)\n","            beta: Coefficient d'addition (B, C, 1, 1)\n","\n","        Returns:\n","            CaractÃ©ristiques modulÃ©es\n","        \"\"\"\n","        # Appliquer la modulation FiLM: gamma * features + beta\n","        return gamma * features + beta\n","\n","\n","class FiLMResidualBlock(nn.Module):\n","    \"\"\"\n","    Bloc rÃ©siduel avec modulation FiLM intÃ©grÃ©e.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0):\n","        super(FiLMResidualBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Modulation FiLM aprÃ¨s la normalisation par batch\n","        self.film = FiLMModulation(out_channels)\n","\n","        # Connexion rÃ©siduelle (shortcut)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        # Dropout pour la rÃ©gularisation\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","    def forward(self, x, gamma=None, beta=None):\n","        residual = x\n","\n","        # Chemin principal\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","\n","        # Appliquer FiLM si les paramÃ¨tres sont fournis\n","        if gamma is not None and beta is not None:\n","            out = self.film(out, gamma, beta)\n","\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Appliquer dropout si activÃ©\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        # Ajouter la connexion rÃ©siduelle\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","# Modification du modÃ¨le U-Net pour intÃ©grer FiLM\n","class UNetWithFiLM(nn.Module):\n","    \"\"\"\n","    ModÃ¨le U-Net amÃ©liorÃ© avec encodage de position, blocs rÃ©siduels, attention spatiale et FiLM.\n","    \"\"\"\n","    def __init__(self, in_channels=1, dropout_rate=0.2, use_checkpointing=False):\n","        super(UNetWithFiLM, self).__init__()\n","        self.use_checkpointing = use_checkpointing\n","\n","        # Encodage de position (ajoute 4 canaux)\n","        self.pos_encoding = PositionalEncoding()\n","\n","        # Encodeur avec blocs rÃ©siduels\n","        self.enc1 = ResidualBlock(in_channels + 4, 64, dropout_rate=dropout_rate)  # +4 pour l'encodage positionnel\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc2 = ResidualBlock(64, 128, dropout_rate=dropout_rate)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc3 = ResidualBlock(128, 256, dropout_rate=dropout_rate)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bottleneck (fond du U)\n","        self.bottleneck = ResidualBlock(256, 512, dropout_rate=dropout_rate)\n","\n","        # GÃ©nÃ©rateurs de paramÃ¨tres FiLM pour chaque niveau du dÃ©codeur\n","        self.film_generator3 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=256)\n","        self.film_generator2 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=128)\n","        self.film_generator1 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=64)\n","\n","        # Modulateurs FiLM pour chaque niveau du dÃ©codeur\n","        self.film3 = FiLMModulation(256)\n","        self.film2 = FiLMModulation(128)\n","        self.film1 = FiLMModulation(64)\n","\n","        # DÃ©codeur avec attention spatiale\n","        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.att3 = SpatialAttentionBlock(256, 256, 256)\n","        self.dec3 = FiLMResidualBlock(2 * 256, 256, dropout_rate=dropout_rate)\n","\n","        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.att2 = SpatialAttentionBlock(128, 128, 128)\n","        self.dec2 = FiLMResidualBlock(2 * 128, 128, dropout_rate=dropout_rate)\n","\n","        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.att1 = SpatialAttentionBlock(64, 64, 64)\n","        self.dec1 = FiLMResidualBlock(2 * 64, 64, dropout_rate=dropout_rate)\n","\n","        # Couche de sortie pour la segmentation binaire (sans sigmoid)\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, dsm, threshold):\n","        \"\"\"\n","        Args:\n","            dsm: Tensor DSM (B, 1, H, W)\n","            threshold: Tensor seuil (B, 1)\n","\n","        Returns:\n","            Masque de segmentation prÃ©dit (B, 1, H, W)\n","        \"\"\"\n","        # Ajouter l'encodage de position\n","        x = self.pos_encoding(dsm)\n","\n","        # Encodeur\n","        e1 = self.enc1(x)\n","        p1 = self.pool1(e1)\n","\n","        e2 = self.enc2(p1)\n","        p2 = self.pool2(e2)\n","\n","        e3 = self.enc3(p2)\n","        p3 = self.pool3(e3)\n","\n","        # Bottleneck\n","        b = self.bottleneck(p3)\n","\n","        # GÃ©nÃ©rer les paramÃ¨tres FiLM pour chaque niveau du dÃ©codeur\n","        gamma3, beta3 = self.film_generator3(threshold)\n","        gamma2, beta2 = self.film_generator2(threshold)\n","        gamma1, beta1 = self.film_generator1(threshold)\n","\n","        # DÃ©codeur avec attention et FiLM\n","        d3 = self.upconv3(b)\n","        a3 = self.att3(e3, d3)\n","        d3 = torch.cat([a3, d3], dim=1)\n","        d3 = self.dec3(d3, gamma3, beta3)\n","\n","        d2 = self.upconv2(d3)\n","        a2 = self.att2(e2, d2)\n","        d2 = torch.cat([a2, d2], dim=1)\n","        d2 = self.dec2(d2, gamma2, beta2)\n","\n","        d1 = self.upconv1(d2)\n","        a1 = self.att1(e1, d1)\n","        d1 = torch.cat([a1, d1], dim=1)\n","        d1 = self.dec1(d1, gamma1, beta1)\n","\n","        # Couche finale pour la segmentation\n","        out = self.final_conv(d1)\n","\n","        return out\n","\n","\n","# Adaptation du module de traitement du seuil\n","# (Cette partie peut Ãªtre retirÃ©e puisque nous utilisons maintenant FiLM)\n","\"\"\"\n","# Ancien module de traitement du seuil (remplacÃ© par FiLM)\n","self.threshold_encoder = nn.Sequential(\n","    nn.Linear(1, 32),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(32, 64),\n","    nn.ReLU(inplace=True),\n","    nn.Linear(64, 128),\n","    nn.ReLU(inplace=True)\n",")\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"15GtVw-H-ALV"},"source":["### Architecture avec attention par canal (Channel Attention Module) de CBAM (+FiLM)"]},{"cell_type":"code","execution_count":89,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1741116991030,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"iTFoR-No-BNI"},"outputs":[],"source":["class ChannelAttention(nn.Module):\n","    \"\"\"\n","    Module d'attention par canal (Channel Attention Module) de CBAM.\n","    Utilise Ã  la fois l'information de max pooling et d'average pooling.\n","    \"\"\"\n","    def __init__(self, channels, reduction_ratio=16):\n","        \"\"\"\n","        Args:\n","            channels: Nombre de canaux d'entrÃ©e\n","            reduction_ratio: Ratio de rÃ©duction pour le goulot d'Ã©tranglement dans le MLP\n","        \"\"\"\n","        super(ChannelAttention, self).__init__()\n","\n","        # Calculer la dimension rÃ©duite (au moins 8 neurones)\n","        reduced_channels = max(8, channels // reduction_ratio)\n","\n","        # MLP partagÃ©: FC -> ReLU -> FC\n","        self.mlp = nn.Sequential(\n","            nn.Linear(channels, reduced_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(reduced_channels, channels)\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tenseur d'entrÃ©e (B, C, H, W)\n","\n","        Returns:\n","            Tenseur d'attention par canal (B, C, 1, 1)\n","        \"\"\"\n","        batch_size, channels, height, width = x.size()\n","\n","        # Average pooling global (spatial)\n","        avg_pool = F.avg_pool2d(x, kernel_size=(height, width)).view(batch_size, channels)\n","        avg_attention = self.mlp(avg_pool)\n","\n","        # Max pooling global (spatial)\n","        max_pool = F.max_pool2d(x, kernel_size=(height, width)).view(batch_size, channels)\n","        max_attention = self.mlp(max_pool)\n","\n","        # Fusionner les deux informations (somme) et appliquer sigmoid\n","        attention = torch.sigmoid(avg_attention + max_attention).view(batch_size, channels, 1, 1)\n","\n","        return attention\n","\n","\n","class SpatialAttention(nn.Module):\n","    \"\"\"\n","    Module d'attention spatiale (Spatial Attention Module) de CBAM.\n","    Utilise l'information des caractÃ©ristiques spatiales agrÃ©gÃ©es par canal.\n","    \"\"\"\n","    def __init__(self, kernel_size=7):\n","        \"\"\"\n","        Args:\n","            kernel_size: Taille du noyau pour la convolution spatiale\n","        \"\"\"\n","        super(SpatialAttention, self).__init__()\n","\n","        # Assurons-nous que kernel_size est impair pour un padding correct\n","        assert kernel_size % 2 == 1, \"La taille du noyau doit Ãªtre impaire\"\n","\n","        # Couche de convolution 2D pour l'attention spatiale\n","        self.conv = nn.Conv2d(\n","            in_channels=2,  # Deux cartes de caractÃ©ristiques: max et avg\n","            out_channels=1,  # Une carte d'attention\n","            kernel_size=kernel_size,\n","            padding=kernel_size // 2,\n","            bias=False\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tenseur d'entrÃ©e (B, C, H, W)\n","\n","        Returns:\n","            Tenseur d'attention spatiale (B, 1, H, W)\n","        \"\"\"\n","        # Calculer les valeurs moyennes et max le long de la dimension des canaux\n","        avg_pool = torch.mean(x, dim=1, keepdim=True)  # (B, 1, H, W)\n","        max_pool, _ = torch.max(x, dim=1, keepdim=True)  # (B, 1, H, W)\n","\n","        # ConcatÃ©ner les deux informations\n","        spatial_info = torch.cat([avg_pool, max_pool], dim=1)  # (B, 2, H, W)\n","\n","        # Appliquer la convolution et sigmoid pour obtenir la carte d'attention\n","        attention = torch.sigmoid(self.conv(spatial_info))  # (B, 1, H, W)\n","\n","        return attention\n","\n","\n","class CBAM(nn.Module):\n","    \"\"\"\n","    Convolutional Block Attention Module (CBAM) complet.\n","    Combine l'attention par canal et l'attention spatiale sÃ©quentiellement.\n","    \"\"\"\n","    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n","        \"\"\"\n","        Args:\n","            channels: Nombre de canaux d'entrÃ©e\n","            reduction_ratio: Ratio de rÃ©duction pour le module d'attention par canal\n","            kernel_size: Taille du noyau pour le module d'attention spatiale\n","        \"\"\"\n","        super(CBAM, self).__init__()\n","\n","        # Modules d'attention\n","        self.channel_attention = ChannelAttention(channels, reduction_ratio)\n","        self.spatial_attention = SpatialAttention(kernel_size)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tenseur d'entrÃ©e (B, C, H, W)\n","\n","        Returns:\n","            Tenseur avec attention appliquÃ©e (B, C, H, W)\n","        \"\"\"\n","        # Appliquer l'attention par canal\n","        channel_attention = self.channel_attention(x)\n","        x = x * channel_attention\n","\n","        # Appliquer l'attention spatiale\n","        spatial_attention = self.spatial_attention(x)\n","        x = x * spatial_attention\n","\n","        return x\n","\n","\n","class ResidualBlockWithCBAM(nn.Module):\n","    \"\"\"\n","    Bloc rÃ©siduel avec module CBAM intÃ©grÃ©.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0):\n","        super(ResidualBlockWithCBAM, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Module CBAM aprÃ¨s les convolutions et avant l'addition de la rÃ©siduelle\n","        self.cbam = CBAM(out_channels)\n","\n","        # Connexion rÃ©siduelle (shortcut)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        # Dropout pour la rÃ©gularisation\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        # Chemin principal\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Appliquer CBAM avant l'addition rÃ©siduelle\n","        out = self.cbam(out)\n","\n","        # Appliquer dropout si activÃ©\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        # Ajouter la connexion rÃ©siduelle\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class DecoderBlockWithCBAM(nn.Module):\n","    \"\"\"\n","    Bloc de dÃ©codeur avec attention CBAM aprÃ¨s l'upsampling.\n","    \"\"\"\n","    def __init__(self, in_channels, skip_channels, out_channels, dropout_rate=0.0):\n","        super(DecoderBlockWithCBAM, self).__init__()\n","\n","        # Upsampling\n","        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","\n","        # CBAM aprÃ¨s l'upsampling (avant la concatÃ©nation avec skip connection)\n","        self.cbam_upconv = CBAM(out_channels)\n","\n","        # Attention pour la skip connection\n","        self.skip_attention = SpatialAttentionBlock(skip_channels, out_channels, out_channels)\n","\n","        # Convolutions aprÃ¨s la concatÃ©nation\n","        self.conv1 = nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # CBAM final (aprÃ¨s les convolutions)\n","        self.cbam_final = CBAM(out_channels)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","    def forward(self, x, skip):\n","        # Upsampling et CBAM\n","        up = self.upconv(x)\n","        up_attended = self.cbam_upconv(up)\n","\n","        # Appliquer l'attention Ã  la skip connection\n","        skip_attended = self.skip_attention(skip, up_attended)\n","\n","        # ConcatÃ©ner\n","        concat = torch.cat([skip_attended, up_attended], dim=1)\n","\n","        # Convolutions\n","        out = self.conv1(concat)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # CBAM final\n","        out = self.cbam_final(out)\n","\n","        # Dropout\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","# ModÃ¨le U-Net avec CBAM intÃ©grÃ©\n","class UNetWithCBAM(nn.Module):\n","    \"\"\"\n","    ModÃ¨le U-Net amÃ©liorÃ© avec encodage de position, blocs rÃ©siduels avec CBAM et dÃ©codeurs avec CBAM.\n","    \"\"\"\n","    def __init__(self, in_channels=1, dropout_rate=0.2, use_checkpointing=False):\n","        super(UNetWithCBAM, self).__init__()\n","        self.use_checkpointing = use_checkpointing\n","\n","        # Encodage de position (ajoute 4 canaux)\n","        self.pos_encoding = PositionalEncoding()\n","\n","        # Encodeur avec blocs rÃ©siduels et CBAM\n","        self.enc1 = ResidualBlockWithCBAM(in_channels + 4, 64, dropout_rate=dropout_rate)  # +4 pour l'encodage positionnel\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc2 = ResidualBlockWithCBAM(64, 128, dropout_rate=dropout_rate)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc3 = ResidualBlockWithCBAM(128, 256, dropout_rate=dropout_rate)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bottleneck (fond du U)\n","        self.bottleneck = ResidualBlockWithCBAM(256, 512, dropout_rate=dropout_rate)\n","\n","        # Module de traitement du seuil de hauteur\n","        self.threshold_encoder = nn.Sequential(\n","            nn.Linear(1, 32),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(32, 64),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(64, 128),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # DÃ©codeur avec blocs de dÃ©codeur CBAM\n","        self.dec3 = DecoderBlockWithCBAM(512 + 128, 256, 256, dropout_rate=dropout_rate)  # +128 pour le seuil encodÃ©\n","        self.dec2 = DecoderBlockWithCBAM(256, 128, 128, dropout_rate=dropout_rate)\n","        self.dec1 = DecoderBlockWithCBAM(128, 64, 64, dropout_rate=dropout_rate)\n","\n","        # Couche de sortie pour la segmentation binaire (sans sigmoid)\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, dsm, threshold):\n","        \"\"\"\n","        Args:\n","            dsm: Tensor DSM (B, 1, H, W)\n","            threshold: Tensor seuil (B, 1)\n","\n","        Returns:\n","            Masque de segmentation prÃ©dit (B, 1, H, W)\n","        \"\"\"\n","        # Ajouter l'encodage de position\n","        x = self.pos_encoding(dsm)\n","\n","        # Encodeur\n","        e1 = self.enc1(x)\n","        p1 = self.pool1(e1)\n","\n","        e2 = self.enc2(p1)\n","        p2 = self.pool2(e2)\n","\n","        e3 = self.enc3(p2)\n","        p3 = self.pool3(e3)\n","\n","        # Bottleneck\n","        b = self.bottleneck(p3)\n","\n","        # Traitement du seuil\n","        t = self.threshold_encoder(threshold)  # (B, 128)\n","\n","        # Redimensionner le seuil encodÃ© pour l'ajouter aux features\n","        _, _, H, W = b.size()\n","        t = t.unsqueeze(-1).unsqueeze(-1)  # (B, 128, 1, 1)\n","        t = t.expand(-1, -1, H, W)  # (B, 128, H, W)\n","\n","        # ConcatÃ©ner le seuil avec le bottleneck\n","        b_with_threshold = torch.cat([b, t], dim=1)\n","\n","        # DÃ©codeur avec CBAM\n","        d3 = self.dec3(b_with_threshold, e3)\n","        d2 = self.dec2(d3, e2)\n","        d1 = self.dec1(d2, e1)\n","\n","        # Couche finale pour la segmentation\n","        out = self.final_conv(d1)\n","\n","        return out\n","\n","\n","# Combinaison FiLM et CBAM en un seul modÃ¨le (version optimale)\n","class UNetWithFiLMAndCBAM(nn.Module):\n","    \"\"\"\n","    ModÃ¨le U-Net amÃ©liorÃ© combinant FiLM pour la modulation conditionnelle et CBAM pour l'attention.\n","    Cette architecture optimale utilise:\n","    - Blocs rÃ©siduels avec CBAM dans l'encodeur\n","    - FiLM pour la modulation conditionnelle dans le dÃ©codeur\n","    - DÃ©codeur avec CBAM aprÃ¨s les skip connections\n","    \"\"\"\n","    def __init__(self, in_channels=1, dropout_rate=0.2, use_checkpointing=False):\n","        super(UNetWithFiLMAndCBAM, self).__init__()\n","        self.use_checkpointing = use_checkpointing\n","\n","        # Encodage de position (ajoute 4 canaux)\n","        self.pos_encoding = PositionalEncoding()\n","\n","        # Encodeur avec blocs rÃ©siduels et CBAM\n","        self.enc1 = ResidualBlockWithCBAM(in_channels + 4, 64, dropout_rate=dropout_rate)  # +4 pour l'encodage positionnel\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc2 = ResidualBlockWithCBAM(64, 128, dropout_rate=dropout_rate)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc3 = ResidualBlockWithCBAM(128, 256, dropout_rate=dropout_rate)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bottleneck (fond du U)\n","        self.bottleneck = ResidualBlockWithCBAM(256, 512, dropout_rate=dropout_rate)\n","\n","        # GÃ©nÃ©rateurs de paramÃ¨tres FiLM pour chaque niveau du dÃ©codeur\n","        self.film_generator3 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=256)\n","        self.film_generator2 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=128)\n","        self.film_generator1 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=64)\n","\n","        # Blocs de dÃ©codeur optimisÃ©s (upconv, FiLM, concatenation, conv, CBAM)\n","        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.film3 = FiLMModulation(256)\n","        self.att3 = SpatialAttentionBlock(256, 256, 256)\n","        self.cbam3 = CBAM(256 * 2)  # Pour aprÃ¨s la concatÃ©nation\n","        self.dec_conv3 = nn.Sequential(\n","            nn.Conv2d(256 * 2, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.film2 = FiLMModulation(128)\n","        self.att2 = SpatialAttentionBlock(128, 128, 128)\n","        self.cbam2 = CBAM(128 * 2)\n","        self.dec_conv2 = nn.Sequential(\n","            nn.Conv2d(128 * 2, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.film1 = FiLMModulation(64)\n","        self.att1 = SpatialAttentionBlock(64, 64, 64)\n","        self.cbam1 = CBAM(64 * 2)\n","        self.dec_conv1 = nn.Sequential(\n","            nn.Conv2d(64 * 2, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","        # Couche de sortie pour la segmentation binaire (sans sigmoid)\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","    def forward(self, dsm, threshold):\n","        \"\"\"\n","        Args:\n","            dsm: Tensor DSM (B, 1, H, W)\n","            threshold: Tensor seuil (B, 1)\n","\n","        Returns:\n","            Masque de segmentation prÃ©dit (B, 1, H, W)\n","        \"\"\"\n","        # Ajouter l'encodage de position\n","        x = self.pos_encoding(dsm)\n","\n","        # Encodeur avec CBAM\n","        e1 = self.enc1(x)\n","        p1 = self.pool1(e1)\n","\n","        e2 = self.enc2(p1)\n","        p2 = self.pool2(e2)\n","\n","        e3 = self.enc3(p2)\n","        p3 = self.pool3(e3)\n","\n","        # Bottleneck\n","        b = self.bottleneck(p3)\n","\n","        # GÃ©nÃ©rer les paramÃ¨tres FiLM pour chaque niveau\n","        gamma3, beta3 = self.film_generator3(threshold)\n","        gamma2, beta2 = self.film_generator2(threshold)\n","        gamma1, beta1 = self.film_generator1(threshold)\n","\n","        # Niveau 3: upconv -> FiLM -> attention -> concat -> CBAM -> conv\n","        d3 = self.upconv3(b)\n","        d3 = self.film3(d3, gamma3, beta3)\n","        a3 = self.att3(e3, d3)\n","        d3 = torch.cat([a3, d3], dim=1)\n","        d3 = self.cbam3(d3)\n","        d3 = self.dec_conv3(d3)\n","        if self.dropout is not None:\n","            d3 = self.dropout(d3)\n","\n","        # Niveau 2\n","        d2 = self.upconv2(d3)\n","        d2 = self.film2(d2, gamma2, beta2)\n","        a2 = self.att2(e2, d2)\n","        d2 = torch.cat([a2, d2], dim=1)\n","        d2 = self.cbam2(d2)\n","        d2 = self.dec_conv2(d2)\n","        if self.dropout is not None:\n","            d2 = self.dropout(d2)\n","\n","        # Niveau 1\n","        d1 = self.upconv1(d2)\n","        d1 = self.film1(d1, gamma1, beta1)\n","        a1 = self.att1(e1, d1)\n","        d1 = torch.cat([a1, d1], dim=1)\n","        d1 = self.cbam1(d1)\n","        d1 = self.dec_conv1(d1)\n","        if self.dropout is not None:\n","            d1 = self.dropout(d1)\n","\n","        # Couche finale\n","        out = self.final_conv(d1)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"1WEtF2Z9_ITT"},"source":["### Architecture avec implÃ©mentation de DropPath (+CBAM +FiLM = UNetWithAllFeatures)"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1741116991037,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"XJFkKyvI_JiR"},"outputs":[],"source":["class DropPath(nn.Module):\n","    \"\"\"\n","    Module DropPath (Stochastic Depth).\n","\n","    Abandonne alÃ©atoirement le chemin rÃ©siduel complet avec une probabilitÃ© drop_prob\n","    pendant l'entraÃ®nement. Pendant l'infÃ©rence, le chemin est toujours conservÃ©.\n","\n","    ImplÃ©mentÃ© selon \"Deep Networks with Stochastic Depth\"\n","    (https://arxiv.org/abs/1603.09382)\n","    \"\"\"\n","    def __init__(self, drop_prob=0.0):\n","        \"\"\"\n","        Args:\n","            drop_prob: ProbabilitÃ© d'abandonner le chemin (0.0 = aucun drop, 1.0 = drop toujours)\n","        \"\"\"\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tenseur d'entrÃ©e\n","\n","        Returns:\n","            Tenseur d'entrÃ©e avec probabilitÃ© (1-drop_prob) ou zÃ©ros avec probabilitÃ© drop_prob\n","        \"\"\"\n","        if self.drop_prob == 0.0 or not self.training:\n","            return x\n","\n","        # GÃ©nÃ©rer un masque de survie pour tout le batch\n","        keep_prob = 1.0 - self.drop_prob\n","        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # Forme pour le broadcasting\n","\n","        # GÃ©nÃ©rer un masque binaire avec une probabilitÃ© keep_prob\n","        random_tensor = torch.rand(shape, dtype=x.dtype, device=x.device) < keep_prob\n","\n","        # Normaliser la sortie pour maintenir l'espÃ©rance statistique\n","        output = x.div(keep_prob) * random_tensor.float()\n","\n","        return output\n","\n","\n","class DropPathScheduler:\n","    \"\"\"\n","    Scheduler pour ajuster le taux de DropPath pendant l'entraÃ®nement.\n","    ImplÃ©mente diffÃ©rentes stratÃ©gies: linÃ©aire, exponentielle, cosine, etc.\n","    \"\"\"\n","    def __init__(self, model, start_prob=0.0, final_prob=0.3, epochs=100, strategy='linear',\n","                 layer_wise=True, deeper_more_drop=True):\n","        \"\"\"\n","        Args:\n","            model: ModÃ¨le contenant des modules DropPath\n","            start_prob: ProbabilitÃ© initiale de drop\n","            final_prob: ProbabilitÃ© finale de drop (atteinte Ã  la fin de l'entraÃ®nement)\n","            epochs: Nombre total d'Ã©poques d'entraÃ®nement\n","            strategy: StratÃ©gie d'Ã©volution ('linear', 'cosine', 'exp', 'constant')\n","            layer_wise: Si True, applique des taux diffÃ©rents selon la profondeur\n","            deeper_more_drop: Si True, les couches plus profondes ont plus de drop\n","        \"\"\"\n","        self.model = model\n","        self.start_prob = start_prob\n","        self.final_prob = final_prob\n","        self.epochs = epochs\n","        self.strategy = strategy\n","        self.layer_wise = layer_wise\n","        self.deeper_more_drop = deeper_more_drop\n","\n","        # Identifier tous les modules DropPath dans le modÃ¨le\n","        self.droppath_modules = []\n","        self._find_droppath_modules(self.model)\n","\n","        self.num_modules = len(self.droppath_modules)\n","        print(f\"DropPathScheduler initialisÃ© avec {self.num_modules} modules DropPath\")\n","\n","    def _find_droppath_modules(self, module, path=\"\"):\n","        \"\"\"\n","        Trouve rÃ©cursivement tous les modules DropPath dans le modÃ¨le.\n","        \"\"\"\n","        for name, child in module.named_children():\n","            new_path = f\"{path}.{name}\" if path else name\n","            if isinstance(child, DropPath):\n","                self.droppath_modules.append((new_path, child))\n","            else:\n","                self._find_droppath_modules(child, new_path)\n","\n","    def step(self, epoch):\n","        \"\"\"\n","        Met Ã  jour les probabilitÃ©s de drop pour l'Ã©poque actuelle.\n","\n","        Args:\n","            epoch: Ã‰poque actuelle (0-indexed)\n","        \"\"\"\n","        if self.num_modules == 0:\n","            return\n","\n","        # Normaliser l'Ã©poque entre 0 et 1\n","        t = epoch / self.epochs\n","\n","        # Calculer la probabilitÃ© globale selon la stratÃ©gie\n","        if self.strategy == 'constant':\n","            global_prob = self.final_prob\n","        elif self.strategy == 'linear':\n","            global_prob = self.start_prob + (self.final_prob - self.start_prob) * t\n","        elif self.strategy == 'cosine':\n","            global_prob = self.start_prob + (self.final_prob - self.start_prob) * (1 - math.cos(t * math.pi)) / 2\n","        elif self.strategy == 'exp':\n","            global_prob = self.start_prob + (self.final_prob - self.start_prob) * (1 - math.exp(-5 * t))\n","        else:\n","            raise ValueError(f\"StratÃ©gie inconnue: {self.strategy}\")\n","\n","        # Appliquer la mÃªme probabilitÃ© Ã  tous les modules (non layer-wise)\n","        if not self.layer_wise:\n","            for _, module in self.droppath_modules:\n","                module.drop_prob = global_prob\n","            return\n","\n","        # Appliquer des probabilitÃ©s diffÃ©rentes selon la profondeur (layer-wise)\n","        for i, (path, module) in enumerate(self.droppath_modules):\n","            if self.deeper_more_drop:\n","                # Les modules plus profonds ont une probabilitÃ© plus Ã©levÃ©e\n","                depth_factor = (i + 1) / self.num_modules\n","                module.drop_prob = global_prob * depth_factor\n","            else:\n","                # Les modules plus superficiels ont une probabilitÃ© plus Ã©levÃ©e\n","                depth_factor = 1.0 - (i / self.num_modules)\n","                module.drop_prob = global_prob * depth_factor\n","\n","        # Afficher les probabilitÃ©s actuelles (pour dÃ©bogage)\n","        if epoch % 10 == 0:\n","            print(f\"Ã‰poque {epoch}: ProbabilitÃ© DropPath globale = {global_prob:.4f}\")\n","            if self.layer_wise:\n","                print(\"ProbabilitÃ©s par couche:\")\n","                for path, module in self.droppath_modules:\n","                    print(f\"  {path}: {module.drop_prob:.4f}\")\n","\n","\n","class ResidualBlockWithDropPath(nn.Module):\n","    \"\"\"\n","    Bloc rÃ©siduel avec DropPath intÃ©grÃ©.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0, drop_path_rate=0.0):\n","        super(ResidualBlockWithDropPath, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Connexion rÃ©siduelle (shortcut)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        # Dropout pour la rÃ©gularisation\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","        # DropPath pour la rÃ©gularisation stochastique de profondeur\n","        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        # Chemin principal\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Appliquer dropout si activÃ©\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        # Appliquer drop path Ã  la sortie avant l'addition rÃ©siduelle\n","        out = self.drop_path(out)\n","\n","        # Ajouter la connexion rÃ©siduelle\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","# Combine DropPath avec FiLM et CBAM\n","class ResidualBlockWithFiLMCBAMDropPath(nn.Module):\n","    \"\"\"\n","    Bloc rÃ©siduel avancÃ© intÃ©grant:\n","    1. Modulation FiLM\n","    2. Attention CBAM\n","    3. DropPath pour la rÃ©gularisation\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0, drop_path_rate=0.0):\n","        super(ResidualBlockWithFiLMCBAMDropPath, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # CBAM aprÃ¨s les convolutions\n","        self.cbam = CBAM(out_channels)\n","\n","        # Modulation FiLM\n","        self.film = FiLMModulation(out_channels)\n","\n","        # Connexion rÃ©siduelle (shortcut)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        # Dropout pour la rÃ©gularisation\n","        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n","\n","        # DropPath pour la rÃ©gularisation stochastique de profondeur\n","        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n","\n","    def forward(self, x, gamma=None, beta=None):\n","        residual = x\n","\n","        # Chemin principal\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Appliquer CBAM\n","        out = self.cbam(out)\n","\n","        # Appliquer FiLM si les paramÃ¨tres sont fournis\n","        if gamma is not None and beta is not None:\n","            out = self.film(out, gamma, beta)\n","\n","        # Appliquer dropout si activÃ©\n","        if self.dropout is not None:\n","            out = self.dropout(out)\n","\n","        # Appliquer drop path Ã  la sortie avant l'addition rÃ©siduelle\n","        out = self.drop_path(out)\n","\n","        # Ajouter la connexion rÃ©siduelle\n","        out += self.shortcut(residual)\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","# ModÃ¨le U-Net complet avec les trois techniques\n","class UNetWithAllFeatures(nn.Module):\n","    \"\"\"\n","    ModÃ¨le U-Net avancÃ© intÃ©grant toutes les techniques:\n","    - FiLM pour la modulation conditionnelle\n","    - CBAM pour l'attention\n","    - DropPath pour la rÃ©gularisation\n","\n","    Cette architecture est organisÃ©e avec:\n","    - Blocs rÃ©siduels avancÃ©s dans l'encodeur (CBAM + DropPath)\n","    - FiLM pour la modulation conditionnelle dans le dÃ©codeur\n","    - Taux de DropPath progressif (plus Ã©levÃ© dans les couches profondes)\n","    \"\"\"\n","    def __init__(self, in_channels=1, dropout_rate=0.2, drop_path_rate=0.2, use_checkpointing=False):\n","        super(UNetWithAllFeatures, self).__init__()\n","        self.use_checkpointing = use_checkpointing\n","\n","        # Encodage de position (ajoute 4 canaux)\n","        self.pos_encoding = PositionalEncoding()\n","\n","        # Taux de DropPath progressif (plus Ã©levÃ© dans les couches profondes)\n","        # Les couches superficielles ont un taux plus faible pour prÃ©server les dÃ©tails\n","        dp_rates = [drop_path_rate * i / 3 for i in range(4)]  # [0, dp/3, 2dp/3, dp]\n","\n","        # Encodeur avec blocs rÃ©siduels, CBAM et DropPath\n","        self.enc1 = ResidualBlockWithFiLMCBAMDropPath(\n","            in_channels + 4, 64, dropout_rate=dropout_rate, drop_path_rate=dp_rates[0])\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc2 = ResidualBlockWithFiLMCBAMDropPath(\n","            64, 128, dropout_rate=dropout_rate, drop_path_rate=dp_rates[1])\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.enc3 = ResidualBlockWithFiLMCBAMDropPath(\n","            128, 256, dropout_rate=dropout_rate, drop_path_rate=dp_rates[2])\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Bottleneck (fond du U) - taux de DropPath maximal\n","        self.bottleneck = ResidualBlockWithFiLMCBAMDropPath(\n","            256, 512, dropout_rate=dropout_rate, drop_path_rate=dp_rates[3])\n","\n","        # GÃ©nÃ©rateurs de paramÃ¨tres FiLM pour chaque niveau du dÃ©codeur\n","        self.film_generator3 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=256)\n","        self.film_generator2 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=128)\n","        self.film_generator1 = FiLMGenerator(input_dim=1, hidden_dim=64, output_dim=64)\n","\n","        # DÃ©codeur - utilisez des taux de DropPath dÃ©croissants pour le dÃ©codeur\n","        # pour Ã©viter de perdre des dÃ©tails spatials fins\n","        dec_dp_rates = list(reversed(dp_rates[:-1])) + [0.0]  # [2dp/3, dp/3, 0]\n","\n","        # Blocs de dÃ©codeur avancÃ©s\n","        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.att3 = SpatialAttentionBlock(256, 256, 256)\n","        self.dec3 = ResidualBlockWithFiLMCBAMDropPath(\n","            2 * 256, 256, dropout_rate=dropout_rate, drop_path_rate=dec_dp_rates[0])\n","\n","        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.att2 = SpatialAttentionBlock(128, 128, 128)\n","        self.dec2 = ResidualBlockWithFiLMCBAMDropPath(\n","            2 * 128, 128, dropout_rate=dropout_rate, drop_path_rate=dec_dp_rates[1])\n","\n","        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.att1 = SpatialAttentionBlock(64, 64, 64)\n","        self.dec1 = ResidualBlockWithFiLMCBAMDropPath(\n","            2 * 64, 64, dropout_rate=dropout_rate, drop_path_rate=dec_dp_rates[2])\n","\n","        # Couche de sortie pour la segmentation binaire (sans sigmoid)\n","        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, dsm, threshold):\n","        \"\"\"\n","        Args:\n","            dsm: Tensor DSM (B, 1, H, W)\n","            threshold: Tensor seuil (B, 1)\n","\n","        Returns:\n","            Masque de segmentation prÃ©dit (B, 1, H, W)\n","        \"\"\"\n","        # Ajouter l'encodage de position\n","        x = self.pos_encoding(dsm)\n","\n","        # Encodeur avec CBAM et DropPath (pas de FiLM ici)\n","        e1 = self.enc1(x)\n","        p1 = self.pool1(e1)\n","\n","        e2 = self.enc2(p1)\n","        p2 = self.pool2(e2)\n","\n","        e3 = self.enc3(p2)\n","        p3 = self.pool3(e3)\n","\n","        # Bottleneck (pas de FiLM ici non plus)\n","        b = self.bottleneck(p3)\n","\n","        # GÃ©nÃ©rer les paramÃ¨tres FiLM pour chaque niveau du dÃ©codeur\n","        gamma3, beta3 = self.film_generator3(threshold)\n","        gamma2, beta2 = self.film_generator2(threshold)\n","        gamma1, beta1 = self.film_generator1(threshold)\n","\n","        # DÃ©codeur avec attention, FiLM et DropPath\n","        d3 = self.upconv3(b)\n","        a3 = self.att3(e3, d3)\n","        d3 = torch.cat([a3, d3], dim=1)\n","        d3 = self.dec3(d3, gamma3, beta3)\n","\n","        d2 = self.upconv2(d3)\n","        a2 = self.att2(e2, d2)\n","        d2 = torch.cat([a2, d2], dim=1)\n","        d2 = self.dec2(d2, gamma2, beta2)\n","\n","        d1 = self.upconv1(d2)\n","        a1 = self.att1(e1, d1)\n","        d1 = torch.cat([a1, d1], dim=1)\n","        d1 = self.dec1(d1, gamma1, beta1)\n","\n","        # Couche finale pour la segmentation\n","        out = self.final_conv(d1)\n","\n","        return out\n","\n","\n","# Utilisation de DropPathScheduler pendant l'entraÃ®nement\n","def train_with_droppath_scheduling(model, train_loader, val_loader, criterion, optimizer, config, device):\n","    \"\"\"\n","    Fonction d'entraÃ®nement avec scheduling DropPath.\n","    \"\"\"\n","    # Initialiser le scheduleur DropPath\n","    droppath_scheduler = DropPathScheduler(\n","        model,\n","        start_prob=0.0,\n","        final_prob=0.3,  # ProbabilitÃ© finale de drop (ajustable)\n","        epochs=config.EPOCHS,\n","        strategy='linear',  # Options: 'linear', 'cosine', 'exp', 'constant'\n","        layer_wise=True,  # DiffÃ©rentes probabilitÃ©s par couche\n","        deeper_more_drop=True  # Les couches profondes ont plus de drop\n","    )\n","\n","    # Reste du code d'entraÃ®nement, ajoutez simplement:\n","    for epoch in range(start_epoch, config.EPOCHS):\n","        # Mettre Ã  jour les probabilitÃ©s de DropPath pour cette Ã©poque\n","        droppath_scheduler.step(epoch)\n","\n","        # Code d'entraÃ®nement existant...\n","\n","    return model, tracker"]},{"cell_type":"markdown","metadata":{"id":"cAtbbVFdBqVe"},"source":["## SECTION 6 : FONCTION FACTORY - CENTRALISATION DES MODÃˆLES"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1741116991086,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"FcKz_YlzCezH"},"outputs":[],"source":["def create_unet_model(config):\n","    \"\"\"\n","    Fonction factory qui crÃ©e une instance du modÃ¨le U-Net selon la configuration.\n","\n","    Args:\n","        config: Configuration contenant les paramÃ¨tres du modÃ¨le\n","            - model_type: Type de modÃ¨le ('basic', 'film', 'cbam', 'droppath', 'all')\n","            - in_channels: Nombre de canaux d'entrÃ©e (dÃ©faut: 1)\n","            - dropout_rate: Taux de dropout (dÃ©faut: 0.2)\n","            - drop_path_rate: Taux de droppath (dÃ©faut: 0.1, utilisÃ© seulement si 'droppath' ou 'all')\n","            - use_gradient_checkpointing: Utilisation du checkpointing (dÃ©faut: False)\n","\n","    Returns:\n","        Un modÃ¨le U-Net configurÃ© selon les options spÃ©cifiÃ©es\n","    \"\"\"\n","    # Mettre Ã  jour les chemins selon le type de modÃ¨le\n","    config = setup_model_paths(config)\n","\n","    # Extraire les paramÃ¨tres du modÃ¨le de la configuration\n","    model_type = getattr(config, 'MODEL_TYPE', 'basic').lower()\n","    in_channels = getattr(config, 'IN_CHANNELS', 1)\n","    dropout_rate = getattr(config, 'DROPOUT_RATE', 0.2)\n","    drop_path_rate = getattr(config, 'DROP_PATH_RATE', 0.1)\n","    use_checkpointing = getattr(config, 'USE_GRADIENT_CHECKPOINTING', False)\n","\n","    # CrÃ©er le modÃ¨le selon le type spÃ©cifiÃ©\n","    if model_type == 'basic':\n","        # ModÃ¨le U-Net standard\n","        model = UNet(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    elif model_type == 'film':\n","        # ModÃ¨le avec modulation FiLM\n","        model = UNetWithFiLM(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    elif model_type == 'cbam':\n","        # ModÃ¨le avec attention CBAM\n","        model = UNetWithCBAM(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    elif model_type == 'droppath':\n","        # ModÃ¨le avec DropPath\n","        model = UNet(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","        # Remplacer les blocs rÃ©siduels par des blocs avec DropPath\n","        model = convert_to_droppath(model, drop_path_rate)\n","\n","    elif model_type == 'film_cbam':\n","        # ModÃ¨le avec FiLM et CBAM\n","        model = UNetWithFiLMAndCBAM(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    elif model_type == 'all':\n","        # ModÃ¨le avec toutes les amÃ©liorations\n","        model = UNetWithAllFeatures(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            drop_path_rate=drop_path_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    else:\n","        print(f\"Type de modÃ¨le non reconnu: {model_type}. Utilisation du modÃ¨le de base.\")\n","        model = UNet(\n","            in_channels=in_channels,\n","            dropout_rate=dropout_rate,\n","            use_checkpointing=use_checkpointing\n","        )\n","\n","    return model\n","\n","\n","def convert_to_droppath(model, drop_path_rate):\n","    \"\"\"\n","    Convertit les blocs rÃ©siduels d'un modÃ¨le existant en blocs avec DropPath.\n","\n","    Args:\n","        model: ModÃ¨le Ã  convertir\n","        drop_path_rate: Taux de DropPath Ã  appliquer\n","\n","    Returns:\n","        ModÃ¨le avec DropPath\n","    \"\"\"\n","    # Liste pour stocker les changements Ã  faire\n","    replacement_dict = {}\n","\n","    # DÃ©tecter les blocs rÃ©siduels et prÃ©parer leur remplacement\n","    for name, module in model.named_children():\n","        if isinstance(module, ResidualBlock):\n","            # CrÃ©er un nouveau bloc avec DropPath\n","            new_module = ResidualBlockWithDropPath(\n","                in_channels=module.conv1.in_channels,\n","                out_channels=module.conv1.out_channels,\n","                kernel_size=module.conv1.kernel_size[0],\n","                dropout_rate=module.dropout.p if module.dropout else 0.0,\n","                drop_path_rate=drop_path_rate\n","            )\n","            replacement_dict[name] = new_module\n","        elif len(list(module.children())) > 0:\n","            # Appliquer rÃ©cursivement aux sous-modules\n","            converted_module = convert_to_droppath(module, drop_path_rate)\n","            replacement_dict[name] = converted_module\n","\n","    # Appliquer les remplacements\n","    for name, new_module in replacement_dict.items():\n","        setattr(model, name, new_module)\n","\n","    return model\n","\n","\n","def get_model_type_string(config):\n","    \"\"\"\n","    GÃ©nÃ¨re une chaÃ®ne de caractÃ¨res dÃ©crivant le type de modÃ¨le pour les noms de fichiers.\n","\n","    Args:\n","        config: Configuration contenant les paramÃ¨tres du modÃ¨le\n","\n","    Returns:\n","        ChaÃ®ne dÃ©crivant le type de modÃ¨le (ex: \"unet_film_cbam\")\n","    \"\"\"\n","    model_type = getattr(config, 'MODEL_TYPE', 'basic').lower()\n","\n","    if model_type == 'basic':\n","        return \"unet\"\n","    elif model_type in ['film', 'cbam', 'droppath']:\n","        return f\"unet_{model_type}\"\n","    elif model_type == 'film_cbam':\n","        return \"unet_film_cbam\"\n","    elif model_type == 'all':\n","        return \"unet_advanced\"\n","    else:\n","        return \"unet_custom\"\n","\n","\n","def setup_model_paths(config):\n","    \"\"\"\n","    Configure les chemins de sauvegarde spÃ©cifiques au type de modÃ¨le.\n","\n","    Args:\n","        config: Configuration Ã  modifier\n","\n","    Returns:\n","        Configuration avec chemins mis Ã  jour\n","    \"\"\"\n","    # Obtenir la chaÃ®ne dÃ©crivant le type de modÃ¨le\n","    model_type_str = get_model_type_string(config)\n","\n","    # CrÃ©er des sous-rÃ©pertoires pour le type de modÃ¨le\n","    base_dir = config.MODELS_DIR\n","    model_dir = os.path.join(base_dir, model_type_str)\n","\n","    # Mettre Ã  jour les chemins dans la configuration\n","    config.UNET_DIR = model_dir\n","    config.CHECKPOINTS_DIR = os.path.join(model_dir, 'checkpoints')\n","    config.LOGS_DIR = os.path.join(model_dir, 'logs')\n","    config.RESULTS_DIR = os.path.join(model_dir, 'results')\n","    config.VISUALIZATIONS_DIR = os.path.join(model_dir, 'visualizations')\n","\n","    # CrÃ©er les rÃ©pertoires s'ils n'existent pas\n","    for dir_path in [\n","        config.CHECKPOINTS_DIR, config.LOGS_DIR,\n","        config.RESULTS_DIR, config.VISUALIZATIONS_DIR\n","    ]:\n","        os.makedirs(dir_path, exist_ok=True)\n","\n","    return config"]},{"cell_type":"markdown","metadata":{"id":"DjdgY47yCiV-"},"source":["## SECTION 7: FONCTIONS D'ENTRAÃŽNEMENT ET D'Ã‰VALUATION"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":44245,"status":"ok","timestamp":1741117035334,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"A5pdMELmi7kO"},"outputs":[],"source":["# =====================================================================\n","# SECTION 7: FONCTIONS D'ENTRAÃŽNEMENT ET D'Ã‰VALUATION\n","# =====================================================================\n","\n","def clear_gpu_memory():\n","    \"\"\"Vide la mÃ©moire GPU non utilisÃ©e\"\"\"\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","def train_epoch(model, data_loader, criterion, optimizer, device, segmentation_metrics, report_interval=10):\n","    \"\"\"\n","    EntraÃ®ne le modÃ¨le pendant une Ã©poque avec des mÃ©triques amÃ©liorÃ©es et une barre de progression.\n","    Remarque: L'Ã©quilibrage des classes est maintenant gÃ©rÃ© par les indices prÃ©-calculÃ©s,\n","    ce qui amÃ©liore les performances en Ã©vitant l'overhead liÃ© au sampler.\n","\n","    Args:\n","        model: ModÃ¨le PyTorch\n","        data_loader: DataLoader pour les donnÃ©es d'entraÃ®nement\n","        criterion: Fonction de perte\n","        optimizer: Optimiseur\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        segmentation_metrics: Objet SegmentationMetrics pour calculer diverses mÃ©triques\n","        report_interval: Intervalle pour la mise Ã  jour de la barre de progression\n","\n","    Returns:\n","        Dictionnaire de mÃ©triques d'entraÃ®nement\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    total_samples = 0\n","\n","    # RÃ©initialiser les mÃ©triques\n","    segmentation_metrics.reset()\n","\n","    # Initialiser le scaler pour la prÃ©cision mixte\n","    scaler = torch.amp.GradScaler('cuda')\n","\n","    # Configuration amÃ©liorÃ©e de tqdm pour une barre de progression nette\n","    progress_bar = tqdm(\n","        total=len(data_loader),\n","        desc=\"EntraÃ®nement\",\n","        position=0,\n","        leave=True,\n","        bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}',\n","        dynamic_ncols=True\n","    )\n","\n","    for batch_idx, (dsm, threshold, target) in enumerate(data_loader):\n","        # DÃ©placer les tenseurs vers le pÃ©riphÃ©rique\n","        dsm = dsm.to(device)\n","        threshold = threshold.to(device)\n","        target = target.to(device)\n","\n","        # Forward pass avec prÃ©cision mixte\n","        optimizer.zero_grad()\n","\n","        with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n","            output = model(dsm, threshold)\n","            loss = criterion(output, target, threshold)\n","\n","        # Backward pass et optimisation avec scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Mettre Ã  jour les mÃ©triques\n","        with torch.no_grad():\n","            segmentation_metrics.update(output, target)\n","            # Mettre Ã  jour les mÃ©triques spÃ©cifiques au seuil\n","            threshold_value = threshold[0].item() * max(data_loader.dataset.thresholds)\n","            closest_threshold = min(data_loader.dataset.thresholds, key=lambda x: abs(x - threshold_value))\n","            segmentation_metrics.update_by_threshold(output, target, closest_threshold)\n","\n","        # Mettre Ã  jour les statistiques\n","        batch_size = dsm.size(0)\n","        running_loss += loss.item() * batch_size\n","        total_samples += batch_size\n","\n","        # Mettre Ã  jour la barre de progression Ã  intervalles rÃ©guliers\n","        if (batch_idx + 1) % report_interval == 0 or batch_idx == len(data_loader) - 1:\n","            # Calculer les mÃ©triques en cours\n","            current_metrics = segmentation_metrics.compute()\n","\n","            # PrÃ©parer les mÃ©triques Ã  afficher\n","            metrics_str = f\"loss={running_loss/total_samples:.4f}\"\n","            for metric_name in ['iou', 'accuracy', 'f1_score']:\n","                if metric_name in current_metrics:\n","                    metrics_str += f\", {metric_name}={current_metrics[metric_name]:.4f}\"\n","\n","            # Mettre Ã  jour la barre de progression\n","            progress_bar.set_postfix_str(metrics_str)\n","            progress_bar.update(min(report_interval, len(data_loader) - progress_bar.n))\n","\n","    # Fermer la barre de progression\n","    progress_bar.close()\n","\n","    # Calculer les mÃ©triques finales\n","    epoch_loss = running_loss / total_samples\n","    metrics = segmentation_metrics.compute()\n","\n","    # PrÃ©parer le dictionnaire de retour\n","    result_metrics = {\n","        'loss': epoch_loss,\n","        'iou': metrics['iou']\n","    }\n","\n","    # Ajouter d'autres mÃ©triques\n","    for metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'kappa']:\n","        if metric_name in metrics:\n","            result_metrics[metric_name] = metrics[metric_name]\n","\n","    # Ajouter les mÃ©triques par seuil\n","    if 'threshold_metrics' in metrics:\n","        result_metrics['threshold_metrics'] = metrics['threshold_metrics']\n","\n","    clear_gpu_memory()\n","    return result_metrics\n","\n","def validate(model, data_loader, criterion, device, segmentation_metrics):\n","    \"\"\"\n","    Ã‰value le modÃ¨le sur l'ensemble de validation avec des mÃ©triques amÃ©liorÃ©es.\n","\n","    Args:\n","        model: ModÃ¨le PyTorch\n","        data_loader: DataLoader pour les donnÃ©es de validation\n","        criterion: Fonction de perte\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        segmentation_metrics: Objet SegmentationMetrics pour calculer diverses mÃ©triques\n","\n","    Returns:\n","        Dictionnaire de mÃ©triques de validation\n","    \"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    total_samples = 0\n","\n","    # RÃ©initialiser les mÃ©triques\n","    segmentation_metrics.reset()\n","\n","    progress_bar = tqdm(\n","        total=len(data_loader),\n","        desc=\"Validation\",\n","        position=0,\n","        leave=True,\n","        bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}',\n","        dynamic_ncols=True\n","    )\n","\n","    with torch.no_grad():\n","        for batch_idx, (dsm, threshold, target) in enumerate(data_loader):\n","            # DÃ©placer les tenseurs vers le pÃ©riphÃ©rique\n","            dsm = dsm.to(device)\n","            threshold = threshold.to(device)\n","            target = target.to(device)\n","\n","            # Forward pass\n","            output = model(dsm, threshold)\n","            loss = criterion(output, target, threshold)\n","\n","            # Mettre Ã  jour les mÃ©triques\n","            segmentation_metrics.update(output, target)\n","\n","            # Mettre Ã  jour les mÃ©triques spÃ©cifiques au seuil\n","            threshold_value = threshold[0].item() * max(data_loader.dataset.thresholds)\n","            closest_threshold = min(data_loader.dataset.thresholds, key=lambda x: abs(x - threshold_value))\n","            segmentation_metrics.update_by_threshold(output, target, closest_threshold)\n","\n","            # Mettre Ã  jour les statistiques\n","            batch_size = dsm.size(0)\n","            running_loss += loss.item() * batch_size\n","            total_samples += batch_size\n","\n","            # Mettre Ã  jour la barre de progression\n","            progress_bar.update(1)\n","\n","    # Fermer la barre de progression\n","    progress_bar.close()\n","\n","    # Calculer les mÃ©triques finales\n","    epoch_loss = running_loss / total_samples\n","    metrics = segmentation_metrics.compute()\n","\n","    # PrÃ©parer le dictionnaire de retour\n","    result_metrics = {\n","        'loss': epoch_loss,\n","        'iou': metrics['iou']\n","    }\n","\n","    # Ajouter d'autres mÃ©triques\n","    for metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'kappa']:\n","        if metric_name in metrics:\n","            result_metrics[metric_name] = metrics[metric_name]\n","\n","    # Ajouter les mÃ©triques par seuil\n","    if 'threshold_metrics' in metrics:\n","        result_metrics['threshold_metrics'] = metrics['threshold_metrics']\n","\n","    return result_metrics\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, config, device, callbacks=None, is_test_mode=False, droppath_scheduler=None):\n","    \"\"\"\n","    Fonction principale pour l'entraÃ®nement du modÃ¨le avec suivi amÃ©liorÃ© des mÃ©triques.\n","\n","    Args:\n","        model: ModÃ¨le U-Net\n","        train_loader: DataLoader pour les donnÃ©es d'entraÃ®nement\n","        val_loader: DataLoader pour les donnÃ©es de validation\n","        criterion: Fonction de perte\n","        optimizer: Optimiseur\n","        scheduler: Scheduler pour le taux d'apprentissage\n","        config: Configuration du projet\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        callbacks: Objet contenant des mÃ©thodes de callback pour personnaliser le processus\n","        is_test_mode: BoolÃ©en indiquant si on est en mode test\n","        droppath_scheduler: Scheduler pour DropPath (optionnel)\n","\n","    Returns:\n","        Tuple (model, tracker)\n","    \"\"\"\n","    # Fixer l'avertissement de future warning de torch.load\n","    import warnings\n","    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch.serialization\")\n","\n","    # Initialiser le tracker de mÃ©triques avec TensorBoard\n","    tracker = LossTracker(log_dir=config.LOGS_DIR)\n","\n","    # Ajouter les mÃ©triques supplÃ©mentaires Ã  suivre\n","    for metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'kappa']:\n","        # Utiliser la mÃ©thode add_metric modifiÃ©e pour Ã©viter les messages redondants\n","        if metric_name not in tracker.metric_names:\n","            tracker.metric_names.append(metric_name)\n","            setattr(tracker, f'train_{metric_name}', [])\n","            setattr(tracker, f'val_{metric_name}', [])\n","            tracker.best_metrics[metric_name] = 0.0\n","            tracker.metrics_history['train'][metric_name] = []\n","            tracker.metrics_history['val'][metric_name] = []\n","\n","    log_file = os.path.join(config.LOGS_DIR, 'training_log.csv')\n","    tracker.set_log_file(log_file)\n","\n","    # Initialiser les objets pour le calcul des mÃ©triques\n","    train_metrics_calculator = SegmentationMetrics(device)\n","    val_metrics_calculator = SegmentationMetrics(device)\n","\n","    # Chemin pour sauvegarder le meilleur modÃ¨le\n","    best_model_path = os.path.join(config.CHECKPOINTS_DIR, 'best_model.pth')\n","\n","    # VÃ©rifier s'il existe un checkpoint pour reprendre l'entraÃ®nement\n","    start_epoch = 0\n","    resume_training = not is_test_mode  # Ne pas reprendre en mode test\n","\n","    if os.path.exists(best_model_path) and resume_training:\n","        print(\"Un modÃ¨le prÃ©cÃ©dent a Ã©tÃ© trouvÃ©. Chargement du modÃ¨le pour reprendre l'entraÃ®nement...\")\n","        try:\n","            checkpoint = torch.load(best_model_path, map_location=device)\n","\n","            if 'epoch' in checkpoint:\n","                start_epoch = checkpoint['epoch'] + 1\n","                model.load_state_dict(checkpoint['model_state_dict'])\n","                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","                # DÃ©placer l'optimiseur sur le bon device si nÃ©cessaire\n","                for state in optimizer.state.values():\n","                    for k, v in state.items():\n","                        if isinstance(v, torch.Tensor):\n","                            state[k] = v.to(device)\n","\n","                if 'scheduler_state_dict' in checkpoint and scheduler is not None:\n","                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","                tracker_path = os.path.join(config.LOGS_DIR, 'metrics.pkl')\n","                if os.path.exists(tracker_path):\n","                    tracker.load(tracker_path)\n","\n","                print(f\"âœ… ModÃ¨le chargÃ© avec succÃ¨s! Reprise de l'entraÃ®nement Ã  partir de l'Ã©poque {start_epoch}\")\n","                print(f\"   Meilleur IoU prÃ©cÃ©dent: {tracker.best_val_iou:.4f}\")\n","\n","                # Ajouter le modÃ¨le Ã  TensorBoard pour visualiser sa structure\n","                try:\n","                    tracker.writer.add_graph(model,\n","                                            (torch.zeros(1, 1, config.TILE_SIZE, config.TILE_SIZE).to(device),\n","                                             torch.zeros(1, 1).to(device)))\n","                except Exception as e:\n","                    print(f\"Note: Impossible d'ajouter le graphe du modÃ¨le Ã  TensorBoard: {str(e)}\")\n","            else:\n","                print(\"âš ï¸ Checkpoint trouvÃ© mais sans information d'Ã©poque. DÃ©marrage Ã  l'Ã©poque 0.\")\n","        except Exception as e:\n","            print(f\"âŒ Erreur lors du chargement du checkpoint: {str(e)}\")\n","            print(\"   DÃ©marrage Ã  l'Ã©poque 0.\")\n","    else:\n","        if is_test_mode:\n","            print(\"Mode test: DÃ©marrage d'un nouvel entraÃ®nement.\")\n","        else:\n","            print(\"Aucun modÃ¨le prÃ©cÃ©dent trouvÃ©. DÃ©marrage d'un nouvel entraÃ®nement.\")\n","\n","        # Ajouter le modÃ¨le Ã  TensorBoard pour visualiser sa structure\n","        sample_input = (torch.zeros(1, 1, config.TILE_SIZE, config.TILE_SIZE).to(device),\n","                         torch.zeros(1, 1).to(device))\n","        try:\n","            tracker.writer.add_graph(model, sample_input)\n","        except Exception as e:\n","            print(f\"Note: Impossible d'ajouter le graphe du modÃ¨le Ã  TensorBoard: {str(e)}\")\n","\n","    # Si des callbacks sont fournis et possÃ¨dent la mÃ©thode before_training_start, l'appeler\n","    if callbacks and hasattr(callbacks, 'before_training_start'):\n","        custom_start_epoch = callbacks.before_training_start(model, optimizer, tracker)\n","        if custom_start_epoch is not None:\n","            start_epoch = custom_start_epoch\n","\n","    # CritÃ¨res d'arrÃªt prÃ©coce\n","    patience = 10\n","    min_delta = 0.001\n","\n","    # EntraÃ®nement par Ã©poque\n","    print(f\"DÃ©but de l'entraÃ®nement pour {config.EPOCHS} Ã©poques\")\n","    start_time = time.time()\n","\n","    # Initialiser la variable epoch en dehors de la boucle pour Ã©viter l'erreur \"cannot access local variable 'epoch'\"\n","    epoch = start_epoch\n","\n","    for epoch in range(start_epoch, config.EPOCHS):\n","        epoch_start = time.time()\n","\n","        # Appliquer DropPath scheduler si prÃ©sent\n","        if droppath_scheduler is not None:\n","            droppath_scheduler.step(epoch)\n","\n","        # EntraÃ®nement et validation\n","        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, train_metrics_calculator)\n","        val_metrics = validate(model, val_loader, criterion, device, val_metrics_calculator)\n","\n","        # Mise Ã  jour du scheduler\n","        if scheduler is not None:\n","            if isinstance(scheduler, ReduceLROnPlateau):\n","                scheduler.step(val_metrics['loss'])\n","            else:\n","                scheduler.step()\n","\n","        # Obtenir le taux d'apprentissage actuel\n","        current_lr = optimizer.param_groups[0]['lr']\n","\n","        # Mettre Ã  jour le tracker\n","        improved = tracker.update(train_metrics, val_metrics, current_lr)\n","\n","        # Ajouter des visualisations de prÃ©dictions Ã  TensorBoard (tous les 5 Ã©poques)\n","        if (epoch % 5 == 0 or epoch == config.EPOCHS - 1) and not is_test_mode:\n","            for threshold in config.THRESHOLDS:\n","                visualize_predictions_tensorboard(\n","                    model, val_loader, device, tracker.writer, epoch, threshold, num_samples=4\n","                )\n","\n","        # Afficher les mÃ©triques\n","        epoch_time = time.time() - epoch_start\n","\n","        # Afficher un rÃ©sumÃ© des mÃ©triques principales\n","        metrics_summary = (\n","            f\"Ã‰poque {epoch+1}/{config.EPOCHS} | \"\n","            f\"Train Loss: {train_metrics['loss']:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n","            f\"Train IoU: {train_metrics['iou']:.4f} | Val IoU: {val_metrics['iou']:.4f} | \"\n","        )\n","\n","        # Ajouter d'autres mÃ©triques si disponibles\n","        for metric_name in ['accuracy', 'f1_score']:\n","            if metric_name in train_metrics and metric_name in val_metrics:\n","                metrics_summary += f\"Val {metric_name.capitalize()}: {val_metrics[metric_name]:.4f} | \"\n","\n","        metrics_summary += f\"LR: {current_lr:.6f} | Temps: {epoch_time:.2f}s\"\n","        print(metrics_summary)\n","\n","        # Si des callbacks sont fournis et possÃ¨dent la mÃ©thode after_epoch, l'appeler\n","        continue_training = True\n","        if callbacks and hasattr(callbacks, 'after_epoch'):\n","            continue_training = callbacks.after_epoch(\n","                epoch, model, optimizer, train_metrics, val_metrics, tracker, improved\n","            )\n","            if continue_training is False:\n","                print(\"EntraÃ®nement interrompu par le callback after_epoch\")\n","                break\n","\n","        # Sauvegarder le meilleur modÃ¨le\n","        should_save = True\n","        if callbacks and hasattr(callbacks, 'should_save_checkpoint'):\n","            should_save = callbacks.should_save_checkpoint(epoch, model, optimizer, improved)\n","\n","        if improved and should_save:\n","            checkpoint = {\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'val_loss': val_metrics['loss'],\n","                'val_iou': val_metrics['iou'],\n","                'val_metrics': val_metrics\n","            }\n","\n","            if scheduler is not None:\n","                checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n","\n","            torch.save(checkpoint, best_model_path)\n","            print(f\"âœ… ModÃ¨le sauvegardÃ©: {best_model_path}\")\n","\n","            # Sauvegarder les mÃ©triques\n","            tracker.save(os.path.join(config.LOGS_DIR, 'metrics.pkl'))\n","\n","        # VÃ©rifier les critÃ¨res d'arrÃªt prÃ©coce\n","        if tracker.epochs_without_improvement >= patience:\n","            print(f\"ArrÃªt prÃ©coce aprÃ¨s {epoch+1} Ã©poques sans amÃ©lioration.\")\n","            break\n","\n","    # Calculer le temps total d'entraÃ®nement\n","    total_time = time.time() - start_time\n","    print(f\"EntraÃ®nement terminÃ© en {total_time/60:.2f} minutes\")\n","\n","    # Stocker le temps d'entraÃ®nement dans le tracker\n","    tracker.train_time = total_time\n","\n","    # Ajouter des hyperparamÃ¨tres Ã  TensorBoard\n","    hparams = {\n","        'batch_size': config.BATCH_SIZE,\n","        'learning_rate': config.LEARNING_RATE,\n","        'dropout_rate': config.DROPOUT_RATE,\n","        'thresholds': str(config.THRESHOLDS),\n","        'epochs': epoch + 1,\n","        'early_stopping_patience': patience,\n","    }\n","\n","    metrics = {\n","        'hparam/best_val_loss': tracker.best_val_loss,\n","        'hparam/best_val_iou': tracker.best_val_iou,\n","        'hparam/train_time_minutes': total_time/60\n","    }\n","\n","    tracker.writer.add_hparams(hparams, metrics)\n","\n","    # Visualiser l'Ã©volution des mÃ©triques\n","    visualize_metrics_evolution(\n","        tracker,\n","        save_path=os.path.join(config.VISUALIZATIONS_DIR, 'learning_curves.png')\n","    )\n","\n","    # Charger le meilleur modÃ¨le\n","    try:\n","        checkpoint = torch.load(best_model_path, map_location=device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        print(f\"Meilleur modÃ¨le chargÃ© (Ã©poque {checkpoint['epoch']+1})\")\n","    except Exception as e:\n","        print(f\"âš ï¸ Impossible de charger le meilleur modÃ¨le: {str(e)}\")\n","        print(\"Utilisation du modÃ¨le actuel comme meilleur modÃ¨le.\")\n","\n","    return model, tracker\n","\n","def test_model(model, test_loader, device, config, segmentation_metrics=None, threshold=0.5, visualize=True):\n","    \"\"\"\n","    Ã‰value le modÃ¨le sur l'ensemble de test avec Ã©valuation complÃ¨te des mÃ©triques.\n","\n","    Args:\n","        model: ModÃ¨le PyTorch\n","        test_loader: DataLoader pour les donnÃ©es de test\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        config: Configuration\n","        segmentation_metrics: Objet SegmentationMetrics (crÃ©Ã© si None)\n","        threshold: Seuil pour binariser les prÃ©dictions\n","        visualize: Si True, gÃ©nÃ¨re des visualisations\n","\n","    Returns:\n","        Dictionnaire de mÃ©triques et rÃ©sultats visuels\n","    \"\"\"\n","    model.eval()\n","\n","    # CrÃ©er l'objet de mÃ©triques si non fourni\n","    if segmentation_metrics is None:\n","        segmentation_metrics = SegmentationMetrics(device)\n","    else:\n","        segmentation_metrics.reset()\n","\n","    # Collecter quelques exemples pour visualisation\n","    visualization_samples = []\n","\n","    progress_bar = tqdm(\n","        total=len(test_loader),\n","        desc=\"Test\",\n","        position=0,\n","        leave=True,\n","        bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}',\n","        dynamic_ncols=True\n","    )\n","\n","    try:\n","        with torch.no_grad():\n","            for dsm, threshold_val, target in test_loader:\n","                # GÃ©rer le cas oÃ¹ le batch est vide\n","                if dsm.size(0) == 0:\n","                    continue\n","\n","                # DÃ©placer les tenseurs vers le pÃ©riphÃ©rique\n","                dsm = dsm.to(device)\n","                threshold_val = threshold_val.to(device)\n","                target = target.to(device)\n","\n","                # Forward pass\n","                output = model(dsm, threshold_val)\n","\n","                # Mettre Ã  jour les mÃ©triques\n","                segmentation_metrics.update(output, target)\n","\n","                # Mettre Ã  jour les mÃ©triques spÃ©cifiques au seuil\n","                threshold_value = threshold_val[0].item() * max(test_loader.dataset.thresholds)\n","                closest_threshold = min(test_loader.dataset.thresholds, key=lambda x: abs(x - threshold_value))\n","                segmentation_metrics.update_by_threshold(output, target, closest_threshold)\n","\n","                # Collecter quelques Ã©chantillons pour visualisation\n","                if visualize and len(visualization_samples) < 5:\n","                    for i in range(min(dsm.size(0), 5 - len(visualization_samples))):\n","                        visualization_samples.append({\n","                            'dsm': dsm[i].cpu().numpy(),\n","                            'target': target[i].cpu().numpy(),\n","                            'output': output[i].cpu().numpy(),\n","                            'threshold': threshold_val[i].item() * max(config.THRESHOLDS)\n","                        })\n","\n","                # Mettre Ã  jour la barre de progression\n","                progress_bar.update(1)\n","\n","        # Fermer la barre de progression\n","        progress_bar.close()\n","\n","        # Calculer les mÃ©triques\n","        metrics = segmentation_metrics.compute()\n","        confusion_data = segmentation_metrics.compute_confusion_matrix()\n","\n","        # Afficher le tableau des mÃ©triques\n","        metrics_table = create_metrics_tables(metrics, title=\"MÃ©triques d'Ã©valuation sur l'ensemble de test\")\n","        print(metrics_table)\n","\n","        # CrÃ©er des visualisations si demandÃ©\n","        results = {\n","            'metrics': metrics,\n","            'confusion_data': confusion_data\n","        }\n","\n","        if visualize and visualization_samples:\n","            try:\n","                # Visualisation de la matrice de confusion\n","                confusion_fig = visualize_confusion_matrix(\n","                    confusion_data,\n","                    title=\"Matrice de confusion sur l'ensemble de test\",\n","                    save_path=os.path.join(config.VISUALIZATIONS_DIR, 'test_confusion_matrix.png')\n","                )\n","                results['confusion_fig'] = confusion_fig\n","\n","                # Visualisation des mÃ©triques par seuil\n","                if 'threshold_metrics' in metrics and metrics['threshold_metrics']:\n","                    threshold_fig = visualize_metrics_by_threshold(\n","                        metrics,\n","                        title=\"MÃ©triques par seuil sur l'ensemble de test\",\n","                        save_path=os.path.join(config.VISUALIZATIONS_DIR, 'test_metrics_by_threshold.png')\n","                    )\n","                    results['threshold_fig'] = threshold_fig\n","\n","                # Visualisation des prÃ©dictions\n","                predictions_fig = visualize_predictions_grid(\n","                    model, test_loader, device, config,\n","                    num_samples=5, threshold_idx=0\n","                )\n","                plt.savefig(os.path.join(config.VISUALIZATIONS_DIR, 'test_predictions.png'), dpi=300, bbox_inches='tight')\n","                plt.show()\n","                results['predictions_fig'] = predictions_fig\n","\n","                # Visualisation comparative par seuil\n","                comparison_fig = visualize_threshold_comparison(\n","                    model, test_loader, device, config,\n","                    sample_idx=0\n","                )\n","                if comparison_fig is not None:\n","                    plt.savefig(os.path.join(config.VISUALIZATIONS_DIR, 'test_threshold_comparison.png'), dpi=300, bbox_inches='tight')\n","                    plt.show()\n","                    results['comparison_fig'] = comparison_fig\n","\n","            except Exception as e:\n","                print(f\"Erreur lors de la visualisation: {str(e)}\")\n","                print(traceback.format_exc())\n","\n","        return results\n","\n","    except Exception as e:\n","        progress_bar.close()\n","        print(f\"Erreur lors de l'Ã©valuation du modÃ¨le: {str(e)}\")\n","        print(traceback.format_exc())\n","\n","        # Retourner des mÃ©triques par dÃ©faut en cas d'erreur\n","        return {\n","            'metrics': {'iou': 0.0, 'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 'kappa': 0.0},\n","            'confusion_data': {'matrix': {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}, 'percentages': {}, 'total_pixels': 0}\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"sIJe4twevIrz"},"source":["## SECTION 8: FONCTIONS DE PRÃ‰DICTION ET UTILITAIRES"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1741117035369,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"KR2vDxTpjEH_"},"outputs":[],"source":["# =====================================================================\n","# SECTION 8: FONCTIONS DE PRÃ‰DICTION ET UTILITAIRES\n","# =====================================================================\n","\n","def predict_on_tiles(model, tile_info, threshold_value, device, config):\n","    \"\"\"\n","    Fait des prÃ©dictions sur un ensemble de tuiles.\n","\n","    Args:\n","        model: ModÃ¨le PyTorch\n","        tile_info: Liste d'informations sur les tuiles\n","        threshold_value: Seuil de hauteur pour la segmentation\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        config: Configuration\n","\n","    Returns:\n","        Liste des prÃ©dictions pour chaque tuile\n","    \"\"\"\n","    model.eval()\n","    predictions = []\n","\n","    # Normaliser le seuil\n","    normalized_threshold = threshold_value / max(config.THRESHOLDS)\n","    threshold_tensor = torch.tensor([[normalized_threshold]], dtype=torch.float32).to(device)\n","\n","    with torch.no_grad():\n","        for info in tqdm(tile_info, desc=f\"PrÃ©dictions (seuil {threshold_value}m)\"):\n","            # Charger la tuile DSM\n","            dsm_tile = np.load(info['dsm_path'])\n","\n","            # Normaliser\n","            dsm_valid = ~np.isnan(dsm_tile)\n","            if np.any(dsm_valid):\n","                dsm_min = np.nanmin(dsm_tile)\n","                dsm_max = np.nanmax(dsm_tile)\n","                dsm_range = dsm_max - dsm_min\n","                if dsm_range > 0:\n","                    dsm_tile = np.where(dsm_valid, (dsm_tile - dsm_min) / dsm_range, 0)\n","                else:\n","                    dsm_tile = np.where(dsm_valid, 0, 0)\n","\n","            # Convertir en tensor\n","            dsm_tensor = torch.tensor(dsm_tile, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n","\n","            # Faire la prÃ©diction\n","            output = model(dsm_tensor, threshold_tensor)\n","\n","            # Convertir en NumPy\n","            prediction = torch.sigmoid(output).squeeze().cpu().numpy()\n","\n","            predictions.append({\n","                'site': info['site'],\n","                'row_idx': info['row_idx'],\n","                'col_idx': info['col_idx'],\n","                'prediction': prediction\n","            })\n","\n","    return predictions\n","\n","def visualize_predictions_with_actual(model, tile_info, threshold_value, device, config, num_samples=5):\n","    \"\"\"\n","    Visualise les prÃ©dictions du modÃ¨le comparÃ©es aux masques rÃ©els.\n","\n","    Args:\n","        model: ModÃ¨le PyTorch\n","        tile_info: Liste d'informations sur les tuiles\n","        threshold_value: Seuil de hauteur pour la segmentation\n","        device: PÃ©riphÃ©rique (CPU ou GPU)\n","        config: Configuration\n","        num_samples: Nombre d'exemples Ã  visualiser\n","    \"\"\"\n","    model.eval()\n","\n","    # SÃ©lectionner des tuiles alÃ©atoires\n","    indices = np.random.choice(len(tile_info), num_samples, replace=False)\n","\n","    # Normaliser le seuil\n","    normalized_threshold = threshold_value / max(config.THRESHOLDS)\n","    threshold_tensor = torch.tensor([[normalized_threshold]], dtype=torch.float32).to(device)\n","\n","    # CrÃ©er la figure\n","    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n","\n","    with torch.no_grad():\n","        for i, idx in enumerate(indices):\n","            info = tile_info[idx]\n","\n","            # Charger la tuile DSM\n","            dsm_tile = np.load(info['dsm_path'])\n","\n","            # Charger le masque rÃ©el\n","            mask_path = info['mask_paths'][threshold_value]\n","            mask_tile = np.load(mask_path)\n","\n","            # Convertir le masque en binaire\n","            mask_valid = (mask_tile != 255)\n","            mask_binary = np.where(mask_valid, (mask_tile > 0).astype(np.float32), 0)\n","\n","            # Normaliser la tuile DSM\n","            dsm_valid = ~np.isnan(dsm_tile)\n","            dsm_display = dsm_tile.copy()  # Pour l'affichage\n","\n","            if np.any(dsm_valid):\n","                dsm_min = np.nanmin(dsm_tile)\n","                dsm_max = np.nanmax(dsm_tile)\n","                dsm_range = dsm_max - dsm_min\n","                if dsm_range > 0:\n","                    dsm_tile = np.where(dsm_valid, (dsm_tile - dsm_min) / dsm_range, 0)\n","                else:\n","                    dsm_tile = np.where(dsm_valid, 0, 0)\n","\n","            # Convertir en tensor\n","            dsm_tensor = torch.tensor(dsm_tile, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n","\n","            # Faire la prÃ©diction\n","            output = model(dsm_tensor, threshold_tensor)\n","\n","            # Convertir en NumPy\n","            prediction = output.squeeze().cpu().numpy()\n","\n","            # Calculer l'IoU\n","            pred_tensor = torch.sigmoid(torch.tensor(prediction, dtype=torch.float32)).unsqueeze(0).unsqueeze(0)\n","            mask_tensor = torch.tensor(mask_binary, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","            iou = iou_metric(pred_tensor, mask_tensor).item()\n","\n","            # Afficher\n","            axes[i, 0].imshow(dsm_display, cmap='terrain')\n","            axes[i, 0].set_title(f\"DSM - {info['site']}\")\n","\n","            axes[i, 1].imshow(mask_binary, cmap='binary')\n","            axes[i, 1].set_title(f\"TrouÃ©es rÃ©elles (seuil {threshold_value}m)\")\n","\n","            axes[i][2].imshow(torch.sigmoid(torch.tensor(prediction)).numpy() > 0.5, cmap='binary')\n","            axes[i, 2].set_title(f\"TrouÃ©es prÃ©dites (IoU: {iou:.4f})\")\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config.VISUALIZATIONS_DIR, f'predictions_threshold_{threshold_value}m.png'), dpi=300)\n","    plt.show()\n","\n","def load_or_create_dummy_data(config):\n","    \"\"\"\n","    Fonction utilitaire pour charger les informations sur les tuiles\n","    ou crÃ©er des donnÃ©es fictives si nÃ©cessaire.\n","\n","    Args:\n","        config: Configuration du projet\n","\n","    Returns:\n","        Tuple (train_tile_info, val_tile_info, test_tile_info)\n","    \"\"\"\n","    train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","    val_info_path = os.path.join(config.TILES_DIR, 'val_tile_info.pkl')\n","    test_info_path = os.path.join(config.TILES_DIR, 'test_tile_info.pkl')\n","\n","    try:\n","        # Essayer de charger les fichiers d'information\n","        if os.path.exists(train_info_path):\n","            with open(train_info_path, 'rb') as f:\n","                train_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {train_info_path}\")\n","\n","        if os.path.exists(val_info_path):\n","            with open(val_info_path, 'rb') as f:\n","                val_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {val_info_path}\")\n","\n","        if os.path.exists(test_info_path):\n","            with open(test_info_path, 'rb') as f:\n","                test_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {test_info_path}\")\n","\n","    except (FileNotFoundError, pickle.UnpicklingError) as e:\n","        print(f\"ERREUR lors du chargement des fichiers: {str(e)}\")\n","\n","        # CrÃ©er des donnÃ©es factices pour test\n","        print(\"CrÃ©ation de donnÃ©es factices pour dÃ©veloppement...\")\n","\n","        def create_dummy_tile_info(num_tiles):\n","            dummy_info = []\n","            tile_size = config.TILE_SIZE\n","\n","            for i in range(num_tiles):\n","                # CrÃ©er des faux chemins de fichiers\n","                dsm_path = f\"/tmp/dummy_dsm_{i}.npy\"\n","                mask_paths = {t: f\"/tmp/dummy_mask_{i}_{t}m.npy\" for t in config.THRESHOLDS}\n","\n","                # CrÃ©er des donnÃ©es factices pour simuler les tuiles\n","                if not os.path.exists(dsm_path):\n","                    np.save(dsm_path, np.random.rand(tile_size, tile_size))\n","\n","                for t, path in mask_paths.items():\n","                    if not os.path.exists(path):\n","                        np.save(path, (np.random.rand(tile_size, tile_size) > 0.7).astype(np.uint8))\n","\n","                dummy_info.append({\n","                    'site': f'dummy_site_{i//10}',\n","                    'row_idx': i % 10,\n","                    'col_idx': i % 10,\n","                    'window': None,\n","                    'dsm_path': dsm_path,\n","                    'mask_paths': mask_paths\n","                })\n","            return dummy_info\n","\n","        # CrÃ©er des donnÃ©es factices\n","        train_tile_info = create_dummy_tile_info(100)\n","        val_tile_info = create_dummy_tile_info(20)\n","        test_tile_info = create_dummy_tile_info(20)\n","\n","    return train_tile_info, val_tile_info, test_tile_info"]},{"cell_type":"markdown","metadata":{"id":"N0t9aiah4N1h"},"source":["## SECTION X: FONCTIONS DE VISUALISATION ET REPORTING"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1741117035378,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"x2KHk4XI5Diz"},"outputs":[],"source":["# =====================================================================\n","# SECTION X: FONCTIONS DE VISUALISATION ET REPORTING\n","# =====================================================================\n","\n","def visualize_metrics_evolution(tracker, save_path=None):\n","    \"\"\"\n","    Visualise l'Ã©volution de toutes les mÃ©triques pendant l'entraÃ®nement.\n","\n","    Args:\n","        tracker: Objet LossTracker contenant l'historique des mÃ©triques\n","        save_path: Chemin pour sauvegarder la figure (optionnel)\n","    \"\"\"\n","    # RÃ©cupÃ©rer les donnÃ©es d'historique\n","    history = {\n","        'epochs': list(range(1, len(tracker.train_losses) + 1)),\n","        'train_loss': tracker.train_losses,\n","        'val_loss': tracker.val_losses,\n","        'train_iou': tracker.train_iou,\n","        'val_iou': tracker.val_iou,\n","        'lr': tracker.lr_history\n","    }\n","\n","    # Ajouter d'autres mÃ©triques si disponibles\n","    for metric_name in ['accuracy', 'precision', 'recall', 'f1_score', 'kappa']:\n","        train_key = f'train_{metric_name}'\n","        val_key = f'val_{metric_name}'\n","\n","        if hasattr(tracker, train_key) and hasattr(tracker, val_key):\n","            history[train_key] = getattr(tracker, train_key)\n","            history[val_key] = getattr(tracker, val_key)\n","\n","    # CrÃ©er des sous-figures pour chaque groupe de mÃ©triques\n","    plt.figure(figsize=(20, 15))\n","\n","    # Nombre de lignes Ã  calculer en fonction des mÃ©triques disponibles\n","    metrics_count = 1  # Loss est toujours prÃ©sent\n","    for metric_pair in [('iou', 'IoU'), ('accuracy', 'Accuracy'),\n","                      ('precision', 'Precision'), ('recall', 'Recall'),\n","                      ('f1_score', 'F1-Score'), ('kappa', 'Kappa')]:\n","        metric_key, _ = metric_pair\n","        train_key = f'train_{metric_key}'\n","        if train_key in history:\n","            metrics_count += 1\n","\n","    # Ajout d'un graphique pour le taux d'apprentissage\n","    num_rows = metrics_count + 1\n","\n","    # 1. Graphique des pertes\n","    plt.subplot(num_rows, 1, 1)\n","    plt.plot(history['epochs'], history['train_loss'], 'b-', label='EntraÃ®nement')\n","    plt.plot(history['epochs'], history['val_loss'], 'r-', label='Validation')\n","    plt.xlabel('Ã‰poque')\n","    plt.ylabel('Perte')\n","    plt.title('Ã‰volution de la fonction de perte')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # 2. Graphiques pour chaque mÃ©trique disponible\n","    current_row = 2\n","\n","    for metric_pair in [('iou', 'IoU'), ('accuracy', 'Accuracy'),\n","                      ('precision', 'Precision'), ('recall', 'Recall'),\n","                      ('f1_score', 'F1-Score'), ('kappa', 'Kappa')]:\n","        metric_key, metric_name = metric_pair\n","        train_key = f'train_{metric_key}'\n","        val_key = f'val_{metric_key}'\n","\n","        if train_key in history:\n","            plt.subplot(num_rows, 1, current_row)\n","            plt.plot(history['epochs'], history[train_key], 'b-', label='EntraÃ®nement')\n","            plt.plot(history['epochs'], history[val_key], 'r-', label='Validation')\n","            plt.xlabel('Ã‰poque')\n","            plt.ylabel(metric_name)\n","            plt.title(f'Ã‰volution de {metric_name}')\n","            plt.legend()\n","            plt.grid(True)\n","            current_row += 1\n","\n","    # Graphique du taux d'apprentissage\n","    plt.subplot(num_rows, 1, num_rows)\n","    plt.plot(history['epochs'], history['lr'], 'g-')\n","    plt.xlabel('Ã‰poque')\n","    plt.ylabel('Taux d\\'apprentissage')\n","    plt.title('Ã‰volution du taux d\\'apprentissage')\n","    plt.grid(True)\n","    plt.yscale('log')\n","\n","    plt.tight_layout()\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=300)\n","        print(f\"Graphiques d'Ã©volution des mÃ©triques sauvegardÃ©s dans {save_path}\")\n","\n","    plt.show()\n","\n","\n","def visualize_metrics_by_threshold(metrics, title=\"MÃ©triques par seuil\", save_path=None):\n","    \"\"\"\n","    Visualise les mÃ©triques en fonction du seuil de hauteur.\n","\n","    Args:\n","        metrics: Dictionnaire de mÃ©triques retournÃ© par SegmentationMetrics.compute()\n","        title: Titre de la figure\n","        save_path: Chemin pour sauvegarder la figure (optionnel)\n","    \"\"\"\n","    if 'threshold_metrics' not in metrics or not metrics['threshold_metrics']:\n","        print(\"Aucune mÃ©trique par seuil disponible\")\n","        return\n","\n","    # Extraire les donnÃ©es\n","    thresholds = sorted(metrics['threshold_metrics'].keys())\n","\n","    # MÃ©triques Ã  visualiser\n","    metric_configs = [\n","        ('accuracy', 'Accuracy', 'tab:blue'),\n","        ('precision', 'Precision', 'tab:orange'),\n","        ('recall', 'Recall', 'tab:green'),\n","        ('f1_score', 'F1-Score', 'tab:red'),\n","        ('iou', 'IoU', 'tab:purple')\n","    ]\n","\n","    # CrÃ©er la figure\n","    plt.figure(figsize=(14, 10))\n","\n","    # Tracer chaque mÃ©trique\n","    for metric_key, metric_name, color in metric_configs:\n","        values = [metrics['threshold_metrics'][t][metric_key] for t in thresholds]\n","        plt.plot(thresholds, values, '-o', color=color, label=metric_name)\n","\n","    plt.xlabel('Seuil de hauteur (m)')\n","    plt.ylabel('Valeur')\n","    plt.title(title)\n","    plt.grid(True)\n","    plt.legend()\n","\n","    # Ajouter des annotations pour chaque point\n","    for metric_key, metric_name, color in metric_configs:\n","        for i, threshold in enumerate(thresholds):\n","            value = metrics['threshold_metrics'][threshold][metric_key]\n","            plt.annotate(\n","                f'{value:.3f}',\n","                (threshold, value),\n","                textcoords=\"offset points\",\n","                xytext=(0, 10),\n","                ha='center',\n","                fontsize=8,\n","                color=color\n","            )\n","\n","    # Ajuster les limites de l'axe y\n","    plt.ylim(0, 1.1)\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=300)\n","        print(f\"Graphique des mÃ©triques par seuil sauvegardÃ© dans {save_path}\")\n","\n","    plt.show()\n","\n","\n","def visualize_confusion_matrix(confusion_data, title=\"Matrice de confusion\", save_path=None):\n","    \"\"\"\n","    Visualise la matrice de confusion sous forme de heatmap.\n","\n","    Args:\n","        confusion_data: Dictionnaire retournÃ© par SegmentationMetrics.compute_confusion_matrix()\n","        title: Titre de la figure\n","        save_path: Chemin pour sauvegarder la figure (optionnel)\n","    \"\"\"\n","    # Extraire les valeurs de la matrice\n","    matrix = confusion_data['matrix']\n","    percentages = confusion_data['percentages']\n","\n","    # CrÃ©er la matrice 2x2\n","    conf_matrix = np.array([\n","        [matrix['tp'], matrix['fp']],\n","        [matrix['fn'], matrix['tn']]\n","    ])\n","\n","    # CrÃ©er la matrice des pourcentages\n","    percent_matrix = np.array([\n","        [percentages['tp_percent'], percentages['fp_percent']],\n","        [percentages['fn_percent'], percentages['tn_percent']]\n","    ])\n","\n","    # DÃ©finir les labels\n","    class_names = ['TrouÃ©e', 'Non-trouÃ©e']\n","\n","    # CrÃ©er la figure\n","    plt.figure(figsize=(10, 8))\n","\n","    # Afficher la heatmap\n","    im = plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","\n","    # Ajouter les titres et labels\n","    plt.title(title)\n","    plt.ylabel('VÃ©ritÃ© terrain')\n","    plt.xlabel('PrÃ©diction')\n","\n","    # Ajouter les classes\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names)\n","    plt.yticks(tick_marks, class_names)\n","\n","    # Ajouter les valeurs dans chaque cellule\n","    thresh = conf_matrix.max() / 2.\n","    for i in range(conf_matrix.shape[0]):\n","        for j in range(conf_matrix.shape[1]):\n","            plt.text(j, i, f\"{conf_matrix[i, j]}\\n({percent_matrix[i, j]:.2f}%)\",\n","                     horizontalalignment=\"center\",\n","                     verticalalignment=\"center\",\n","                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n","\n","    # Ajouter des informations supplÃ©mentaires\n","    info_text = f\"\"\"\n","    Total pixels: {confusion_data['total_pixels']:,}\n","\n","    Vrais positifs (TP): {matrix['tp']:,} ({percentages['tp_percent']:.2f}%)\n","    Faux positifs (FP): {matrix['fp']:,} ({percentages['fp_percent']:.2f}%)\n","    Vrais nÃ©gatifs (TN): {matrix['tn']:,} ({percentages['tn_percent']:.2f}%)\n","    Faux nÃ©gatifs (FN): {matrix['fn']:,} ({percentages['fn_percent']:.2f}%)\n","    \"\"\"\n","\n","    plt.figtext(0.5, -0.05, info_text, horizontalalignment='center', fontsize=10,\n","                bbox=dict(facecolor='white', alpha=0.8))\n","\n","    plt.tight_layout()\n","\n","    if save_path:\n","        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","        print(f\"Matrice de confusion sauvegardÃ©e dans {save_path}\")\n","\n","    plt.show()\n","\n","\n","def visualize_predictions_grid(model, data_loader, device, config, num_samples=5, threshold_idx=0):\n","    \"\"\"\n","    CrÃ©e une grille de visualisations des prÃ©dictions sur plusieurs Ã©chantillons.\n","\n","    Args:\n","        model: ModÃ¨le U-Net\n","        data_loader: DataLoader contenant les donnÃ©es\n","        device: PÃ©riphÃ©rique (GPU/CPU)\n","        config: Configuration\n","        num_samples: Nombre d'Ã©chantillons Ã  visualiser\n","        threshold_idx: Index du seuil de hauteur Ã  utiliser\n","\n","    Returns:\n","        Figure matplotlib\n","    \"\"\"\n","    model.eval()\n","\n","    # SÃ©lectionner le seuil\n","    threshold = config.THRESHOLDS[threshold_idx]\n","    normalized_threshold = threshold / max(config.THRESHOLDS)\n","    threshold_tensor = torch.tensor([[normalized_threshold]], dtype=torch.float32).to(device)\n","\n","    # Collecter les Ã©chantillons\n","    samples = []\n","    with torch.no_grad():\n","        for dsm, _, target in data_loader:\n","            # Prendre seulement le nombre d'Ã©chantillons nÃ©cessaires\n","            for i in range(min(dsm.size(0), num_samples - len(samples))):\n","                dsm_i = dsm[i:i+1].to(device)\n","                target_i = target[i:i+1].to(device)\n","\n","                # PrÃ©diction\n","                output_i = model(dsm_i, threshold_tensor)\n","                pred_binary = (torch.sigmoid(output_i) > 0.5).float()\n","\n","                # Calculer l'IoU pour cet Ã©chantillon\n","                iou = iou_metric(output_i, target_i).item()\n","\n","                # Ajouter l'Ã©chantillon\n","                samples.append({\n","                    'dsm': dsm_i.cpu().numpy()[0, 0],\n","                    'target': target_i.cpu().numpy()[0, 0],\n","                    'output': torch.sigmoid(output_i).cpu().numpy()[0, 0],\n","                    'pred_binary': pred_binary.cpu().numpy()[0, 0],\n","                    'iou': iou\n","                })\n","\n","                if len(samples) >= num_samples:\n","                    break\n","\n","            if len(samples) >= num_samples:\n","                break\n","\n","    # CrÃ©er la figure\n","    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n","\n","    # Titres des colonnes\n","    column_titles = ['DSM', 'VÃ©ritÃ© terrain', 'PrÃ©diction (probabilitÃ©)', 'PrÃ©diction (binaire)']\n","\n","    # Ajouter les titres des colonnes\n","    for j, title in enumerate(column_titles):\n","        fig.text(0.125 + j * 0.22, 0.95, title, ha='center', va='center', fontsize=14)\n","\n","    # Pour chaque Ã©chantillon\n","    for i, sample in enumerate(samples):\n","        # DSM\n","        im = axes[i, 0].imshow(sample['dsm'], cmap='terrain')\n","        fig.colorbar(im, ax=axes[i, 0], fraction=0.046, pad=0.04)\n","        axes[i, 0].set_title(f\"DSM\")\n","\n","        # VÃ©ritÃ© terrain\n","        axes[i, 1].imshow(sample['target'], cmap='binary', vmin=0, vmax=1)\n","        axes[i, 1].set_title(f\"TrouÃ©es rÃ©elles (seuil {threshold}m)\")\n","\n","        # PrÃ©diction (probabilitÃ©)\n","        im = axes[i, 2].imshow(sample['output'], cmap='plasma', vmin=0, vmax=1)\n","        fig.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n","        axes[i, 2].set_title(f\"ProbabilitÃ©\")\n","\n","        # PrÃ©diction (binaire)\n","        axes[i, 3].imshow(sample['pred_binary'], cmap='binary', vmin=0, vmax=1)\n","        axes[i, 3].set_title(f\"IoU: {sample['iou']:.4f}\")\n","\n","    # Supprimer les axes\n","    for ax in axes.flat:\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    return fig\n","\n","\n","def visualize_threshold_comparison(model, data_loader, device, config, sample_idx=0):\n","    \"\"\"\n","    Visualise les prÃ©dictions du modÃ¨le pour diffÃ©rents seuils de hauteur sur un mÃªme Ã©chantillon.\n","\n","    Args:\n","        model: ModÃ¨le U-Net\n","        data_loader: DataLoader contenant les donnÃ©es\n","        device: PÃ©riphÃ©rique (GPU/CPU)\n","        config: Configuration\n","        sample_idx: Index de l'Ã©chantillon Ã  visualiser\n","\n","    Returns:\n","        Figure matplotlib\n","    \"\"\"\n","    model.eval()\n","\n","    # Obtenir un Ã©chantillon\n","    sample_found = False\n","    dsm_sample = None\n","\n","    with torch.no_grad():\n","        for idx, (dsm, _, _) in enumerate(data_loader):\n","            if idx * data_loader.batch_size + sample_idx < len(data_loader.dataset):\n","                if sample_idx < dsm.size(0):\n","                    dsm_sample = dsm[sample_idx:sample_idx+1].to(device)\n","                    sample_found = True\n","                    break\n","\n","    if not sample_found or dsm_sample is None:\n","        print(f\"Ã‰chantillon Ã  l'index {sample_idx} non trouvÃ©\")\n","        return None\n","\n","    # CrÃ©er la figure\n","    rows = len(config.THRESHOLDS)\n","    fig, axes = plt.subplots(rows, 3, figsize=(12, 4 * rows))\n","\n","    # Afficher le DSM original seulement une fois\n","    dsm_np = dsm_sample.cpu().numpy()[0, 0]\n","\n","    # Pour chaque seuil\n","    for i, threshold in enumerate(config.THRESHOLDS):\n","        # Normaliser le seuil\n","        normalized_threshold = threshold / max(config.THRESHOLDS)\n","        threshold_tensor = torch.tensor([[normalized_threshold]], dtype=torch.float32).to(device)\n","\n","        # PrÃ©diction\n","        output = model(dsm_sample, threshold_tensor)\n","        pred_prob = torch.sigmoid(output).cpu().numpy()[0, 0]\n","        pred_binary = (pred_prob > 0.5).astype(float)\n","\n","        # DSM\n","        if i == 0:\n","            im = axes[i, 0].imshow(dsm_np, cmap='terrain')\n","            fig.colorbar(im, ax=axes[i, 0], fraction=0.046, pad=0.04)\n","        else:\n","            axes[i, 0].imshow(dsm_np, cmap='terrain')\n","\n","        axes[i, 0].set_title(f\"DSM (seuil {threshold}m)\")\n","\n","        # PrÃ©diction (probabilitÃ©)\n","        im = axes[i, 1].imshow(pred_prob, cmap='plasma', vmin=0, vmax=1)\n","        fig.colorbar(im, ax=axes[i, 1], fraction=0.046, pad=0.04)\n","        axes[i, 1].set_title(f\"ProbabilitÃ©\")\n","\n","        # PrÃ©diction (binaire)\n","        axes[i, 2].imshow(pred_binary, cmap='binary', vmin=0, vmax=1)\n","        axes[i, 2].set_title(f\"TrouÃ©es prÃ©dites\")\n","\n","    # Supprimer les axes\n","    for ax in axes.flat:\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    return fig\n","\n","\n","def create_metrics_tables(metrics, title=\"MÃ©triques d'Ã©valuation\"):\n","    \"\"\"\n","    CrÃ©e des tableaux formatÃ©s pour afficher les mÃ©triques dans la console.\n","\n","    Args:\n","        metrics: Dictionnaire de mÃ©triques\n","        title: Titre du tableau\n","\n","    Returns:\n","        ChaÃ®ne formatÃ©e contenant le tableau de mÃ©triques\n","    \"\"\"\n","    # Fonction pour formater les nombres\n","    def format_num(num):\n","        return f\"{num:.4f}\"\n","\n","    # Largeur des colonnes\n","    col_width = 16\n","\n","    # CrÃ©er la ligne d'en-tÃªte avec des sÃ©parateurs\n","    header = f\"{'=' * (col_width * 5)}\\n\"\n","    header += f\"{title.center(col_width * 5)}\\n\"\n","    header += f\"{'-' * (col_width * 5)}\\n\"\n","\n","    # Tableau des mÃ©triques globales\n","    global_table = f\"MÃ‰TRIQUES GLOBALES:\\n\"\n","    global_table += f\"{'MÃ©trique'.ljust(col_width)}{'Valeur'.ljust(col_width)}\\n\"\n","    global_table += f\"{'-' * (col_width * 2)}\\n\"\n","\n","    for metric_name, display_name in [\n","        ('accuracy', 'Accuracy'),\n","        ('balanced_accuracy', 'Balanced Acc.'),\n","        ('precision', 'Precision'),\n","        ('recall', 'Recall'),\n","        ('f1_score', 'F1-Score'),\n","        ('iou', 'IoU'),\n","        ('kappa', 'Kappa')\n","    ]:\n","        if metric_name in metrics:\n","            global_table += f\"{display_name.ljust(col_width)}{format_num(metrics[metric_name]).ljust(col_width)}\\n\"\n","\n","    # Tableau des moyennes par image\n","    mean_table = f\"\\nMOYENNES PAR IMAGE:\\n\"\n","    mean_table += f\"{'MÃ©trique'.ljust(col_width)}{'Valeur'.ljust(col_width)}\\n\"\n","    mean_table += f\"{'-' * (col_width * 2)}\\n\"\n","\n","    for metric_name, display_name in [\n","        ('precision_mean', 'Precision Mean'),\n","        ('recall_mean', 'Recall Mean'),\n","        ('f1_mean', 'F1-Score Mean'),\n","        ('iou_mean', 'IoU Mean')\n","    ]:\n","        if metric_name in metrics:\n","            mean_table += f\"{display_name.ljust(col_width)}{format_num(metrics[metric_name]).ljust(col_width)}\\n\"\n","\n","    # Tableau des mÃ©triques par seuil\n","    threshold_table = \"\"\n","    if 'threshold_metrics' in metrics and metrics['threshold_metrics']:\n","        threshold_table += f\"\\nMÃ‰TRIQUES PAR SEUIL:\\n\"\n","\n","        # En-tÃªte avec les seuils\n","        threshold_header = \"MÃ©trique\".ljust(col_width)\n","        thresholds = sorted(metrics['threshold_metrics'].keys())\n","\n","        for threshold in thresholds:\n","            threshold_header += f\"{f'Seuil {threshold}m'.ljust(col_width)}\"\n","\n","        threshold_table += threshold_header + \"\\n\"\n","        threshold_table += f\"{'-' * (col_width * (len(thresholds) + 1))}\\n\"\n","\n","        # Remplir le tableau par mÃ©trique\n","        for metric_name, display_name in [\n","            ('accuracy', 'Accuracy'),\n","            ('precision', 'Precision'),\n","            ('recall', 'Recall'),\n","            ('f1_score', 'F1-Score'),\n","            ('iou', 'IoU'),\n","            ('precision_mean', 'Prec. Mean'),\n","            ('recall_mean', 'Recall Mean'),\n","            ('f1_mean', 'F1 Mean'),\n","            ('iou_mean', 'IoU Mean')\n","        ]:\n","            row = display_name.ljust(col_width)\n","\n","            for threshold in thresholds:\n","                t_metrics = metrics['threshold_metrics'][threshold]\n","                if metric_name in t_metrics:\n","                    row += format_num(t_metrics[metric_name]).ljust(col_width)\n","                else:\n","                    row += \"N/A\".ljust(col_width)\n","\n","            threshold_table += row + \"\\n\"\n","\n","    # Assembler tous les tableaux\n","    return header + global_table + mean_table + threshold_table + f\"{'=' * (col_width * 5)}\"\n","\n","\n","def visualize_predictions_tensorboard(model, dataloader, device, writer, epoch, threshold_value, num_samples=4):\n","    \"\"\"\n","    Visualise quelques prÃ©dictions du modÃ¨le dans TensorBoard avec des mÃ©triques enrichies.\n","\n","    Args:\n","        model: ModÃ¨le U-Net\n","        dataloader: DataLoader contenant les donnÃ©es de validation ou test\n","        device: PÃ©riphÃ©rique (GPU/CPU)\n","        writer: SummaryWriter de TensorBoard\n","        epoch: Ã‰poque actuelle\n","        threshold_value: Valeur de seuil utilisÃ©e (pour le titre)\n","        num_samples: Nombre d'exemples Ã  visualiser\n","    \"\"\"\n","    model.eval()\n","\n","    # Create an object SegmentationMetrics to calculate metrics on these samples\n","    metrics_calculator = SegmentationMetrics(device)\n","\n","    # Get some samples\n","    samples = []\n","    with torch.no_grad():\n","        for dsm, threshold, target in dataloader:\n","            if len(samples) >= num_samples:\n","                break\n","\n","            # Take only the necessary number of samples\n","            for i in range(min(dsm.size(0), num_samples - len(samples))):\n","                # Check if this threshold corresponds to the one requested\n","                threshold_val = threshold[i].item() * max(dataloader.dataset.thresholds)\n","                closest_threshold = min(dataloader.dataset.thresholds, key=lambda x: abs(x - threshold_val))\n","\n","                if abs(closest_threshold - threshold_value) > 1.0:  # Tolerance of 1m\n","                    continue\n","\n","                # Prediction for this sample\n","                dsm_i = dsm[i:i+1].to(device)\n","                threshold_i = threshold[i:i+1].to(device)\n","                target_i = target[i:i+1].to(device)\n","\n","                # Prediction\n","                output_i = model(dsm_i, threshold_i)\n","\n","                # Calculate metrics for this sample\n","                metrics_calculator.reset()\n","                metrics_calculator.update(output_i, target_i)\n","                sample_metrics = metrics_calculator.compute()\n","\n","                # Add to samples\n","                samples.append({\n","                    'dsm': dsm_i.cpu(),\n","                    'target': target_i.cpu(),\n","                    'output': output_i.cpu(),\n","                    'threshold_val': threshold_val,\n","                    'metrics': sample_metrics\n","                })\n","\n","    # Create a grid of images for TensorBoard\n","    if samples:\n","        # Prepare the DSM images (convert to RGB for visualization)\n","        dsm_grid = []\n","        for sample in samples:\n","            # Normalize for visualization\n","            dsm_img = sample['dsm'][0]  # Shape [C, H, W]\n","            dsm_img = dsm_img.repeat(3, 1, 1)  # Convert to RGB by repeating the channel\n","            dsm_grid.append(dsm_img)\n","\n","        # Prepare the mask images\n","        target_grid = []\n","        for sample in samples:\n","            # Convert to RGB (white = gap, black = non-gap)\n","            target_img = sample['target'][0].repeat(3, 1, 1)\n","            target_grid.append(target_img)\n","\n","        # Prepare the prediction images (probabilities)\n","        prob_grid = []\n","        for sample in samples:\n","            # Apply a colormap for better visualization of probabilities\n","            prob = torch.sigmoid(sample['output'][0])\n","            # Create an RGB image with red channel for high probabilities\n","            prob_rgb = torch.zeros(3, prob.size(1), prob.size(2))\n","            prob_rgb[0] = prob  # Red channel\n","            prob_grid.append(prob_rgb)\n","\n","        # Prepare the binary prediction images\n","        pred_grid = []\n","        for sample in samples:\n","            # Convert to RGB\n","            pred_binary = (torch.sigmoid(sample['output'][0]) > 0.5).float()\n","            pred_img = pred_binary.repeat(3, 1, 1)\n","            pred_grid.append(pred_img)\n","\n","        # Merge into a single tensor for each type\n","        dsm_tensor = torch.stack(dsm_grid)\n","        target_tensor = torch.stack(target_grid)\n","        prob_tensor = torch.stack(prob_grid)\n","        pred_tensor = torch.stack(pred_grid)\n","\n","        # Write to TensorBoard\n","        writer.add_images(f'DSM/threshold_{threshold_value}m', dsm_tensor, epoch)\n","        writer.add_images(f'Masks_True/threshold_{threshold_value}m', target_tensor, epoch)\n","        writer.add_images(f'Probabilities/threshold_{threshold_value}m', prob_tensor, epoch)\n","        writer.add_images(f'Predictions/threshold_{threshold_value}m', pred_tensor, epoch)\n","\n","        # Create overlay images (prediction on DSM) - CORRECTION ICI\n","        overlay_grid = []\n","        for i, sample in enumerate(samples):\n","            # DSM in background with prediction in color\n","            dsm_rgb = dsm_grid[i]\n","            pred_binary = (torch.sigmoid(sample['output'][0]) > 0.5).float()\n","\n","            # Create a colored version of the mask prediction (red)\n","            red_mask = torch.zeros_like(dsm_rgb)  # CrÃ©er un masque de mÃªme taille que dsm_rgb\n","            red_mask[0] = pred_binary  # Appliquer le masque binaire au canal rouge\n","\n","            # Add the red mask to the DSM image with some transparency\n","            overlay = dsm_rgb.clone()\n","            overlay[0] = torch.clamp(overlay[0] + red_mask[0] * 0.7, 0, 1)  # Red channel\n","            overlay_grid.append(overlay)\n","\n","        # Merge and add to TensorBoard\n","        overlay_tensor = torch.stack(overlay_grid)\n","        writer.add_images(f'Overlay/threshold_{threshold_value}m', overlay_tensor, epoch)\n","\n","        # Also add the metrics for each sample\n","        for i, sample in enumerate(samples):\n","            metrics = sample['metrics']\n","            for metric_name, value in metrics.items():\n","                if isinstance(value, (int, float)):\n","                    writer.add_scalar(f'Samples/threshold_{threshold_value}m/sample_{i+1}/{metric_name}', value, epoch)\n","\n","        # Calculate and add the average metrics for this batch of samples\n","        avg_metrics = {}\n","        for metric_name in ['iou', 'accuracy', 'precision', 'recall', 'f1_score']:\n","            values = [s['metrics'].get(metric_name, 0) for s in samples]\n","            if values:\n","                avg_metrics[metric_name] = sum(values) / len(values)\n","                writer.add_scalar(f'Averages/threshold_{threshold_value}m/{metric_name}', avg_metrics[metric_name], epoch)\n","\n","        # Create an image with text to display the average metrics\n","        import matplotlib.pyplot as plt\n","        from io import BytesIO\n","        from PIL import Image\n","\n","        fig, ax = plt.subplots(figsize=(10, 6))\n","        ax.axis('off')\n","        ax.text(0.5, 0.5, '\\n'.join([f\"{m.capitalize()}: {v:.4f}\" for m, v in avg_metrics.items()]),\n","                ha='center', va='center', fontsize=14)\n","\n","        # Convert the figure to an image for TensorBoard\n","        buf = BytesIO()\n","        fig.savefig(buf, format='png')\n","        buf.seek(0)\n","        img = Image.open(buf)\n","        img_tensor = torch.tensor(np.array(img)).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n","\n","        writer.add_images(f'Metrics/threshold_{threshold_value}m', img_tensor, epoch)\n","        plt.close(fig)\n","\n","def print_config_summary(config, title=\"Configuration utilisÃ©e\"):\n","    \"\"\"Affiche un rÃ©sumÃ© des paramÃ¨tres importants de la configuration\"\"\"\n","    print(\"\\n\" + \"=\" * 60)\n","    print(f\"{title.center(60)}\")\n","    print(\"=\" * 60)\n","    print(f\"â€¢ Nombre d'Ã©poques: {config.EPOCHS}\")\n","    print(f\"â€¢ Taille des batchs: {config.BATCH_SIZE}\")\n","    print(f\"â€¢ Type de modÃ¨le: {getattr(config, 'MODEL_TYPE', 'basic')}\")\n","    print(f\"â€¢ Seuils utilisÃ©s: {config.THRESHOLDS}\")\n","    if hasattr(config, 'MAX_TRAIN_TILES'):\n","        print(f\"â€¢ Tuiles d'entraÃ®nement: {config.MAX_TRAIN_TILES}\")\n","        print(f\"â€¢ Tuiles de validation: {config.MAX_VAL_TILES}\")\n","        print(f\"â€¢ Tuiles de test: {config.MAX_TEST_TILES}\")\n","    print(\"=\" * 60 + \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"rZcvG8FRyPGX"},"source":["## SECTION 9: CONFIGURATION ENVIRONNEMENT, WORKFLOW ET TEST\n"]},{"cell_type":"code","execution_count":95,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1741117035385,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"aICgkg0tz1CA"},"outputs":[],"source":["# =====================================================================\n","# SECTION 9: CONFIGURATION ET TEST RAPIDE\n","# =====================================================================\n","\n","def setup_quick_test_environment(config, use_quick_mode=False, max_train_tiles=20, max_val_tiles=5, max_test_tiles=5, epochs=None):\n","    \"\"\"\n","    Configure un environnement de test rapide pour vÃ©rifier le workflow complet\n","    avec un nombre rÃ©duit de donnÃ©es et d'Ã©poques.\n","\n","    Args:\n","        config: La configuration existante\n","        use_quick_mode: Si True, active le mode de test rapide\n","        max_train_tiles: Nombre maximum de tuiles d'entraÃ®nement Ã  utiliser\n","        max_val_tiles: Nombre maximum de tuiles de validation Ã  utiliser\n","        max_test_tiles: Nombre maximum de tuiles de test Ã  utiliser\n","        epochs: Nombre d'Ã©poques personnalisÃ© (remplace config.EPOCHS si fourni)\n","\n","    Returns:\n","        Une configuration modifiÃ©e pour le test rapide\n","    \"\"\"\n","    if not use_quick_mode:\n","        return config\n","\n","    print(\"\\n=== MODE TEST RAPIDE ACTIVÃ‰ ===\")\n","\n","    # CrÃ©er une copie de la configuration pour ne pas modifier l'originale\n","    quick_config = Config()\n","\n","    # HÃ©riter des chemins et configurations de base\n","    for attr, value in vars(config).items():\n","        setattr(quick_config, attr, value)\n","\n","    # RÃ©duire les paramÃ¨tres pour un test rapide\n","    quick_config.THRESHOLDS = config.THRESHOLDS[:2] if len(config.THRESHOLDS) > 2 else config.THRESHOLDS\n","\n","    # Stocker les limites de tuiles\n","    quick_config.MAX_TRAIN_TILES = max_train_tiles\n","    quick_config.MAX_VAL_TILES = max_val_tiles\n","    quick_config.MAX_TEST_TILES = max_test_tiles\n","\n","    # Appliquer le nombre d'Ã©poques personnalisÃ© si fourni\n","    if epochs is not None:\n","        quick_config.EPOCHS = epochs\n","\n","    # CrÃ©ation d'un dossier de test isolÃ© avec timestamp pour Ã©viter les conflits\n","    import datetime\n","    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    test_dir = os.path.join(config.LOGS_DIR, f'quick_test_{timestamp}')\n","    os.makedirs(test_dir, exist_ok=True)\n","\n","    # Remplacer les chemins de sauvegarde par des chemins dans le dossier de test\n","    quick_config.QUICK_TEST_DIR = test_dir\n","    quick_config.TEST_CHECKPOINTS_DIR = os.path.join(test_dir, 'checkpoints')\n","    quick_config.TEST_LOGS_DIR = os.path.join(test_dir, 'logs')\n","    quick_config.TEST_RESULTS_DIR = os.path.join(test_dir, 'results')\n","    quick_config.TEST_VISUALIZATIONS_DIR = os.path.join(test_dir, 'visualizations')\n","\n","    # CrÃ©er les sous-rÃ©pertoires\n","    for dir_path in [\n","        quick_config.TEST_CHECKPOINTS_DIR,\n","        quick_config.TEST_LOGS_DIR,\n","        quick_config.TEST_RESULTS_DIR,\n","        quick_config.TEST_VISUALIZATIONS_DIR\n","    ]:\n","        os.makedirs(dir_path, exist_ok=True)\n","\n","    # Sauvegarder la configuration rapide\n","    quick_config.save_config(os.path.join(quick_config.TEST_LOGS_DIR, 'quick_config.json'))\n","\n","    # Afficher le rÃ©sumÃ© de configuration\n","    print_config_summary(quick_config, \"Configuration du mode test rapide\")\n","    print(f\"Environnement de test isolÃ© crÃ©Ã© dans: {test_dir}\")\n","\n","    return quick_config\n","\n","def train_model_for_test(model, train_loader, val_loader, criterion, optimizer, scheduler, config, device):\n","    \"\"\"\n","    Version simplifiÃ©e de train_model pour les tests rapides.\n","    RÃ©utilise la fonction principale en modifiant certains paramÃ¨tres pour le test.\n","\n","    Args:\n","        Les mÃªmes que train_model\n","\n","    Returns:\n","        Tuple (model, tracker)\n","    \"\"\"\n","    # CrÃ©er une copie de la configuration pour le mode test\n","    test_config = copy.deepcopy(config)\n","\n","    # Modifier les paramÃ¨tres spÃ©cifiques pour le test\n","    if hasattr(config, 'QUICK_TEST_DIR'):\n","        # Remplacer les chemins par ceux du test\n","        test_config.CHECKPOINTS_DIR = getattr(config, 'TEST_CHECKPOINTS_DIR', config.CHECKPOINTS_DIR)\n","        test_config.LOGS_DIR = getattr(config, 'TEST_LOGS_DIR', config.LOGS_DIR)\n","        test_config.RESULTS_DIR = getattr(config, 'TEST_RESULTS_DIR', config.RESULTS_DIR)\n","        test_config.VISUALIZATIONS_DIR = getattr(config, 'TEST_VISUALIZATIONS_DIR', config.VISUALIZATIONS_DIR)\n","\n","    # Forcer l'ignore des checkpoints existants pour le test\n","    class TestTrainingCallbacks:\n","        def before_training_start(self, model, optimizer, tracker):\n","            \"\"\"Callback appelÃ© avant le dÃ©but de l'entraÃ®nement\"\"\"\n","            print(\"Mode TEST: Ignorer les checkpoints existants, dÃ©marrage Ã  l'Ã©poque 0\")\n","            return 0  # Forcer l'Ã©poque de dÃ©part Ã  0\n","\n","        def after_epoch(self, epoch, model, optimizer, train_metrics, val_metrics, tracker, improved):\n","            \"\"\"Callback appelÃ© aprÃ¨s chaque Ã©poque\"\"\"\n","            # Limiter le nombre d'Ã©valuations visuelles en mode test\n","            return True  # Continuer l'entraÃ®nement\n","\n","        def should_save_checkpoint(self, epoch, model, optimizer, improved):\n","            \"\"\"Callback pour dÃ©terminer si on doit sauvegarder un checkpoint\"\"\"\n","            # En mode test, on peut limiter les sauvegardes\n","            return improved and epoch > 0  # Sauvegarder seulement si amÃ©liorÃ© et aprÃ¨s Ã©poque 0\n","\n","    # CrÃ©er les callbacks de test\n","    test_callbacks = TestTrainingCallbacks()\n","\n","    # Appeler la fonction principale avec les paramÃ¨tres modifiÃ©s\n","    return train_model(\n","        model,\n","        train_loader,\n","        val_loader,\n","        criterion,\n","        optimizer,\n","        scheduler,\n","        test_config,\n","        device,\n","        callbacks=test_callbacks,\n","        is_test_mode=True  # Indicateur spÃ©cial pour le mode test\n","    )\n","\n","def reduce_dataset_size(dataset_info, max_size=20):\n","    \"\"\"\n","    RÃ©duit la taille d'un dataset pour les tests rapides.\n","\n","    Args:\n","        dataset_info: Liste d'informations sur les tuiles\n","        max_size: Nombre maximum de tuiles Ã  conserver\n","\n","    Returns:\n","        Une version rÃ©duite de la liste d'informations\n","    \"\"\"\n","    if len(dataset_info) <= max_size:\n","        return dataset_info\n","\n","    # RÃ©duire la taille du dataset en sÃ©lectionnant un sous-ensemble\n","    # StratÃ©gie: prendre des exemples distribuÃ©s sur tous les sites\n","    sites = set(info['site'] for info in dataset_info)\n","\n","    # Assurer une distribution Ã©quilibrÃ©e entre les sites\n","    reduced_info = []\n","    samples_per_site = max(1, max_size // len(sites))\n","\n","    for site in sites:\n","        site_samples = [info for info in dataset_info if info['site'] == site]\n","        # Prendre les premiers Ã©chantillons (ou on pourrait les choisir alÃ©atoirement)\n","        site_selection = site_samples[:samples_per_site]\n","        reduced_info.extend(site_selection)\n","\n","    # Si nous n'avons pas assez d'Ã©chantillons, en ajouter d'autres\n","    if len(reduced_info) < max_size:\n","        remaining = [info for info in dataset_info if info not in reduced_info]\n","        reduced_info.extend(remaining[:max_size - len(reduced_info)])\n","\n","    print(f\"Dataset rÃ©duit de {len(dataset_info)} Ã  {len(reduced_info)} Ã©chantillons\")\n","    return reduced_info[:max_size]  # Assurer que nous ne dÃ©passons pas max_sizedef run_workflow_test(use_quick_mode=True, custom_epochs=None, max_train_tiles=20, max_val_tiles=5, max_test_tiles=5):\n","    \"\"\"\n","    ExÃ©cute un test rapide du workflow complet.\n","\n","    Args:\n","        use_quick_mode: Si True, utilise la configuration rapide\n","        custom_epochs: Nombre personnalisÃ© d'Ã©poques pour le test\n","        max_train_tiles: Nombre maximum de tuiles d'entraÃ®nement pour le mode test\n","        max_val_tiles: Nombre maximum de tuiles de validation pour le mode test\n","        max_test_tiles: Nombre maximum de tuiles de test pour le mode test\n","\n","    Returns:\n","        Les rÃ©sultats du test\n","    \"\"\"\n","    print(\"\\n=== DÃ‰MARRAGE DU TEST DE WORKFLOW ===\\n\")\n","\n","    # Initialiser la configuration standard\n","    standard_config = Config()\n","\n","    # Appliquer la configuration rapide si demandÃ©\n","    active_config = setup_quick_test_environment(\n","        standard_config,\n","        use_quick_mode,\n","        max_train_tiles,\n","        max_val_tiles,\n","        max_test_tiles,\n","        epochs=custom_epochs  # Passer les Ã©poques personnalisÃ©es\n","    )\n","\n","    # Si custom_epochs est fourni, l'utiliser pour Ã©craser EPOCHS\n","    if custom_epochs is not None and custom_epochs > 0:\n","        active_config.EPOCHS = custom_epochs\n","        print(f\"Nombre d'Ã©poques personnalisÃ©: {active_config.EPOCHS}\")\n","\n","    # Configurer l'entraÃ®nement avec la configuration active\n","    print(\"\\nConfiguration de l'environnement d'entraÃ®nement...\")\n","\n","    # Au lieu d'appeler setup_model_training(), crÃ©ons directement ce dont nous avons besoin\n","    train_info_path = os.path.join(active_config.TILES_DIR, 'train_tile_info.pkl')\n","    val_info_path = os.path.join(active_config.TILES_DIR, 'val_tile_info.pkl')\n","    test_info_path = os.path.join(active_config.TILES_DIR, 'test_tile_info.pkl')\n","\n","    try:\n","        # Essayer de charger les fichiers d'information\n","        if os.path.exists(train_info_path):\n","            with open(train_info_path, 'rb') as f:\n","                train_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {train_info_path}\")\n","\n","        if os.path.exists(val_info_path):\n","            with open(val_info_path, 'rb') as f:\n","                val_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {val_info_path}\")\n","\n","        if os.path.exists(test_info_path):\n","            with open(test_info_path, 'rb') as f:\n","                test_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {test_info_path}\")\n","\n","    except (FileNotFoundError, pickle.UnpicklingError) as e:\n","        print(f\"ERREUR lors du chargement des fichiers: {str(e)}\")\n","\n","        # CrÃ©er des donnÃ©es factices pour test\n","        print(\"CrÃ©ation de donnÃ©es factices pour test uniquement...\")\n","\n","        def create_dummy_tile_info(num_tiles):\n","            dummy_info = []\n","            tile_size = active_config.TILE_SIZE\n","\n","            for i in range(num_tiles):\n","                # CrÃ©er des faux chemins de fichiers\n","                dsm_path = f\"/tmp/dummy_dsm_{i}.npy\"\n","                mask_paths = {t: f\"/tmp/dummy_mask_{i}_{t}m.npy\" for t in active_config.THRESHOLDS}\n","\n","                # CrÃ©er des donnÃ©es factices pour simuler les tuiles\n","                if not os.path.exists(dsm_path):\n","                    np.save(dsm_path, np.random.rand(tile_size, tile_size))\n","\n","                for t, path in mask_paths.items():\n","                    if not os.path.exists(path):\n","                        np.save(path, (np.random.rand(tile_size, tile_size) > 0.7).astype(np.uint8))\n","\n","                dummy_info.append({\n","                    'site': f'dummy_site_{i//10}',\n","                    'row_idx': i % 10,\n","                    'col_idx': i % 10,\n","                    'window': None,\n","                    'dsm_path': dsm_path,\n","                    'mask_paths': mask_paths\n","                })\n","            return dummy_info\n","\n","        # CrÃ©er des donnÃ©es factices\n","        train_tile_info = create_dummy_tile_info(50)\n","        val_tile_info = create_dummy_tile_info(10)\n","        test_tile_info = create_dummy_tile_info(10)\n","\n","    # RÃ©duire les datasets pour le test rapide\n","    if use_quick_mode:\n","        print(\"\\nRÃ©duction des datasets pour le test rapide...\")\n","        train_tile_info = reduce_dataset_size(train_tile_info, max_size=max_train_tiles)\n","        val_tile_info = reduce_dataset_size(val_tile_info, max_size=max_val_tiles)\n","        test_tile_info = reduce_dataset_size(test_tile_info, max_size=max_test_tiles)\n","\n","    # CrÃ©er les dataloaders\n","    print(\"CrÃ©ation des DataLoaders...\")\n","    train_loader, val_loader, test_loader = create_data_loaders(\n","        active_config, train_tile_info, val_tile_info, test_tile_info\n","    )\n","\n","    # CrÃ©er le modÃ¨le\n","    print(\"Initialisation du modÃ¨le U-Net...\")\n","    model = UNet(\n","        in_channels=1,\n","        dropout_rate=active_config.DROPOUT_RATE,\n","        use_checkpointing=active_config.USE_GRADIENT_CHECKPOINTING\n","    ).to(DEVICE)\n","\n","    # Calculer les poids pour chaque seuil\n","    threshold_stats = {}\n","    for threshold in active_config.THRESHOLDS:\n","        # Ces valeurs sont approximatives basÃ©es sur les donnÃ©es fournies\n","        if threshold == 10:\n","            ratio_trouees = 0.15  # Environ 15% de trouÃ©es\n","        elif threshold == 15:\n","            ratio_trouees = 0.30  # Environ 30% de trouÃ©es\n","        elif threshold == 20:\n","            ratio_trouees = 0.50  # Environ 50% de trouÃ©es\n","        elif threshold == 25:\n","            ratio_trouees = 0.70  # Environ 70% de trouÃ©es\n","        elif threshold == 30:\n","            ratio_trouees = 0.85  # Environ 85% de trouÃ©es\n","        else:\n","            ratio_trouees = 0.50  # Valeur par dÃ©faut\n","\n","        threshold_stats[threshold] = ratio_trouees\n","\n","    # CrÃ©er les poids pour chaque seuil\n","    threshold_weights = create_threshold_weights(active_config, threshold_stats)\n","\n","    # DÃ©finir la fonction de perte combinÃ©e\n","    criterion = CombinedFocalDiceLoss(\n","        alpha=0.5,  # Ã‰quilibre entre BCE et Dice\n","        gamma=2.0,  # ParamÃ¨tre focal standard\n","        threshold_weights=threshold_weights\n","    )\n","\n","    print(\"Fonction de perte combinÃ©e (BCE-Focal + Dice) initialisÃ©e avec pondÃ©ration adaptative aux seuils\")\n","\n","    optimizer = optim.Adam(model.parameters(), lr=active_config.LEARNING_RATE)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","\n","    # CrÃ©er le dictionnaire de setup\n","    setup_dict = {\n","        'model': model,\n","        'train_loader': train_loader,\n","        'val_loader': val_loader,\n","        'test_loader': test_loader,\n","        'criterion': criterion,\n","        'optimizer': optimizer,\n","        'scheduler': scheduler,\n","        'config': active_config,\n","        'device': DEVICE,\n","        'train_tile_info': train_tile_info,\n","        'val_tile_info': val_tile_info,\n","        'test_tile_info': test_tile_info\n","    }\n","\n","    # ExÃ©cuter un entraÃ®nement rapide avec la version spÃ©ciale pour tests\n","    print(\"\\nLancement de l'entraÃ®nement test...\")\n","    model, tracker = train_model_for_test(\n","        setup_dict['model'],\n","        setup_dict['train_loader'],\n","        setup_dict['val_loader'],\n","        setup_dict['criterion'],\n","        setup_dict['optimizer'],\n","        setup_dict['scheduler'],\n","        setup_dict['config'],\n","        setup_dict['device']\n","    )\n","\n","    # Tester rapidement le modÃ¨le en utilisant le dossier de visualisations dÃ©diÃ© aux tests\n","    print(\"\\nEvaluation du modÃ¨le...\")\n","    visualizations_dir = getattr(active_config, 'TEST_VISUALIZATIONS_DIR', active_config.VISUALIZATIONS_DIR)\n","\n","    # Fonction de test modifiÃ©e pour utiliser les chemins de test\n","    def test_model_for_test(model, test_loader, device, config, threshold=0.5):\n","        \"\"\"Version de test_model qui utilise les dossiers de test\"\"\"\n","        model.eval()\n","        total_iou = 0.0\n","        total_samples = 0\n","        visualization_samples = []\n","\n","        try:\n","            with torch.no_grad():\n","                for dsm, threshold_val, target in tqdm(test_loader, desc=\"Test\"):\n","                    if dsm.size(0) == 0:\n","                        continue\n","                    dsm = dsm.to(device)\n","                    threshold_val = threshold_val.to(device)\n","                    target = target.to(device)\n","                    output = model(dsm, threshold_val)\n","                    iou = iou_metric(output, target)\n","                    batch_size = dsm.size(0)\n","                    total_iou += iou.item() * batch_size\n","                    total_samples += batch_size\n","\n","                    if len(visualization_samples) < 5:\n","                        for i in range(min(batch_size, 5 - len(visualization_samples))):\n","                            visualization_samples.append({\n","                                'dsm': dsm[i].cpu().numpy(),\n","                                'target': target[i].cpu().numpy(),\n","                                'output': output[i].cpu().numpy(),\n","                                'threshold': threshold_val[i].item() * max(config.THRESHOLDS)\n","                            })\n","\n","            mean_iou = total_iou / total_samples if total_samples > 0 else 0.0\n","            print(f\"Ã‰valuation sur {total_samples} Ã©chantillons\")\n","            print(f\"IoU moyen: {mean_iou:.4f}\")\n","\n","            if visualization_samples:\n","                try:\n","                    fig, axes = plt.subplots(len(visualization_samples), 3, figsize=(15, 5*len(visualization_samples)))\n","                    if len(visualization_samples) == 1:\n","                        axes = [axes]\n","\n","                    for i, sample in enumerate(visualization_samples):\n","                        axes[i][0].imshow(sample['dsm'][0], cmap='terrain')\n","                        axes[i][0].set_title(f\"DSM\")\n","                        axes[i][1].imshow(sample['target'][0], cmap='binary')\n","                        axes[i][1].set_title(f\"RÃ©el (seuil {sample['threshold']:.1f}m)\")\n","                        pred_binary = (torch.sigmoid(torch.tensor(sample['output'][0])) > threshold).float().cpu().numpy()\n","                        axes[i][2].imshow(pred_binary, cmap='binary')\n","                        sample_iou = iou_metric(\n","                            torch.tensor(sample['output']).unsqueeze(0),\n","                            torch.tensor(sample['target']).unsqueeze(0)\n","                        ).item()\n","                        axes[i][2].set_title(f\"PrÃ©dit (IoU: {sample_iou:.4f})\")\n","\n","                    plt.tight_layout()\n","\n","                    # Utiliser le dossier de visualisations de test\n","                    visualizations_dir = getattr(config, 'TEST_VISUALIZATIONS_DIR', config.VISUALIZATIONS_DIR)\n","                    os.makedirs(visualizations_dir, exist_ok=True)\n","\n","                    plt.savefig(os.path.join(visualizations_dir, 'test_predictions.png'), dpi=300)\n","                    plt.show()\n","                except Exception as e:\n","                    print(f\"Erreur lors de la visualisation: {str(e)}\")\n","\n","            return {'iou': mean_iou}\n","\n","        except Exception as e:\n","            print(f\"Erreur lors de l'Ã©valuation du modÃ¨le: {str(e)}\")\n","            return {'iou': 0.0}\n","\n","    # Ã‰valuer avec la fonction modifiÃ©e\n","    test_metrics = test_model_for_test(\n","        model,\n","        setup_dict['test_loader'],\n","        setup_dict['device'],\n","        setup_dict['config']\n","    )\n","\n","    # Tester une prÃ©diction simple en utilisant les dossiers de test\n","    print(\"\\nTest de prÃ©diction...\")\n","\n","    # Version modifiÃ©e qui utilise les dossiers de test\n","    def visualize_predictions_for_test(model, tile_info, threshold_value, device, config, num_samples=2):\n","        \"\"\"Version qui utilise les dossiers de test pour les visualisations\"\"\"\n","        visualizations_dir = getattr(config, 'TEST_VISUALIZATIONS_DIR', config.VISUALIZATIONS_DIR)\n","        model.eval()\n","\n","        indices = list(range(min(num_samples, len(tile_info))))\n","        normalized_threshold = threshold_value / max(config.THRESHOLDS)\n","        threshold_tensor = torch.tensor([[normalized_threshold]], dtype=torch.float32).to(device)\n","\n","        fig, axes = plt.subplots(len(indices), 3, figsize=(15, 5*len(indices)))\n","        if len(indices) == 1:\n","            axes = [axes]\n","\n","        with torch.no_grad():\n","            for i, idx in enumerate(indices):\n","                info = tile_info[idx]\n","                dsm_tile = np.load(info['dsm_path'])\n","                mask_path = info['mask_paths'][threshold_value]\n","                mask_tile = np.load(mask_path)\n","\n","                mask_valid = (mask_tile != 255)\n","                mask_binary = np.where(mask_valid, (mask_tile > 0).astype(np.float32), 0)\n","\n","                dsm_valid = ~np.isnan(dsm_tile)\n","                dsm_display = dsm_tile.copy()\n","\n","                if np.any(dsm_valid):\n","                    dsm_min = np.nanmin(dsm_tile)\n","                    dsm_max = np.nanmax(dsm_tile)\n","                    dsm_range = dsm_max - dsm_min\n","                    if dsm_range > 0:\n","                        dsm_tile = np.where(dsm_valid, (dsm_tile - dsm_min) / dsm_range, 0)\n","                    else:\n","                        dsm_tile = np.where(dsm_valid, 0, 0)\n","\n","                dsm_tensor = torch.tensor(dsm_tile, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n","                output = model(dsm_tensor, threshold_tensor)\n","                prediction = output.squeeze().cpu().numpy()\n","\n","                pred_tensor = torch.sigmoid(torch.tensor(prediction, dtype=torch.float32)).unsqueeze(0).unsqueeze(0)\n","                mask_tensor = torch.tensor(mask_binary, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n","                iou = iou_metric(pred_tensor, mask_tensor).item()\n","\n","                axes[i][0].imshow(dsm_display, cmap='terrain')\n","                axes[i][0].set_title(f\"DSM - {info['site']}\")\n","                axes[i][1].imshow(mask_binary, cmap='binary')\n","                axes[i][1].set_title(f\"TrouÃ©es rÃ©elles (seuil {threshold_value}m)\")\n","                axes[i][2].imshow(torch.sigmoid(torch.tensor(prediction)).numpy() > 0.5, cmap='binary')\n","                axes[i][2].set_title(f\"TrouÃ©es prÃ©dites (IoU: {iou:.4f})\")\n","\n","        plt.tight_layout()\n","        os.makedirs(visualizations_dir, exist_ok=True)\n","        plt.savefig(os.path.join(visualizations_dir, f'predictions_threshold_{threshold_value}m.png'), dpi=300)\n","        plt.show()\n","\n","    # Visualisation avec la fonction modifiÃ©e\n","    visualize_predictions_for_test(\n","        model,\n","        setup_dict['test_tile_info'][:2],\n","        active_config.THRESHOLDS[0],\n","        setup_dict['device'],\n","        active_config,\n","        num_samples=2\n","    )\n","\n","    # RÃ©sumÃ© des rÃ©sultats\n","    print(\"\\n=== RÃ‰SUMÃ‰ DU TEST DE WORKFLOW ===\")\n","    print(f\"Configuration: {'Rapide' if use_quick_mode else 'Standard'}\")\n","    print(f\"Nombre d'Ã©poques rÃ©alisÃ©es: {len(tracker.train_losses)}\")\n","    print(f\"IoU final sur le jeu de test: {test_metrics['iou']:.4f}\")\n","    print(f\"Meilleur IoU de validation: {tracker.best_val_iou:.4f}\")\n","    print(f\"RÃ©sultats enregistrÃ©s dans: {getattr(active_config, 'TEST_VISUALIZATIONS_DIR', active_config.VISUALIZATIONS_DIR)}\")\n","\n","    return {\n","        'model': model,\n","        'tracker': tracker,\n","        'test_metrics': test_metrics,\n","        'config': active_config\n","    }\n","\n","class TestTrainingCallbacks:\n","    \"\"\"\n","    Classe de callbacks pour le mode test, qui permet de personnaliser\n","    le comportement de l'entraÃ®nement en mode test.\n","    \"\"\"\n","    def before_training_start(self, model, optimizer, tracker):\n","        \"\"\"\n","        Callback appelÃ© avant le dÃ©but de l'entraÃ®nement.\n","        Force le dÃ©marrage Ã  l'Ã©poque 0 en mode test.\n","        \"\"\"\n","        print(\"Mode TEST: Ignorer les checkpoints existants, dÃ©marrage Ã  l'Ã©poque 0\")\n","        return 0  # Forcer l'Ã©poque de dÃ©part Ã  0\n","\n","    def after_epoch(self, epoch, model, optimizer, train_metrics, val_metrics, tracker, improved):\n","        \"\"\"\n","        Callback appelÃ© aprÃ¨s chaque Ã©poque.\n","        Permet de limiter certaines opÃ©rations en mode test.\n","        \"\"\"\n","        # Par exemple, limiter les Ã©valuations visuelles\n","        return True  # Continuer l'entraÃ®nement\n","\n","    def should_save_checkpoint(self, epoch, model, optimizer, improved):\n","        \"\"\"\n","        Callback pour dÃ©terminer si on doit sauvegarder un checkpoint.\n","        En mode test, on peut limiter les sauvegardes.\n","        \"\"\"\n","        # Sauvegarder seulement si amÃ©liorÃ© et aprÃ¨s l'Ã©poque 0\n","        return improved and epoch > 0\n","\n","def estimate_optimal_epochs(config, initial_epochs=30, patience=10):\n","    \"\"\"\n","    Estime le nombre optimal d'Ã©poques basÃ© sur des rÃ¨gles heuristiques\n","    et des bonnes pratiques.\n","\n","    Args:\n","        config: La configuration\n","        initial_epochs: Nombre d'Ã©poques initial Ã  considÃ©rer\n","        patience: Patience pour l'arrÃªt prÃ©coce\n","\n","    Returns:\n","        Estimation du nombre optimal d'Ã©poques et recommandations\n","    \"\"\"\n","    # Facteurs qui influencent le nombre d'Ã©poques optimal\n","    dataset_size = 1000  # Valeur par dÃ©faut si non disponible\n","    try:\n","        # Essayer de charger les informations sur les tuiles\n","        train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","        if os.path.exists(train_info_path):\n","            with open(train_info_path, 'rb') as f:\n","                train_tile_info = pickle.load(f)\n","                dataset_size = len(train_tile_info)\n","    except:\n","        pass  # Utiliser la valeur par dÃ©faut\n","\n","    # Heuristiques pour l'estimation\n","    if dataset_size < 100:\n","        base_epochs = 50  # Plus d'Ã©poques pour petits datasets\n","    elif dataset_size < 500:\n","        base_epochs = 40\n","    elif dataset_size < 1000:\n","        base_epochs = 30\n","    else:\n","        base_epochs = 25  # Moins d'Ã©poques pour grands datasets\n","\n","    # Ajuster en fonction du dropout (plus de dropout = plus d'Ã©poques)\n","    dropout_factor = 1.0 + config.DROPOUT_RATE\n","\n","    # Ajuster en fonction de la taille du batch (plus petit batch = moins d'Ã©poques)\n","    batch_factor = max(0.8, min(1.2, 32 / config.BATCH_SIZE))\n","\n","    # Calcul final\n","    estimated_epochs = int(base_epochs * dropout_factor * batch_factor)\n","\n","    # Recommandation pour une approche progressive\n","    phase1 = min(10, estimated_epochs // 3)\n","    phase2 = min(20, estimated_epochs // 2)\n","\n","    recommendations = {\n","        'initial_estimate': estimated_epochs,\n","        'with_early_stopping': estimated_epochs + patience,\n","        'progressive_approach': {\n","            'phase1': phase1,\n","            'phase2': phase2,\n","            'phase3': estimated_epochs\n","        },\n","        'explanation': f\"\"\"\n","            Recommandation d'une approche progressive en 3 phases :\n","            1. Phase de test rapide ({phase1} Ã©poques) : VÃ©rifier la convergence initiale\n","            2. Phase intermÃ©diaire ({phase2} Ã©poques) : Affiner l'apprentissage\n","            3. Phase complÃ¨te ({estimated_epochs} Ã©poques) : Optimiser les performances\n","\n","            Utiliser un arrÃªt prÃ©coce avec patience={patience} pour Ã©viter le surapprentissage.\n","            Surveiller la validation Ã  chaque phase pour dÃ©cider s'il faut poursuivre.\n","        \"\"\"\n","    }\n","\n","    return recommendations\n","\n","def setup_model_training():\n","    \"\"\"\n","    Configure l'environnement pour l'entraÃ®nement du modÃ¨le U-Net.\n","    \"\"\"\n","    print(\"=== CONFIGURATION POUR L'ENTRAÃŽNEMENT DU MODÃˆLE U-NET ===\\n\")\n","\n","    # Configuration des chemins selon le type de modÃ¨le\n","    config = Config()\n","    config = setup_model_paths(config)\n","\n","    # Sauvegarder la configuration\n","    config.save_config()\n","\n","    # Afficher le type de modÃ¨le utilisÃ©\n","    model_type = config.MODEL_TYPE\n","    print(f\"\\nType de modÃ¨le sÃ©lectionnÃ©: {model_type}\")\n","\n","    # Charger les informations sur les tuiles\n","    print(\"\\n1. CHARGEMENT DES INFORMATIONS SUR LES TUILES\")\n","    print(\"-------------------------------------------\")\n","\n","    train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","    val_info_path = os.path.join(config.TILES_DIR, 'val_tile_info.pkl')\n","    test_info_path = os.path.join(config.TILES_DIR, 'test_tile_info.pkl')\n","\n","    # VÃ©rifier l'existence du rÃ©pertoire et des fichiers individuellement\n","    print(f\"RÃ©pertoire TILES_DIR: {config.TILES_DIR}\")\n","    print(f\"Ce rÃ©pertoire existe: {os.path.exists(config.TILES_DIR)}\")\n","\n","    if os.path.exists(config.TILES_DIR):\n","        print(f\"Contenu du rÃ©pertoire: {os.listdir(config.TILES_DIR)}\")\n","\n","    print(f\"Chemin train_info: {train_info_path} (existe: {os.path.exists(train_info_path)})\")\n","    print(f\"Chemin val_info: {val_info_path} (existe: {os.path.exists(val_info_path)})\")\n","    print(f\"Chemin test_info: {test_info_path} (existe: {os.path.exists(test_info_path)})\")\n","\n","    # Tenter de charger les fichiers existants ou crÃ©er des donnÃ©es factices si nÃ©cessaire\n","    try:\n","        # Essayer de charger les fichiers d'information\n","        if os.path.exists(train_info_path):\n","            with open(train_info_path, 'rb') as f:\n","                train_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {train_info_path}\")\n","\n","        if os.path.exists(val_info_path):\n","            with open(val_info_path, 'rb') as f:\n","                val_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {val_info_path}\")\n","\n","        if os.path.exists(test_info_path):\n","            with open(test_info_path, 'rb') as f:\n","                test_tile_info = pickle.load(f)\n","        else:\n","            raise FileNotFoundError(f\"Fichier manquant: {test_info_path}\")\n","\n","    except (FileNotFoundError, pickle.UnpicklingError) as e:\n","        print(f\"ERREUR lors du chargement des fichiers: {str(e)}\")\n","        print(\"\\nVÃ©rifier que:\")\n","        print(\"1. Vous avez bien exÃ©cutÃ© le script de prÃ©paration des donnÃ©es\")\n","        print(f\"2. Les chemins sont corrects: BASE_DIR={config.BASE_DIR}\")\n","        print(\"3. Les fichiers .pkl ne sont pas corrompus\")\n","\n","        # Demander Ã  l'utilisateur s'il souhaite utiliser des donnÃ©es factices pour le dÃ©veloppement\n","        use_dummy = input(\"\\nVoulez-vous utiliser des donnÃ©es factices pour le dÃ©veloppement? (o/n): \")\n","\n","        if use_dummy.lower() == 'o':\n","            print(\"CrÃ©ation de donnÃ©es factices pour test uniquement...\")\n","\n","            # Fonction pour crÃ©er des donnÃ©es factices\n","            def create_dummy_tile_info(num_tiles):\n","                dummy_info = []\n","                tile_size = config.TILE_SIZE\n","\n","                for i in range(num_tiles):\n","                    # CrÃ©er des faux chemins de fichiers (ils n'existent pas rÃ©ellement)\n","                    dsm_path = f\"/tmp/dummy_dsm_{i}.npy\"\n","                    mask_paths = {t: f\"/tmp/dummy_mask_{i}_{t}m.npy\" for t in config.THRESHOLDS}\n","\n","                    # CrÃ©er des donnÃ©es factices pour simuler les tuiles\n","                    if not os.path.exists(dsm_path):\n","                        np.save(dsm_path, np.random.rand(tile_size, tile_size))\n","\n","                    for t, path in mask_paths.items():\n","                        if not os.path.exists(path):\n","                            np.save(path, (np.random.rand(tile_size, tile_size) > 0.7).astype(np.uint8))\n","\n","                    dummy_info.append({\n","                        'site': f'dummy_site_{i//10}',\n","                        'row_idx': i % 10,\n","                        'col_idx': i % 10,\n","                        'window': None,\n","                        'dsm_path': dsm_path,\n","                        'mask_paths': mask_paths\n","                    })\n","                return dummy_info\n","\n","            # CrÃ©er des donnÃ©es factices\n","            train_tile_info = create_dummy_tile_info(100)  # 100 tuiles d'entrainement\n","            val_tile_info = create_dummy_tile_info(20)     # 20 tuiles de validation\n","            test_tile_info = create_dummy_tile_info(20)    # 20 tuiles de test\n","\n","            print(\"DonnÃ©es factices crÃ©Ã©es avec succÃ¨s!\")\n","        else:\n","            raise FileNotFoundError(\"Les fichiers d'information sur les tuiles n'existent pas. ExÃ©cutez d'abord le script de prÃ©paration des donnÃ©es.\")\n","\n","    print(f\"Nombre de tuiles d'entraÃ®nement: {len(train_tile_info)}\")\n","    print(f\"Nombre de tuiles de validation: {len(val_tile_info)}\")\n","    print(f\"Nombre de tuiles de test: {len(test_tile_info)}\")\n","\n","    # CrÃ©er les DataLoaders\n","    print(\"\\n2. CRÃ‰ATION DES DATALOADERS\")\n","    print(\"---------------------------\")\n","\n","    train_loader, val_loader, test_loader = create_data_loaders(\n","        config, train_tile_info, val_tile_info, test_tile_info\n","    )\n","\n","    print(f\"Nombre de batchs d'entraÃ®nement: {len(train_loader)}\")\n","    print(f\"Nombre de batchs de validation: {len(val_loader)}\")\n","    print(f\"Nombre de batchs de test: {len(test_loader)}\")\n","\n","    print(\"\\n3. CRÃ‰ATION DU MODÃˆLE U-NET\")\n","    print(\"----------------------------\")\n","\n","    # CrÃ©er le modÃ¨le\n","    # Utiliser la factory de modÃ¨les\n","    model = create_unet_model(config).to(DEVICE)\n","\n","    # Afficher le nombre de paramÃ¨tres\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Nombre total de paramÃ¨tres: {total_params:,}\")\n","    print(f\"Nombre de paramÃ¨tres entraÃ®nables: {trainable_params:,}\")\n","\n","    print(\"\\n4. CONFIGURATION DE LA FONCTION DE PERTE ET DE L'OPTIMISATION\")\n","    print(\"-----------------------------------------------------------\")\n","\n","    # Calculer les poids pour chaque seuil\n","    threshold_stats = {}\n","    for threshold in config.THRESHOLDS:\n","        # Ces valeurs sont approximatives basÃ©es sur les donnÃ©es fournies\n","        if threshold == 10:\n","            ratio_trouees = 0.15  # Environ 15% de trouÃ©es\n","        elif threshold == 15:\n","            ratio_trouees = 0.30  # Environ 30% de trouÃ©es\n","        elif threshold == 20:\n","            ratio_trouees = 0.50  # Environ 50% de trouÃ©es\n","        elif threshold == 25:\n","            ratio_trouees = 0.70  # Environ 70% de trouÃ©es\n","        elif threshold == 30:\n","            ratio_trouees = 0.85  # Environ 85% de trouÃ©es\n","        else:\n","            ratio_trouees = 0.50  # Valeur par dÃ©faut\n","\n","        threshold_stats[threshold] = ratio_trouees\n","\n","    # CrÃ©er les poids pour chaque seuil\n","    threshold_weights = create_threshold_weights(config, threshold_stats)\n","\n","    # DÃ©finir la fonction de perte combinÃ©e\n","    criterion = CombinedFocalDiceLoss(\n","        alpha=0.5,  # Ã‰quilibre entre BCE et Dice\n","        gamma=2.0,  # ParamÃ¨tre focal standard\n","        threshold_weights=threshold_weights\n","    )\n","\n","    # DÃ©finir l'optimiseur\n","    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # DÃ©finir le scheduler\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","\n","    # Initialiser l'objet de mÃ©triques\n","    segmentation_metrics = SegmentationMetrics(DEVICE)\n","\n","    print(\"\\nModÃ¨le U-Net configurÃ© avec succÃ¨s!\")\n","    print(\"PrÃªt pour l'entraÃ®nement avec fonction de perte combinÃ©e et mÃ©triques avancÃ©es.\")\n","\n","    # CrÃ©er le dictionnaire setup_dict\n","    setup_dict = {\n","        'model': model,\n","        'train_loader': train_loader,\n","        'val_loader': val_loader,\n","        'test_loader': test_loader,\n","        'criterion': criterion,\n","        'optimizer': optimizer,\n","        'scheduler': scheduler,\n","        'config': config,\n","        'device': DEVICE,\n","        'train_tile_info': train_tile_info,\n","        'val_tile_info': val_tile_info,\n","        'test_tile_info': test_tile_info\n","    }\n","\n","    # PLACER LE CODE ICI, juste avant le return\n","    # Si le modÃ¨le utilise DropPath, ajouter le scheduler\n","    model_type = config.MODEL_TYPE\n","    if model_type in ['droppath', 'all']:\n","        droppath_scheduler = DropPathScheduler(\n","            model,\n","            start_prob=0.0,\n","            final_prob=config.DROP_PATH_RATE,\n","            epochs=config.EPOCHS,\n","            strategy='linear',\n","            layer_wise=True,\n","            deeper_more_drop=True\n","        )\n","        setup_dict['droppath_scheduler'] = droppath_scheduler\n","\n","    return setup_dict\n"]},{"cell_type":"markdown","metadata":{"id":"spkgTRG7vOjX"},"source":["## SECTION 10: EXÃ‰CUTION DANS COLAB"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1741117035393,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"nriZKreh0Czy"},"outputs":[],"source":["# =====================================================================\n","# SECTION 10: EXÃ‰CUTION DANS COLAB\n","# =====================================================================\n","\n","def run_workflow_colab(mode='full', custom_epochs=None, model_type='basic', batch_size=None,\n","                      max_train_tiles=20, max_val_tiles=5, max_test_tiles=5, model_path=None):\n","    \"\"\"\n","    Fonction principale pour exÃ©cuter le workflow dans Colab avec diffÃ©rents modes.\n","\n","    Args:\n","        mode: Mode d'exÃ©cution ('test', 'estimate', 'full', 'evaluate')\n","        custom_epochs: Nombre personnalisÃ© d'Ã©poques (si None, utilise la valeur par dÃ©faut ou estimÃ©e)\n","        model_type: Type de modÃ¨le Ã  utiliser ('basic', 'film', 'cbam', etc.)\n","        batch_size: Taille des batchs pour l'entraÃ®nement\n","        max_train_tiles: Nombre maximum de tuiles d'entraÃ®nement pour le mode test\n","        max_val_tiles: Nombre maximum de tuiles de validation pour le mode test\n","        max_test_tiles: Nombre maximum de tuiles de test pour le mode test\n","        model_path: Chemin vers un modÃ¨le Ã  Ã©valuer (pour le mode 'evaluate')\n","\n","    Returns:\n","        RÃ©sultats de l'exÃ©cution (modÃ¨le, tracker, mÃ©triques)\n","    \"\"\"\n","    import os\n","    import sys\n","    import pickle\n","    import numpy as np\n","    import torch\n","    import torch.optim as optim\n","    from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","    # Configuration initiale\n","    config = Config()\n","\n","    # DÃ©finir le type de modÃ¨le\n","    config.MODEL_TYPE = model_type\n","\n","    # Mettre Ã  jour la taille des batchs si spÃ©cifiÃ©e\n","    if batch_size is not None:\n","        config.BATCH_SIZE = batch_size\n","\n","    # Mettre Ã  jour le nombre d'Ã©poques si spÃ©cifiÃ©\n","    if custom_epochs is not None:\n","        config.EPOCHS = custom_epochs\n","\n","    # Configuration pour le mode test ou les autres modes\n","    if mode == 'test':\n","        config = setup_quick_test_environment(\n","            config,\n","            use_quick_mode=True,\n","            max_train_tiles=max_train_tiles,\n","            max_val_tiles=max_val_tiles,\n","            max_test_tiles=max_test_tiles,\n","            epochs=custom_epochs\n","        )\n","    else:\n","        # Pour les autres modes, active_config est simplement config\n","        config = setup_model_paths(config)\n","        print_config_summary(config, f\"Configuration du workflow ({mode})\")\n","\n","    # Mode Ã©valuation uniquement\n","    if mode == 'evaluate':\n","        # [code existant pour l'Ã©valuation]\n","        # ...\n","        return {\n","            'model': model,\n","            'test_metrics': test_metrics,\n","            'config': config\n","        }\n","\n","    # Estimer le nombre optimal d'Ã©poques si demandÃ©\n","    if mode == 'estimate' or mode == 'full_with_estimate':\n","        # [code existant pour l'estimation]\n","        # ...\n","        if mode == 'estimate':\n","            return {'recommendations': recommendations}\n","\n","    # Continuer avec l'entraÃ®nement (test ou complet)\n","    print(\"\\n=== EXÃ‰CUTION DE L'ENTRAÃŽNEMENT \" + (\"RAPIDE\" if mode == 'test' else \"COMPLET\") + \" ===\")\n","\n","    # Configuration commune pour les modes test et full\n","    # Chargement des donnÃ©es et crÃ©ation des DataLoaders\n","    print(\"Configuration de l'environnement d'entraÃ®nement...\")\n","    train_tile_info, val_tile_info, test_tile_info = load_or_create_dummy_data(config)\n","\n","    # RÃ©duire les datasets si en mode test\n","    if mode == 'test':\n","        train_tile_info = reduce_dataset_size(train_tile_info, max_size=max_train_tiles)\n","        val_tile_info = reduce_dataset_size(val_tile_info, max_size=max_val_tiles)\n","        test_tile_info = reduce_dataset_size(test_tile_info, max_size=max_test_tiles)\n","\n","    # CrÃ©er les DataLoaders\n","    print(\"CrÃ©ation des DataLoaders...\")\n","    train_loader, val_loader, test_loader = create_data_loaders(\n","        config, train_tile_info, val_tile_info, test_tile_info\n","    )\n","\n","    # CrÃ©er le modÃ¨le\n","    print(\"Initialisation du modÃ¨le U-Net...\")\n","    model = create_unet_model(config).to(DEVICE)\n","\n","    # DÃ©finir la fonction de perte, l'optimiseur et le scheduler\n","    print(\"\\n4. CONFIGURATION DE LA FONCTION DE PERTE ET DE L'OPTIMISATION\")\n","    print(\"-----------------------------------------------------------\")\n","\n","    # Charger les ratios de trouÃ©es depuis le fichier s'il existe\n","    gap_ratios_path = os.path.join(config.PROCESSED_DIR, 'gap_ratios.json')\n","    threshold_stats = {}\n","\n","    if os.path.exists(gap_ratios_path):\n","        try:\n","            with open(gap_ratios_path, 'r') as f:\n","                gap_ratios_data = json.load(f)\n","                threshold_stats = {int(k): float(v) for k, v in gap_ratios_data.items()}\n","            print(f\"Ratios de trouÃ©es chargÃ©s: {threshold_stats}\")\n","        except Exception as e:\n","            print(f\"âš ï¸ Erreur lors du chargement des ratios de trouÃ©es: {str(e)}\")\n","            # Utiliser les valeurs approximatives comme fallback\n","            threshold_stats = {}\n","            for threshold in config.THRESHOLDS:\n","                # Ces valeurs sont approximatives basÃ©es sur les donnÃ©es fournies\n","                if threshold == 10:\n","                    ratio_trouees = 0.15  # Environ 15% de trouÃ©es\n","                elif threshold == 15:\n","                    ratio_trouees = 0.30  # Environ 30% de trouÃ©es\n","                elif threshold == 20:\n","                    ratio_trouees = 0.50  # Environ 50% de trouÃ©es\n","                elif threshold == 25:\n","                    ratio_trouees = 0.70  # Environ 70% de trouÃ©es\n","                elif threshold == 30:\n","                    ratio_trouees = 0.85  # Environ 85% de trouÃ©es\n","                else:\n","                    ratio_trouees = 0.50  # Valeur par dÃ©faut\n","                threshold_stats[threshold] = ratio_trouees\n","    else:\n","        # Utiliser les valeurs approximatives si le fichier n'existe pas\n","        threshold_stats = {}\n","        for threshold in config.THRESHOLDS:\n","            # Ces valeurs sont approximatives basÃ©es sur les donnÃ©es fournies\n","            if threshold == 10:\n","                ratio_trouees = 0.15  # Environ 15% de trouÃ©es\n","            elif threshold == 15:\n","                ratio_trouees = 0.30  # Environ 30% de trouÃ©es\n","            elif threshold == 20:\n","                ratio_trouees = 0.50  # Environ 50% de trouÃ©es\n","            elif threshold == 25:\n","                ratio_trouees = 0.70  # Environ 70% de trouÃ©es\n","            elif threshold == 30:\n","                ratio_trouees = 0.85  # Environ 85% de trouÃ©es\n","            else:\n","                ratio_trouees = 0.50  # Valeur par dÃ©faut\n","            threshold_stats[threshold] = ratio_trouees\n","        print(\"Utilisation de ratios de trouÃ©es par dÃ©faut.\")\n","\n","    # CrÃ©er les poids pour chaque seuil\n","    threshold_weights = create_threshold_weights(config, threshold_stats)\n","\n","    # DÃ©finir la fonction de perte combinÃ©e\n","    criterion = CombinedFocalDiceLoss(\n","        alpha=0.5,  # Ã‰quilibre entre BCE et Dice\n","        gamma=2.0,  # ParamÃ¨tre focal standard\n","        threshold_weights=threshold_weights\n","    )\n","\n","    # DÃ©finir l'optimiseur\n","    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # DÃ©finir le scheduler\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","\n","    # Initialiser l'objet de mÃ©triques\n","    segmentation_metrics = SegmentationMetrics(DEVICE)\n","\n","    # Si le modÃ¨le utilise DropPath, ajouter le scheduler de DropPath\n","    droppath_scheduler = None\n","    if config.MODEL_TYPE in ['droppath', 'all']:\n","        droppath_scheduler = DropPathScheduler(\n","            model,\n","            start_prob=0.0,\n","            final_prob=config.DROP_PATH_RATE,\n","            epochs=config.EPOCHS,\n","            strategy='linear',\n","            layer_wise=True,\n","            deeper_more_drop=True\n","        )\n","\n","    print(\"Fonction de perte combinÃ©e (BCE-Focal + Dice) initialisÃ©e avec pondÃ©ration adaptative aux seuils\")\n","    print(f\"Optimiseur: Adam avec lr={config.LEARNING_RATE}\")\n","    print(f\"Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n","    if droppath_scheduler:\n","        print(f\"DropPath scheduler configurÃ©: taux final={config.DROP_PATH_RATE}\")\n","\n","    # Configurer pour l'entraÃ®nement (adaptations spÃ©cifiques au mode test)\n","    if mode == 'test':\n","        # En mode test, utiliser une version simplifiÃ©e de train_model\n","        # qui ignore les checkpoints existants et limite certaines fonctionnalitÃ©s\n","        is_test_mode = True\n","        callbacks = TestTrainingCallbacks()  # DÃ©finir cette classe si elle n'existe pas dÃ©jÃ \n","    else:\n","        is_test_mode = False\n","        callbacks = None\n","\n","    # EntraÃ®ner le modÃ¨le (avec les paramÃ¨tres adaptÃ©s au mode)\n","    print(f\"Nombre d'Ã©poques pour l'entraÃ®nement: {config.EPOCHS}\")\n","    model, tracker = train_model(\n","        model, train_loader, val_loader, criterion, optimizer, scheduler,\n","        config, DEVICE, callbacks=callbacks, is_test_mode=is_test_mode,\n","        droppath_scheduler=droppath_scheduler if 'droppath_scheduler' in locals() else None\n","    )\n","\n","    # Test et Ã©valuation (commun aux deux modes)\n","    test_metrics = test_model(\n","        model, test_loader, DEVICE, config,\n","        visualize=True\n","    )\n","\n","    # Afficher les mÃ©triques de test\n","    print(\"\\nMÃ©triques finales sur l'ensemble de test:\")\n","    for metric, value in test_metrics['metrics'].items():\n","        if isinstance(value, (int, float)):\n","            print(f\"{metric}: {value:.4f}\")\n","\n","    return {\n","        'model': model,\n","        'tracker': tracker,\n","        'test_metrics': test_metrics,\n","        'config': config\n","    }\n","\n","def run_workflow_colab(mode='full', custom_epochs=None, model_type='basic', batch_size=None,\n","                      max_train_tiles=20, max_val_tiles=5, max_test_tiles=5, model_path=None):\n","    \"\"\"\n","    Fonction principale pour exÃ©cuter le workflow dans Colab avec diffÃ©rents modes.\n","\n","    Args:\n","        mode: Mode d'exÃ©cution ('test', 'estimate', 'full', 'evaluate')\n","        custom_epochs: Nombre personnalisÃ© d'Ã©poques (si None, utilise la valeur par dÃ©faut ou estimÃ©e)\n","        model_type: Type de modÃ¨le Ã  utiliser ('basic', 'film', 'cbam', etc.)\n","        batch_size: Taille des batchs pour l'entraÃ®nement\n","        max_train_tiles: Nombre maximum de tuiles d'entraÃ®nement pour le mode test\n","        max_val_tiles: Nombre maximum de tuiles de validation pour le mode test\n","        max_test_tiles: Nombre maximum de tuiles de test pour le mode test\n","        model_path: Chemin vers un modÃ¨le Ã  Ã©valuer (pour le mode 'evaluate')\n","\n","    Returns:\n","        RÃ©sultats de l'exÃ©cution (modÃ¨le, tracker, mÃ©triques)\n","    \"\"\"\n","    import os\n","    import sys\n","    import pickle\n","    import numpy as np\n","    import torch\n","    import torch.optim as optim\n","    from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","    # Configuration initiale\n","    config = Config()\n","\n","    # DÃ©finir le type de modÃ¨le\n","    config.MODEL_TYPE = model_type\n","\n","    # Mettre Ã  jour la taille des batchs si spÃ©cifiÃ©e\n","    if batch_size is not None:\n","        config.BATCH_SIZE = batch_size\n","\n","    # Mettre Ã  jour le nombre d'Ã©poques si spÃ©cifiÃ©\n","    if custom_epochs is not None:\n","        config.EPOCHS = custom_epochs\n","\n","    # Si on est en mode test, crÃ©er une configuration active modifiÃ©e\n","    # N'affichons pas de configuration ici, elle sera affichÃ©e dans setup_quick_test_environment\n","    if mode == 'test':\n","        active_config = setup_quick_test_environment(\n","            config,\n","            use_quick_mode=True,\n","            max_train_tiles=max_train_tiles,\n","            max_val_tiles=max_val_tiles,\n","            max_test_tiles=max_test_tiles,\n","            epochs=custom_epochs  # Passer le nombre d'Ã©poques personnalisÃ©\n","        )\n","    else:\n","        # Pour les autres modes, active_config est simplement config\n","        active_config = config\n","        # Affichons la configuration une seule fois ici\n","        print_config_summary(active_config, f\"Configuration du workflow ({mode})\")\n","\n","    # Mode Ã©valuation uniquement\n","    if mode == 'evaluate':\n","        print(\"\\n=== Ã‰VALUATION DU MODÃˆLE ===\")\n","\n","        # Configurer les chemins selon le type de modÃ¨le\n","        config = setup_model_paths(config)\n","\n","        # Charger les informations sur les tuiles\n","        train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","        val_info_path = os.path.join(config.TILES_DIR, 'val_tile_info.pkl')\n","        test_info_path = os.path.join(config.TILES_DIR, 'test_tile_info.pkl')\n","\n","        try:\n","            with open(train_info_path, 'rb') as f:\n","                train_tile_info = pickle.load(f)\n","            with open(val_info_path, 'rb') as f:\n","                val_tile_info = pickle.load(f)\n","            with open(test_info_path, 'rb') as f:\n","                test_tile_info = pickle.load(f)\n","        except Exception as e:\n","            print(f\"Erreur lors du chargement des fichiers de tuiles: {str(e)}\")\n","            return {\"error\": \"Impossible de charger les donnÃ©es\"}\n","\n","        # CrÃ©er les dataloaders\n","        train_loader, val_loader, test_loader = create_data_loaders(\n","            config, train_tile_info, val_tile_info, test_tile_info\n","        )\n","\n","        # CrÃ©er le modÃ¨le selon le type spÃ©cifiÃ©\n","        model = create_unet_model(config).to(DEVICE)\n","\n","        # DÃ©terminer le chemin du modÃ¨le Ã  charger\n","        if model_path:\n","            # Convertir le chemin au format du systÃ¨me actuel\n","            # Gestion des chemins Windows\n","            if '\\\\' in model_path and not os.path.exists(model_path):\n","                # Convertir le chemin Windows en chemin compatible avec le systÃ¨me actuel\n","                model_path = os.path.normpath(model_path)\n","\n","            checkpoint_path = model_path\n","            print(f\"Utilisation du modÃ¨le spÃ©cifiÃ©: {checkpoint_path}\")\n","        else:\n","            # Utiliser le meilleur modÃ¨le sauvegardÃ© par dÃ©faut\n","            checkpoint_path = os.path.join(config.CHECKPOINTS_DIR, 'best_model.pth')\n","            print(f\"Utilisation du dernier modÃ¨le enregistrÃ©: {checkpoint_path}\")\n","\n","        # Charger le modÃ¨le\n","        try:\n","            if os.path.exists(checkpoint_path):\n","                checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n","                if 'model_state_dict' in checkpoint:\n","                    model.load_state_dict(checkpoint['model_state_dict'])\n","                    print(\"ModÃ¨le chargÃ© avec succÃ¨s.\")\n","\n","                    # Afficher les mÃ©triques du modÃ¨le si disponibles\n","                    if 'val_metrics' in checkpoint:\n","                        print(\"\\nMÃ©triques du modÃ¨le lors de la sauvegarde:\")\n","                        metrics = checkpoint['val_metrics']\n","                        for metric_name in ['iou', 'accuracy', 'precision', 'recall', 'f1_score']:\n","                            if metric_name in metrics:\n","                                print(f\"  {metric_name}: {metrics[metric_name]:.4f}\")\n","                else:\n","                    print(\"Format de checkpoint non reconnu, utilisation d'un modÃ¨le non initialisÃ©.\")\n","            else:\n","                print(f\"Aucun modÃ¨le trouvÃ© Ã  {checkpoint_path}\")\n","                print(\"Ã‰valuation avec un modÃ¨le non entraÃ®nÃ©.\")\n","        except Exception as e:\n","            print(f\"Erreur lors du chargement du modÃ¨le: {str(e)}\")\n","            print(\"Ã‰valuation avec un modÃ¨le non entraÃ®nÃ©.\")\n","\n","        # Ã‰valuer le modÃ¨le avec toutes les visualisations\n","        print(\"\\nÃ‰valuation du modÃ¨le...\")\n","        test_metrics = test_model(\n","            model,\n","            test_loader,\n","            DEVICE,\n","            config,\n","            visualize=True  # Activer toutes les visualisations\n","        )\n","\n","        # GÃ©nÃ©rer des visualisations supplÃ©mentaires pour chaque seuil\n","        print(\"\\nGÃ©nÃ©ration des visualisations par seuil...\")\n","        for threshold in config.THRESHOLDS:\n","            visualize_predictions_with_actual(\n","                model,\n","                test_tile_info[:5],  # Utiliser seulement 5 exemples\n","                threshold,\n","                DEVICE,\n","                config,\n","                num_samples=3\n","            )\n","\n","            # Visualisation comparative des seuils\n","            if threshold == config.THRESHOLDS[0]:  # Seulement pour le premier seuil\n","                visualize_threshold_comparison(\n","                    model,\n","                    test_loader,\n","                    DEVICE,\n","                    config,\n","                    sample_idx=0\n","                )\n","\n","        return {\n","            'model': model,\n","            'test_metrics': test_metrics,\n","            'config': config\n","        }\n","\n","    # Estimer le nombre optimal d'Ã©poques si demandÃ©\n","    if mode == 'estimate' or mode == 'full_with_estimate':\n","        print(\"\\n=== ESTIMATION DU NOMBRE OPTIMAL D'Ã‰POQUES ===\")\n","        recommendations = estimate_optimal_epochs(config)\n","        print(f\"Estimation initiale: {recommendations['initial_estimate']} Ã©poques\")\n","        print(f\"Avec arrÃªt prÃ©coce: jusqu'Ã  {recommendations['with_early_stopping']} Ã©poques\")\n","        print(\"\\nApproche progressive recommandÃ©e:\")\n","        print(f\"Phase 1: {recommendations['progressive_approach']['phase1']} Ã©poques\")\n","        print(f\"Phase 2: {recommendations['progressive_approach']['phase2']} Ã©poques\")\n","        print(f\"Phase 3: {recommendations['progressive_approach']['phase3']} Ã©poques\")\n","        print(\"\\nExplication:\")\n","        print(recommendations['explanation'])\n","\n","        # Mettre Ã  jour le nombre d'Ã©poques\n","        if custom_epochs is not None:\n","            config.EPOCHS = custom_epochs\n","            print(f\"\\nNombre d'Ã©poques dÃ©fini manuellement: {config.EPOCHS}\")\n","        else:\n","            config.EPOCHS = recommendations['initial_estimate']\n","            print(f\"\\nNombre d'Ã©poques mis Ã  jour selon l'estimation: {config.EPOCHS}\")\n","\n","        # Si seulement estimation, terminer ici\n","        if mode == 'estimate':\n","            return {'recommendations': recommendations}\n","\n","    # ExÃ©cuter en mode test rapide si demandÃ©\n","    if mode == 'test':\n","        print(f\"Nombre d'Ã©poques pour le test: {custom_epochs}\")\n","        results = run_workflow_test(\n","            use_quick_mode=True,\n","            custom_epochs=custom_epochs,\n","            max_train_tiles=max_train_tiles,\n","            max_val_tiles=max_val_tiles,\n","            max_test_tiles=max_test_tiles\n","        )\n","        return results\n","\n","    # Continuer avec l'entraÃ®nement complet\n","    if mode in ['full', 'full_with_estimate']:\n","        config.USE_GRADIENT_CHECKPOINTING = True\n","        print(\"Gradient checkpointing activÃ© pour Ã©conomiser la mÃ©moire GPU\")\n","\n","        print(\"\\n=== EXÃ‰CUTION DE L'ENTRAÃŽNEMENT COMPLET ===\")\n","\n","        print(\"Configuration de l'environnement d'entraÃ®nement...\")\n","\n","        train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","        val_info_path = os.path.join(config.TILES_DIR, 'val_tile_info.pkl')\n","        test_info_path = os.path.join(config.TILES_DIR, 'test_tile_info.pkl')\n","\n","        try:\n","            # Essayer de charger les fichiers d'information\n","            if os.path.exists(train_info_path):\n","                with open(train_info_path, 'rb') as f:\n","                    train_tile_info = pickle.load(f)\n","            else:\n","                raise FileNotFoundError(f\"Fichier manquant: {train_info_path}\")\n","\n","            if os.path.exists(val_info_path):\n","                with open(val_info_path, 'rb') as f:\n","                    val_tile_info = pickle.load(f)\n","            else:\n","                raise FileNotFoundError(f\"Fichier manquant: {val_info_path}\")\n","\n","            if os.path.exists(test_info_path):\n","                with open(test_info_path, 'rb') as f:\n","                    test_tile_info = pickle.load(f)\n","            else:\n","                raise FileNotFoundError(f\"Fichier manquant: {test_info_path}\")\n","\n","        except (FileNotFoundError, pickle.UnpicklingError) as e:\n","            print(f\"ERREUR lors du chargement des fichiers: {str(e)}\")\n","\n","            # CrÃ©er des donnÃ©es factices pour test\n","            print(\"CrÃ©ation de donnÃ©es factices pour dÃ©veloppement...\")\n","\n","            def create_dummy_tile_info(num_tiles):\n","                dummy_info = []\n","                tile_size = config.TILE_SIZE\n","\n","                for i in range(num_tiles):\n","                    dsm_path = f\"/tmp/dummy_dsm_{i}.npy\"\n","                    mask_paths = {t: f\"/tmp/dummy_mask_{i}_{t}m.npy\" for t in config.THRESHOLDS}\n","\n","                    if not os.path.exists(dsm_path):\n","                        np.save(dsm_path, np.random.rand(tile_size, tile_size))\n","\n","                    for t, path in mask_paths.items():\n","                        if not os.path.exists(path):\n","                            np.save(path, (np.random.rand(tile_size, tile_size) > 0.7).astype(np.uint8))\n","\n","                    dummy_info.append({\n","                        'site': f'dummy_site_{i//10}',\n","                        'row_idx': i % 10,\n","                        'col_idx': i % 10,\n","                        'window': None,\n","                        'dsm_path': dsm_path,\n","                        'mask_paths': mask_paths\n","                    })\n","                return dummy_info\n","\n","            # CrÃ©er des donnÃ©es factices\n","            train_tile_info = create_dummy_tile_info(100)\n","            val_tile_info = create_dummy_tile_info(20)\n","            test_tile_info = create_dummy_tile_info(20)\n","\n","        # CrÃ©er les dataloaders\n","        print(\"CrÃ©ation des DataLoaders...\")\n","        train_loader, val_loader, test_loader = create_data_loaders(\n","            config, train_tile_info, val_tile_info, test_tile_info\n","        )\n","\n","        # CrÃ©er le modÃ¨le\n","        print(\"Initialisation du modÃ¨le U-Net...\")\n","        model = create_unet_model(config).to(DEVICE)\n","\n","        # DÃ©finir la fonction de perte et l'optimiseur\n","        # Calculer les poids pour chaque seuil\n","        threshold_stats = {}\n","        for threshold in config.THRESHOLDS:\n","            # Ces valeurs sont approximatives basÃ©es sur les donnÃ©es fournies\n","            if threshold == 10:\n","                ratio_trouees = 0.15  # Environ 15% de trouÃ©es\n","            elif threshold == 15:\n","                ratio_trouees = 0.30  # Environ 30% de trouÃ©es\n","            elif threshold == 20:\n","                ratio_trouees = 0.50  # Environ 50% de trouÃ©es\n","            elif threshold == 25:\n","                ratio_trouees = 0.70  # Environ 70% de trouÃ©es\n","            elif threshold == 30:\n","                ratio_trouees = 0.85  # Environ 85% de trouÃ©es\n","            else:\n","                ratio_trouees = 0.50  # Valeur par dÃ©faut\n","\n","            threshold_stats[threshold] = ratio_trouees\n","\n","        # CrÃ©er les poids pour chaque seuil\n","        threshold_weights = create_threshold_weights(config, threshold_stats)\n","\n","        # DÃ©finir la fonction de perte combinÃ©e\n","        criterion = CombinedFocalDiceLoss(\n","            alpha=0.5,  # Ã‰quilibre entre BCE et Dice\n","            gamma=2.0,  # ParamÃ¨tre focal standard\n","            threshold_weights=threshold_weights\n","        )\n","\n","        optimizer = optim.Adam(model.parameters(), lr=active_config.LEARNING_RATE)\n","        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n","\n","        print(f\"Nombre d'Ã©poques pour l'entraÃ®nement complet: {config.EPOCHS}\")\n","\n","        # CrÃ©er le dictionnaire de setup\n","        setup_dict = {\n","            'model': model,\n","            'train_loader': train_loader,\n","            'val_loader': val_loader,\n","            'test_loader': test_loader,\n","            'criterion': criterion,\n","            'optimizer': optimizer,\n","            'scheduler': scheduler,\n","            'config': config,\n","            'device': DEVICE,\n","            'train_tile_info': train_tile_info,\n","            'val_tile_info': val_tile_info,\n","            'test_tile_info': test_tile_info\n","        }\n","\n","        # Si le modÃ¨le utilise DropPath, ajouter le scheduler\n","        if model_type in ['droppath', 'all']:\n","            droppath_scheduler = DropPathScheduler(\n","                model,\n","                start_prob=0.0,\n","                final_prob=config.DROP_PATH_RATE,\n","                epochs=config.EPOCHS,\n","                strategy='linear',\n","                layer_wise=True,\n","                deeper_more_drop=True\n","            )\n","            setup_dict['droppath_scheduler'] = droppath_scheduler\n","        else:\n","            droppath_scheduler = None\n","\n","        # EntraÃ®ner le modÃ¨le\n","        model, tracker = train_model(\n","            setup_dict['model'],\n","            setup_dict['train_loader'],\n","            setup_dict['val_loader'],\n","            setup_dict['criterion'],\n","            setup_dict['optimizer'],\n","            setup_dict['scheduler'],\n","            setup_dict['config'],\n","            setup_dict['device'],\n","            droppath_scheduler=droppath_scheduler\n","        )\n","\n","        # Ã‰valuer le modÃ¨le sur l'ensemble de test\n","        test_metrics = test_model(\n","            model,\n","            setup_dict['test_loader'],\n","            setup_dict['device'],\n","            setup_dict['config'],\n","            visualize=True  # Toujours activer les visualisations\n","        )\n","\n","        # Afficher les mÃ©triques de test\n","        print(\"\\nMÃ©triques finales sur l'ensemble de test:\")\n","        for metric, value in test_metrics['metrics'].items():\n","            if isinstance(value, (int, float)):\n","                print(f\"{metric}: {value:.4f}\")\n","\n","        return {\n","            'model': model,\n","            'tracker': tracker,\n","            'test_metrics': test_metrics,\n","            'config': setup_dict['config']\n","        }\n","\n","    # Mode non pris en charge\n","    return {\"error\": \"Mode non pris en charge\"}\n","\n","def create_colab_interface():\n","    \"\"\"\n","    CrÃ©e une interface interactive dans Google Colab avec des widgets.\n","    Cette fonction est appelÃ©e uniquement lorsque le code est exÃ©cutÃ© dans Colab.\n","    \"\"\"\n","    from ipywidgets import widgets\n","    from IPython.display import display, clear_output\n","    import traceback\n","\n","    # Titre principal\n","    display(widgets.HTML('<h2>Configuration du workflow U-Net pour la dÃ©tection des trouÃ©es forestiÃ¨res</h2>'))\n","\n","    # Widget pour le type de modÃ¨le\n","    model_type_selector = widgets.Dropdown(\n","        options=[\n","            ('U-Net standard', 'basic'),\n","            ('U-Net avec FiLM', 'film'),\n","            ('U-Net avec CBAM', 'cbam'),\n","            ('U-Net avec DropPath', 'droppath'),\n","            ('U-Net avec FiLM+CBAM', 'film_cbam'),\n","            ('U-Net avec FiLM+CBAM+DropPath', 'all')\n","        ],\n","        value='basic',\n","        description='Type de modÃ¨le:',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    # Widget de sÃ©lection du mode\n","    mode_selector = widgets.Dropdown(\n","        options=[\n","            ('Test rapide', 'test'),\n","            ('Estimer le nombre optimal d\\'Ã©poques', 'estimate'),\n","            ('EntraÃ®nement complet', 'full'),\n","            ('Ã‰valuation uniquement', 'evaluate')\n","        ],\n","        value='test',\n","        description='Mode:',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    # Widget pour le chemin du modÃ¨le (pour le mode Ã©valuation)\n","    model_path_input = widgets.Text(\n","        value='',\n","        placeholder='Chemin vers le modÃ¨le Ã  Ã©valuer (optionnel)',\n","        description='Chemin du modÃ¨le:',\n","        disabled=True,\n","        style={'description_width': 'initial'},\n","        layout=widgets.Layout(width='100%', display='none')\n","    )\n","\n","    # Widget pour le nombre d'Ã©poques\n","    epochs_input = widgets.IntSlider(\n","        value=2,\n","        min=1,\n","        max=100,\n","        step=1,\n","        description='Nombre d\\'Ã©poques:',\n","        disabled=False,\n","        continuous_update=False,\n","        orientation='horizontal',\n","        readout=True,\n","        readout_format='d',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    # Widget pour la taille des batchs\n","    batch_size_input = widgets.IntSlider(\n","        value=32,\n","        min=4,\n","        max=128,\n","        step=4,\n","        description='Taille des batchs:',\n","        disabled=False,\n","        continuous_update=False,\n","        orientation='horizontal',\n","        readout=True,\n","        readout_format='d',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    # Widgets pour le nombre de tuiles en mode test\n","    train_tiles_input = widgets.IntSlider(\n","        value=70,\n","        min=10,\n","        max=210,\n","        step=5,\n","        description='Tuiles entraÃ®nement:',\n","        disabled=False,\n","        continuous_update=False,\n","        readout=True,\n","        readout_format='d',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    val_tiles_input = widgets.IntSlider(\n","        value=15,\n","        min=5,\n","        max=100,\n","        step=5,\n","        description='Tuiles validation:',\n","        disabled=False,\n","        continuous_update=False,\n","        readout=True,\n","        readout_format='d',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    test_tiles_input = widgets.IntSlider(\n","        value=15,\n","        min=5,\n","        max=100,\n","        step=1,\n","        description='Tuiles test:',\n","        disabled=False,\n","        continuous_update=False,\n","        readout=True,\n","        readout_format='d',\n","        style={'description_width': 'initial'}\n","    )\n","\n","    # Description des diffÃ©rents modes\n","    modes_description = widgets.HTML(\n","        \"\"\"\n","        <div style=\"background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;\">\n","            <h4>Description des modes:</h4>\n","            <ul>\n","                <li><strong>Test rapide</strong>: ExÃ©cute un workflow simplifiÃ© avec un petit sous-ensemble de donnÃ©es pour vÃ©rifier que tout fonctionne.</li>\n","                <li><strong>Estimer le nombre optimal d'Ã©poques</strong>: Calcule et recommande un nombre d'Ã©poques adaptÃ© Ã  vos donnÃ©es.</li>\n","                <li><strong>EntraÃ®nement complet</strong>: Lance l'entraÃ®nement avec l'ensemble des donnÃ©es et le nombre d'Ã©poques spÃ©cifiÃ©.</li>\n","                <li><strong>Ã‰valuation uniquement</strong>: Ã‰value un modÃ¨le dÃ©jÃ  entraÃ®nÃ© sans le rÃ©entraÃ®ner. Vous pouvez spÃ©cifier un chemin vers un modÃ¨le sauvegardÃ©.</li>\n","            </ul>\n","        </div>\n","        \"\"\"\n","    )\n","\n","    # Information sur la progression\n","    status_info = widgets.HTML(\n","        value=\"<div style='color: #6c757d;'><i>PrÃªt Ã  exÃ©cuter. Configurez les options puis cliquez sur 'ExÃ©cuter'.</i></div>\"\n","    )\n","\n","    # Bouton d'exÃ©cution\n","    run_button = widgets.Button(\n","        description='ExÃ©cuter',\n","        button_style='success',\n","        tooltip='Cliquez pour exÃ©cuter le workflow',\n","        icon='play',\n","        layout=widgets.Layout(width='200px', margin='10px 0px')\n","    )\n","\n","    # Affichage des options\n","    display(widgets.VBox([\n","        mode_selector,\n","        model_type_selector,\n","        epochs_input,\n","        model_path_input,\n","        batch_size_input,\n","        widgets.HBox([train_tiles_input, val_tiles_input, test_tiles_input]),\n","        modes_description,\n","        status_info,\n","        run_button\n","    ]))\n","\n","    # Zone de rÃ©sultat\n","    output_area = widgets.Output()\n","    display(output_area)\n","\n","    # Fonction appelÃ©e lors du clic sur le bouton d'exÃ©cution\n","    def on_run_button_clicked(b):\n","        # DÃ©sactiver le bouton pendant l'exÃ©cution\n","        run_button.disabled = True\n","\n","        # Mettre Ã  jour le statut\n","        status_info.value = \"<div style='color: #007bff; font-weight: bold;'><i>ExÃ©cution en cours...</i></div>\"\n","\n","        # RÃ©cupÃ©rer les paramÃ¨tres\n","        model_type = model_type_selector.value\n","        mode = mode_selector.value\n","        epochs = epochs_input.value\n","        batch_size = batch_size_input.value\n","        max_train_tiles = train_tiles_input.value\n","        max_val_tiles = val_tiles_input.value\n","        max_test_tiles = test_tiles_input.value\n","        model_path = model_path_input.value if mode == 'evaluate' else None\n","\n","        with output_area:\n","            clear_output()\n","            print(f\"Lancement du workflow en mode: {mode}\")\n","\n","            try:\n","                # ExÃ©cuter le workflow avec les paramÃ¨tres choisis\n","                results = run_workflow_colab(\n","                    mode=mode,\n","                    custom_epochs=epochs,  # Passer directement le nombre d'Ã©poques demandÃ©\n","                    model_type=model_type,\n","                    batch_size=batch_size,\n","                    max_train_tiles=max_train_tiles,\n","                    max_val_tiles=max_val_tiles,\n","                    max_test_tiles=max_test_tiles,\n","                    model_path=model_path\n","                )\n","                print(\"\\nExÃ©cution terminÃ©e avec succÃ¨s!\")\n","            except Exception as e:\n","                print(f\"\\nErreur pendant l'exÃ©cution: {str(e)}\")\n","                print(traceback.format_exc())\n","\n","        # RÃ©activer le bouton\n","        run_button.disabled = False\n","\n","        # Mettre Ã  jour le statut\n","        status_info.value = \"<div style='color: #28a745;'><i>ExÃ©cution terminÃ©e. Vous pouvez lancer une nouvelle exÃ©cution.</i></div>\"\n","\n","    # Associer la fonction au bouton\n","    run_button.on_click(on_run_button_clicked)\n","\n","    # Fonction pour mettre Ã  jour la visibilitÃ© des widgets selon le mode\n","    def on_mode_change(change):\n","        if change['type'] == 'change' and change['name'] == 'value':\n","            mode = change['new']\n","\n","            # Ajuster la visibilitÃ© et les valeurs par dÃ©faut selon le mode\n","            if mode == 'test':\n","                epochs_input.value = 2\n","                epochs_input.disabled = False\n","                model_path_input.disabled = True\n","                model_path_input.layout.display = 'none'\n","                train_tiles_input.layout.display = 'block'\n","                val_tiles_input.layout.display = 'block'\n","                test_tiles_input.layout.display = 'block'\n","            elif mode == 'evaluate':\n","                epochs_input.disabled = True\n","                model_path_input.disabled = False\n","                model_path_input.layout.display = 'block'\n","                train_tiles_input.layout.display = 'none'\n","                val_tiles_input.layout.display = 'none'\n","                test_tiles_input.layout.display = 'none'\n","            else:\n","                epochs_input.value = 30\n","                epochs_input.disabled = False\n","                model_path_input.disabled = True\n","                model_path_input.layout.display = 'none'\n","                train_tiles_input.layout.display = 'none'\n","                val_tiles_input.layout.display = 'none'\n","                test_tiles_input.layout.display = 'none'\n","\n","            # Mettre Ã  jour l'indication du statut\n","            status_info.value = f\"<div style='color: #6c757d;'><i>Mode '{mode}' sÃ©lectionnÃ©. Configurez les options puis cliquez sur 'ExÃ©cuter'.</i></div>\"\n","\n","    # Observer les changements de mode\n","    mode_selector.observe(on_mode_change, names='value')\n","\n","    # Initialiser l'Ã©tat des widgets selon le mode initial\n","    on_mode_change({'type': 'change', 'name': 'value', 'new': mode_selector.value})"]},{"cell_type":"markdown","metadata":{"id":"t4LS4QHRyNy3"},"source":["# PARTIE : EXÃ‰CUTION"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f2ddbe4fd6e0473681dd9f93246e1928","7ad7d39b273d448aa5c848e3c4bad254","72727e41e96d41af8c619e3b968a727a","52621bd22dd04259a16f95e58ee51452","2f82a11d85b94815aefda69bfcdd3f95","d77d7dc659524f28a995a89bec769c52","35b09c6a060d4e1288c4b2f9c1e27558","7091c5a0515440b6abb92d2514fb218c","ed7f6ed79f684bcb9428fb15f1099694","34497f0a2bd041d2acc1addcf6c95fd3","6d26c5bce16044108cccecf8be8629c0","955fc8d015894b7a89a73d33af2e8bfa","08df4667bf2044d7ad7f4f3bb79eefd8","67b3e030c594460493750318f4dc08c1","607be93d83a342e79dacd6ca54e1aade","f25837c5a19a4fc3972b90be404593c7","648c3088b0314c1abf4e6cfef9a6c81e","59a815cecd0149d9a915967dadf617d5","a547dc9778a04be880b987bd01318002","50302ff62994468ca21737d0410e971a","851c81f17ed344ae9e55ef3710cd3c97","1599a6d30932403ba0ca1334fa329878","099876bcbdb94b7c9696c20c9e1233a9","317cfe01770d419ea01f7350ae254350","a1bbff3517a045bdb4f0149137bfd2e2","091aa5eda3194255aab69002b4b85002","38f8a35798094e7cb716ad5c6c3df956","64a69154704941bfb2dd23c59e411bd3","af30ff4110614255aff4a59b03eb1219","3b67c303b91f4f8c8e48259fec1b6bd0","87196788699445c79b45c14a05d3d96c","a122aa703b7547098411c07a004bca90","b24e9c51f6984211ba33377d1ac9cb3a","463c6379698c49d99b16777606e6a374","78726b69722f46e1ac2efafe13a998d6","79f0f9ee0b8f467c81e30acef47ac964","b909edfa187944d78eadeae21bc59d77","4e60040d77864a6f895933a831c1f76a","4ca8c890a8774182b590219d8f15ee56","c3bff4bbd8cf4d5694ede9bb17ce568d","ff4d47e30d3646e2bb964bf8b5e21653","82200b5472ce4549adfd61b1af9d42a9"]},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1741117624874,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"},"user_tz":-60},"id":"notlaTPK0KLc","outputId":"c37b0e55-8934-4074-c4f2-a9b8893d81da"},"outputs":[{"output_type":"display_data","data":{"text/plain":["HTML(value='<h2>Configuration du workflow U-Net pour la dÃ©tection des trouÃ©es forestiÃ¨res</h2>')"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ddbe4fd6e0473681dd9f93246e1928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Dropdown(description='Mode:', options=(('Test rapide', 'test'), (\"Estimer le nombre optimal d'Ã©â€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52621bd22dd04259a16f95e58ee51452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4d47e30d3646e2bb964bf8b5e21653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# =====================================================================\n","# SECTION 11: FONCTION PRINCIPALE (MAIN)\n","# =====================================================================\n","\n","if __name__ == \"__main__\":\n","    # Cette cellule peut Ãªtre exÃ©cutÃ©e directement dans Colab\n","    if 'google.colab' in str(get_ipython()):\n","        # Utiliser l'interface Colab\n","        create_colab_interface()\n","    else:\n","        # ExÃ©cution traditionnelle (hors Colab)\n","        import argparse\n","\n","        # Configurer les arguments de ligne de commande\n","        parser = argparse.ArgumentParser(description=\"EntraÃ®nement d'un modÃ¨le U-Net pour la dÃ©tection des trouÃ©es forestiÃ¨res\")\n","        parser.add_argument('--mode', type=str, choices=['test', 'estimate', 'full', 'evaluate'], default='full',\n","                          help=\"Mode d'exÃ©cution (test=test rapide, estimate=estimer Ã©poques, full=entraÃ®nement complet, evaluate=Ã©valuation)\")\n","        parser.add_argument('--epochs', type=int, default=30, help=\"Nombre d'Ã©poques pour l'entraÃ®nement\")\n","        parser.add_argument('--batch-size', type=int, default=None, help=\"Taille des batchs (si non spÃ©cifiÃ©, utilise la valeur de config)\")\n","        parser.add_argument('--model-type', type=str, default='basic',\n","                          choices=['basic', 'film', 'cbam', 'droppath', 'film_cbam', 'all'],\n","                          help=\"Type de modÃ¨le U-Net Ã  utiliser\")\n","        parser.add_argument('--train-tiles', type=int, default=20, help=\"Nombre de tuiles d'entraÃ®nement en mode test\")\n","        parser.add_argument('--val-tiles', type=int, default=5, help=\"Nombre de tuiles de validation en mode test\")\n","        parser.add_argument('--test-tiles', type=int, default=5, help=\"Nombre de tuiles de test en mode test\")\n","        parser.add_argument('--model-path', type=str, default=None, help=\"Chemin vers un modÃ¨le Ã  Ã©valuer (mode evaluate)\")\n","        parser.add_argument('--save-dir', type=str, default=None, help=\"RÃ©pertoire personnalisÃ© pour sauvegarder les rÃ©sultats\")\n","        args = parser.parse_args()\n","\n","        # Choix interactif si pas d'arguments spÃ©cifiÃ©s\n","        if len(sys.argv) == 1:\n","            print(\"=\" * 80)\n","            print(\"DÃ‰TECTION DES TROUÃ‰ES FORESTIÃˆRES - MODÃˆLE U-NET\")\n","            print(\"=\" * 80)\n","            print(\"\\nAucun argument fourni. Utilisation du mode interactif.\")\n","\n","            mode_options = {\n","                '1': ('test', \"Test rapide\"),\n","                '2': ('estimate', \"Estimer le nombre optimal d'Ã©poques\"),\n","                '3': ('full', \"EntraÃ®nement complet\"),\n","                '4': ('evaluate', \"Ã‰valuation uniquement\")  # Correction ici: full_with_estimate -> evaluate\n","            }\n","\n","            model_options = {\n","                '1': ('basic', \"U-Net standard\"),\n","                '2': ('film', \"U-Net avec FiLM\"),\n","                '3': ('cbam', \"U-Net avec CBAM\"),\n","                '4': ('droppath', \"U-Net avec DropPath\"),\n","                '5': ('film_cbam', \"U-Net avec FiLM+CBAM\"),\n","                '6': ('all', \"U-Net avec FiLM+CBAM+DropPath\")\n","            }\n","\n","            # Demander le mode\n","            print(\"\\nChoisissez un mode:\")\n","            for key, (_, desc) in mode_options.items():\n","                print(f\"{key}. {desc}\")\n","\n","            mode_choice = input(\"Votre choix (1-4): \")\n","            while mode_choice not in mode_options:\n","                mode_choice = input(\"Choix invalide. RÃ©essayez (1-4): \")\n","\n","            mode = mode_options[mode_choice][0]\n","\n","            # Demander le type de modÃ¨le\n","            print(\"\\nChoisissez un type de modÃ¨le:\")\n","            for key, (_, desc) in model_options.items():\n","                print(f\"{key}. {desc}\")\n","\n","            model_choice = input(\"Votre choix (1-6): \")\n","            while model_choice not in model_options:\n","                model_choice = input(\"Choix invalide. RÃ©essayez (1-6): \")\n","\n","            model_type = model_options[model_choice][0]\n","\n","            # Demander le nombre d'Ã©poques\n","            epochs = input(\"\\nNombre d'Ã©poques (dÃ©faut: 2 pour test, 30 pour entraÃ®nement): \")\n","            epochs = int(epochs) if epochs.isdigit() else (2 if mode == 'test' else 30)\n","\n","            # Demander la taille des batchs\n","            batch_size = input(\"\\nTaille des batchs (dÃ©faut: 32): \")\n","            batch_size = int(batch_size) if batch_size.isdigit() else 32\n","\n","            # ParamÃ¨tres supplÃ©mentaires pour le mode test\n","            if mode == 'test':\n","                train_tiles = input(\"\\nNombre de tuiles d'entraÃ®nement en mode test (dÃ©faut: 20): \")\n","                train_tiles = int(train_tiles) if train_tiles.isdigit() else 20\n","\n","                val_tiles = input(\"Nombre de tuiles de validation en mode test (dÃ©faut: 5): \")\n","                val_tiles = int(val_tiles) if val_tiles.isdigit() else 5\n","\n","                test_tiles = input(\"Nombre de tuiles de test en mode test (dÃ©faut: 5): \")\n","                test_tiles = int(test_tiles) if test_tiles.isdigit() else 5\n","            else:\n","                train_tiles, val_tiles, test_tiles = 20, 5, 5\n","\n","            # Demander le chemin du modÃ¨le pour l'Ã©valuation\n","            model_path = None\n","            if mode == 'evaluate':\n","                model_path = input(\"\\nChemin vers le modÃ¨le Ã  Ã©valuer (laisser vide pour utiliser le meilleur modÃ¨le): \")\n","                model_path = model_path if model_path else None\n","\n","            # ExÃ©cuter avec les paramÃ¨tres choisis\n","            print(\"\\nExÃ©cution avec les paramÃ¨tres suivants:\")\n","            print(f\"Mode: {mode_options[mode_choice][1]}\")\n","            print(f\"Type de modÃ¨le: {model_options[model_choice][1]}\")\n","            print(f\"Nombre d'Ã©poques: {epochs}\")\n","            print(f\"Taille des batchs: {batch_size}\")\n","            if mode == 'test':\n","                print(f\"Nombre de tuiles d'entraÃ®nement: {train_tiles}\")\n","                print(f\"Nombre de tuiles de validation: {val_tiles}\")\n","                print(f\"Nombre de tuiles de test: {test_tiles}\")\n","            if mode == 'evaluate' and model_path:\n","                print(f\"Chemin du modÃ¨le: {model_path}\")\n","\n","            try:\n","                results = run_workflow_colab(\n","                    mode=mode,\n","                    custom_epochs=epochs,\n","                    model_type=model_type,\n","                    batch_size=batch_size,\n","                    max_train_tiles=train_tiles,\n","                    max_val_tiles=val_tiles,\n","                    max_test_tiles=test_tiles,\n","                    model_path=model_path\n","                )\n","                print(\"\\nExÃ©cution terminÃ©e avec succÃ¨s!\")\n","            except Exception as e:\n","                print(f\"\\nErreur pendant l'exÃ©cution: {str(e)}\")\n","                print(traceback.format_exc())\n","                sys.exit(1)\n","\n","            sys.exit(0)\n","\n","        try:\n","            print(\"=\" * 80)\n","            print(\"DÃ‰TECTION DES TROUÃ‰ES FORESTIÃˆRES - MODÃˆLE U-NET\")\n","            print(\"=\" * 80)\n","\n","            # ExÃ©cuter avec les paramÃ¨tres des arguments de ligne de commande\n","            results = run_workflow_colab(\n","                mode=args.mode,\n","                custom_epochs=args.epochs,\n","                model_type=args.model_type,\n","                batch_size=args.batch_size,\n","                max_train_tiles=args.train_tiles,\n","                max_val_tiles=args.val_tiles,\n","                max_test_tiles=args.test_tiles,\n","                model_path=args.model_path\n","            )\n","            print(\"\\nExÃ©cution terminÃ©e avec succÃ¨s!\")\n","\n","        except Exception as e:\n","            print(\"\\n\" + \"!\" * 80)\n","            print(f\"ERREUR: {str(e)}\")\n","            print(traceback.format_exc())\n","            print(\"!\" * 80)\n","            sys.exit(1)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMmI8HeLULh5Y1VSHHFPaJ9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f2ddbe4fd6e0473681dd9f93246e1928":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ad7d39b273d448aa5c848e3c4bad254","placeholder":"â€‹","style":"IPY_MODEL_72727e41e96d41af8c619e3b968a727a","value":"<h2>Configuration du workflow U-Net pour la dÃ©tection des trouÃ©es forestiÃ¨res</h2>"}},"7ad7d39b273d448aa5c848e3c4bad254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72727e41e96d41af8c619e3b968a727a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52621bd22dd04259a16f95e58ee51452":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_2f82a11d85b94815aefda69bfcdd3f95","IPY_MODEL_d77d7dc659524f28a995a89bec769c52","IPY_MODEL_35b09c6a060d4e1288c4b2f9c1e27558","IPY_MODEL_7091c5a0515440b6abb92d2514fb218c","IPY_MODEL_ed7f6ed79f684bcb9428fb15f1099694","IPY_MODEL_34497f0a2bd041d2acc1addcf6c95fd3","IPY_MODEL_6d26c5bce16044108cccecf8be8629c0","IPY_MODEL_955fc8d015894b7a89a73d33af2e8bfa","IPY_MODEL_08df4667bf2044d7ad7f4f3bb79eefd8"],"layout":"IPY_MODEL_67b3e030c594460493750318f4dc08c1"}},"2f82a11d85b94815aefda69bfcdd3f95":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["Test rapide","Estimer le nombre optimal d'Ã©poques","EntraÃ®nement complet","Ã‰valuation uniquement"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Mode:","description_tooltip":null,"disabled":false,"index":2,"layout":"IPY_MODEL_607be93d83a342e79dacd6ca54e1aade","style":"IPY_MODEL_f25837c5a19a4fc3972b90be404593c7"}},"d77d7dc659524f28a995a89bec769c52":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["U-Net standard","U-Net avec FiLM","U-Net avec CBAM","U-Net avec DropPath","U-Net avec FiLM+CBAM","U-Net avec FiLM+CBAM+DropPath"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Type de modÃ¨le:","description_tooltip":null,"disabled":false,"index":5,"layout":"IPY_MODEL_648c3088b0314c1abf4e6cfef9a6c81e","style":"IPY_MODEL_59a815cecd0149d9a915967dadf617d5"}},"35b09c6a060d4e1288c4b2f9c1e27558":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":false,"description":"Nombre d'Ã©poques:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_a547dc9778a04be880b987bd01318002","max":100,"min":1,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_50302ff62994468ca21737d0410e971a","value":5}},"7091c5a0515440b6abb92d2514fb218c":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Chemin du modÃ¨le:","description_tooltip":null,"disabled":true,"layout":"IPY_MODEL_851c81f17ed344ae9e55ef3710cd3c97","placeholder":"Chemin vers le modÃ¨le Ã  Ã©valuer (optionnel)","style":"IPY_MODEL_1599a6d30932403ba0ca1334fa329878","value":""}},"ed7f6ed79f684bcb9428fb15f1099694":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":false,"description":"Taille des batchs:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_099876bcbdb94b7c9696c20c9e1233a9","max":128,"min":4,"orientation":"horizontal","readout":true,"readout_format":"d","step":4,"style":"IPY_MODEL_317cfe01770d419ea01f7350ae254350","value":32}},"34497f0a2bd041d2acc1addcf6c95fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1bbff3517a045bdb4f0149137bfd2e2","IPY_MODEL_091aa5eda3194255aab69002b4b85002","IPY_MODEL_38f8a35798094e7cb716ad5c6c3df956"],"layout":"IPY_MODEL_64a69154704941bfb2dd23c59e411bd3"}},"6d26c5bce16044108cccecf8be8629c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af30ff4110614255aff4a59b03eb1219","placeholder":"â€‹","style":"IPY_MODEL_3b67c303b91f4f8c8e48259fec1b6bd0","value":"\n        <div style=\"background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;\">\n            <h4>Description des modes:</h4>\n            <ul>\n                <li><strong>Test rapide</strong>: ExÃ©cute un workflow simplifiÃ© avec un petit sous-ensemble de donnÃ©es pour vÃ©rifier que tout fonctionne.</li>\n                <li><strong>Estimer le nombre optimal d'Ã©poques</strong>: Calcule et recommande un nombre d'Ã©poques adaptÃ© Ã  vos donnÃ©es.</li>\n                <li><strong>EntraÃ®nement complet</strong>: Lance l'entraÃ®nement avec l'ensemble des donnÃ©es et le nombre d'Ã©poques spÃ©cifiÃ©.</li>\n                <li><strong>Ã‰valuation uniquement</strong>: Ã‰value un modÃ¨le dÃ©jÃ  entraÃ®nÃ© sans le rÃ©entraÃ®ner. Vous pouvez spÃ©cifier un chemin vers un modÃ¨le sauvegardÃ©.</li>\n            </ul>\n        </div>\n        "}},"955fc8d015894b7a89a73d33af2e8bfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87196788699445c79b45c14a05d3d96c","placeholder":"â€‹","style":"IPY_MODEL_a122aa703b7547098411c07a004bca90","value":"<div style='color: #28a745;'><i>ExÃ©cution terminÃ©e. Vous pouvez lancer une nouvelle exÃ©cution.</i></div>"}},"08df4667bf2044d7ad7f4f3bb79eefd8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"ExÃ©cuter","disabled":false,"icon":"play","layout":"IPY_MODEL_b24e9c51f6984211ba33377d1ac9cb3a","style":"IPY_MODEL_463c6379698c49d99b16777606e6a374","tooltip":"Cliquez pour exÃ©cuter le workflow"}},"67b3e030c594460493750318f4dc08c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"607be93d83a342e79dacd6ca54e1aade":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25837c5a19a4fc3972b90be404593c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial"}},"648c3088b0314c1abf4e6cfef9a6c81e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59a815cecd0149d9a915967dadf617d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial"}},"a547dc9778a04be880b987bd01318002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50302ff62994468ca21737d0410e971a":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial","handle_color":null}},"851c81f17ed344ae9e55ef3710cd3c97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"1599a6d30932403ba0ca1334fa329878":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial"}},"099876bcbdb94b7c9696c20c9e1233a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"317cfe01770d419ea01f7350ae254350":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial","handle_color":null}},"a1bbff3517a045bdb4f0149137bfd2e2":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":false,"description":"Tuiles entraÃ®nement:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_78726b69722f46e1ac2efafe13a998d6","max":210,"min":10,"orientation":"horizontal","readout":true,"readout_format":"d","step":5,"style":"IPY_MODEL_79f0f9ee0b8f467c81e30acef47ac964","value":70}},"091aa5eda3194255aab69002b4b85002":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":false,"description":"Tuiles validation:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_b909edfa187944d78eadeae21bc59d77","max":100,"min":5,"orientation":"horizontal","readout":true,"readout_format":"d","step":5,"style":"IPY_MODEL_4e60040d77864a6f895933a831c1f76a","value":15}},"38f8a35798094e7cb716ad5c6c3df956":{"model_module":"@jupyter-widgets/controls","model_name":"IntSliderModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntSliderModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"IntSliderView","continuous_update":false,"description":"Tuiles test:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_4ca8c890a8774182b590219d8f15ee56","max":100,"min":5,"orientation":"horizontal","readout":true,"readout_format":"d","step":1,"style":"IPY_MODEL_c3bff4bbd8cf4d5694ede9bb17ce568d","value":15}},"64a69154704941bfb2dd23c59e411bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af30ff4110614255aff4a59b03eb1219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b67c303b91f4f8c8e48259fec1b6bd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87196788699445c79b45c14a05d3d96c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a122aa703b7547098411c07a004bca90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b24e9c51f6984211ba33377d1ac9cb3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":"10px 0px","max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"200px"}},"463c6379698c49d99b16777606e6a374":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"78726b69722f46e1ac2efafe13a998d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79f0f9ee0b8f467c81e30acef47ac964":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial","handle_color":null}},"b909edfa187944d78eadeae21bc59d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e60040d77864a6f895933a831c1f76a":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial","handle_color":null}},"4ca8c890a8774182b590219d8f15ee56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3bff4bbd8cf4d5694ede9bb17ce568d":{"model_module":"@jupyter-widgets/controls","model_name":"SliderStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"SliderStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"initial","handle_color":null}},"ff4d47e30d3646e2bb964bf8b5e21653":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_82200b5472ce4549adfd61b1af9d42a9","msg_id":"","outputs":[{"output_type":"stream","name":"stdout","text":["Lancement du workflow en mode: full\n","\n","============================================================\n","              Configuration du workflow (full)              \n","============================================================\n","â€¢ Nombre d'Ã©poques: 5\n","â€¢ Taille des batchs: 32\n","â€¢ Type de modÃ¨le: all\n","â€¢ Seuils utilisÃ©s: [10, 15, 20, 25, 30]\n","============================================================\n","\n","Gradient checkpointing activÃ© pour Ã©conomiser la mÃ©moire GPU\n","\n","=== EXÃ‰CUTION DE L'ENTRAÃŽNEMENT COMPLET ===\n","Configuration de l'environnement d'entraÃ®nement...\n","CrÃ©ation des DataLoaders...\n","Ratios de trouÃ©es chargÃ©s: {10: 0.26858609863690985, 15: 0.35607602650330145, 20: 0.48573710298274864, 25: 0.642932864192114, 30: 0.8042339058966496}\n","Initialisation du modÃ¨le U-Net...\n","Nombre d'Ã©poques pour l'entraÃ®nement complet: 5\n","DropPathScheduler initialisÃ© avec 5 modules DropPath\n","TensorBoard initialisÃ© dans: /content/drive/MyDrive/ForestGaps_DeepLearning_Workflow/models/unet_advanced/logs. Pour visualiser, exÃ©cutez: tensorboard --logdir=/content/drive/MyDrive/ForestGaps_DeepLearning_Workflow/models/unet_advanced/logs\n","Aucun modÃ¨le prÃ©cÃ©dent trouvÃ©. DÃ©marrage d'un nouvel entraÃ®nement.\n"]},{"output_type":"stream","name":"stdout","text":["DÃ©but de l'entraÃ®nement pour 5 Ã©poques\n","Ã‰poque 0: ProbabilitÃ© DropPath globale = 0.0000\n","ProbabilitÃ©s par couche:\n","  enc2.drop_path: 0.0000\n","  enc3.drop_path: 0.0000\n","  bottleneck.drop_path: 0.0000\n","  dec3.drop_path: 0.0000\n","  dec2.drop_path: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   0%|                              | 0/582 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   0%|                              | 0/582 [01:22<?, ?it/s, loss=1.3084, iou=0.4825, accuracy=0.6165, f1_score=0.6509]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   2%|â–Œ                             | 10/582 [01:22<1:18:26,  8.23s/it, loss=1.3084, iou=0.4825, accuracy=0.6165, f1_score=0.6509]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   2%|â–Œ                             | 10/582 [02:06<1:18:26,  8.23s/it, loss=1.2708, iou=0.5314, accuracy=0.6563, f1_score=0.6940]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   3%|â–ˆ                             | 20/582 [02:06<55:54,  5.97s/it, loss=1.2708, iou=0.5314, accuracy=0.6563, f1_score=0.6940]  "]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   3%|â–ˆ                             | 20/582 [02:49<55:54,  5.97s/it, loss=1.2160, iou=0.5564, accuracy=0.6922, f1_score=0.7150]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   5%|â–ˆâ–Œ                            | 30/582 [02:49<47:51,  5.20s/it, loss=1.2160, iou=0.5564, accuracy=0.6922, f1_score=0.7150]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   5%|â–ˆâ–Œ                            | 30/582 [03:28<47:51,  5.20s/it, loss=1.1886, iou=0.5752, accuracy=0.7098, f1_score=0.7303]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   7%|â–ˆâ–ˆ                            | 40/582 [03:28<42:39,  4.72s/it, loss=1.1886, iou=0.5752, accuracy=0.7098, f1_score=0.7303]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   7%|â–ˆâ–ˆ                            | 40/582 [04:18<42:39,  4.72s/it, loss=1.1500, iou=0.5900, accuracy=0.7259, f1_score=0.7421]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   9%|â–ˆâ–ˆâ–Œ                           | 50/582 [04:18<42:35,  4.80s/it, loss=1.1500, iou=0.5900, accuracy=0.7259, f1_score=0.7421]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:   9%|â–ˆâ–ˆâ–Œ                           | 50/582 [05:00<42:35,  4.80s/it, loss=1.1022, iou=0.5974, accuracy=0.7345, f1_score=0.7480]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  10%|â–ˆâ–ˆâ–ˆ                           | 60/582 [05:00<40:10,  4.62s/it, loss=1.1022, iou=0.5974, accuracy=0.7345, f1_score=0.7480]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  10%|â–ˆâ–ˆâ–ˆ                           | 60/582 [05:33<40:10,  4.62s/it, loss=1.0434, iou=0.6041, accuracy=0.7415, f1_score=0.7532]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  12%|â–ˆâ–ˆâ–ˆâ–Œ                          | 70/582 [05:33<35:36,  4.17s/it, loss=1.0434, iou=0.6041, accuracy=0.7415, f1_score=0.7532]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  12%|â–ˆâ–ˆâ–ˆâ–Œ                          | 70/582 [06:07<35:36,  4.17s/it, loss=1.0457, iou=0.6105, accuracy=0.7488, f1_score=0.7582]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  14%|â–ˆâ–ˆâ–ˆâ–ˆ                          | 80/582 [06:07<32:43,  3.91s/it, loss=1.0457, iou=0.6105, accuracy=0.7488, f1_score=0.7582]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  14%|â–ˆâ–ˆâ–ˆâ–ˆ                          | 80/582 [06:38<32:43,  3.91s/it, loss=1.0151, iou=0.6153, accuracy=0.7559, f1_score=0.7618]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 90/582 [06:38<30:12,  3.68s/it, loss=1.0151, iou=0.6153, accuracy=0.7559, f1_score=0.7618]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 90/582 [07:23<30:12,  3.68s/it, loss=0.9718, iou=0.6225, accuracy=0.7631, f1_score=0.7673]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 100/582 [07:23<31:34,  3.93s/it, loss=0.9718, iou=0.6225, accuracy=0.7631, f1_score=0.7673]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 100/582 [07:49<31:34,  3.93s/it, loss=0.9622, iou=0.6301, accuracy=0.7688, f1_score=0.7731]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 110/582 [07:49<27:38,  3.51s/it, loss=0.9622, iou=0.6301, accuracy=0.7688, f1_score=0.7731]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 110/582 [08:18<27:38,  3.51s/it, loss=0.9440, iou=0.6365, accuracy=0.7739, f1_score=0.7779]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 120/582 [08:18<25:40,  3.33s/it, loss=0.9440, iou=0.6365, accuracy=0.7739, f1_score=0.7779]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 120/582 [08:41<25:40,  3.33s/it, loss=0.9426, iou=0.6414, accuracy=0.7779, f1_score=0.7815]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 130/582 [08:41<22:43,  3.02s/it, loss=0.9426, iou=0.6414, accuracy=0.7779, f1_score=0.7815]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 130/582 [09:34<22:43,  3.02s/it, loss=0.9234, iou=0.6466, accuracy=0.7815, f1_score=0.7854]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 140/582 [09:34<27:16,  3.70s/it, loss=0.9234, iou=0.6466, accuracy=0.7815, f1_score=0.7854]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 140/582 [09:56<27:16,  3.70s/it, loss=0.9229, iou=0.6495, accuracy=0.7838, f1_score=0.7875]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 150/582 [09:56<23:27,  3.26s/it, loss=0.9229, iou=0.6495, accuracy=0.7838, f1_score=0.7875]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.29610925912857056\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 150/582 [10:21<23:27,  3.26s/it, loss=0.9039, iou=0.6532, accuracy=0.7873, f1_score=0.7902]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 160/582 [10:21<21:12,  3.02s/it, loss=0.9039, iou=0.6532, accuracy=0.7873, f1_score=0.7902]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5582730770111084\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.5765434503555298\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 160/582 [10:46<21:12,  3.02s/it, loss=0.8912, iou=0.6569, accuracy=0.7912, f1_score=0.7929]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 170/582 [10:46<19:48,  2.88s/it, loss=0.8912, iou=0.6569, accuracy=0.7912, f1_score=0.7929]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.2336481809616089\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3854838609695435\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.4190363883972168\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5403258800506592\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5538378953933716\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3964275121688843\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.23863977193832397\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3805631399154663\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.274215817451477\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 170/582 [11:14<19:48,  2.88s/it, loss=0.8642, iou=0.6600, accuracy=0.7942, f1_score=0.7952]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 180/582 [11:14<19:08,  2.86s/it, loss=0.8642, iou=0.6600, accuracy=0.7942, f1_score=0.7952]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.2072874903678894\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 180/582 [11:31<19:08,  2.86s/it, loss=0.8491, iou=0.6637, accuracy=0.7973, f1_score=0.7978]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 190/582 [11:31<16:24,  2.51s/it, loss=0.8491, iou=0.6637, accuracy=0.7973, f1_score=0.7978]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1910697221755981\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3927946090698242\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5533729791641235\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 190/582 [11:53<16:24,  2.51s/it, loss=0.8385, iou=0.6675, accuracy=0.7995, f1_score=0.8006]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 200/582 [11:53<15:17,  2.40s/it, loss=0.8385, iou=0.6675, accuracy=0.7995, f1_score=0.8006]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1627585887908936\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2124649286270142\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2633271217346191\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5897130966186523\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3300427198410034\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2866101264953613\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5965715646743774\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2025734186172485\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                   | 200/582 [12:20<15:17,  2.40s/it, loss=0.8138, iou=0.6716, accuracy=0.8030, f1_score=0.8035]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 210/582 [12:20<15:26,  2.49s/it, loss=0.8138, iou=0.6716, accuracy=0.8030, f1_score=0.8035]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.304103970527649\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2590998411178589\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3171298503875732\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2109525203704834\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0628465414047241\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.45575645565986633\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0583267211914062\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 210/582 [12:39<15:26,  2.49s/it, loss=0.7920, iou=0.6756, accuracy=0.8065, f1_score=0.8064]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 220/582 [12:39<13:53,  2.30s/it, loss=0.7920, iou=0.6756, accuracy=0.8065, f1_score=0.8064]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4881059229373932\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5342918634414673\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.39918768405914307\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3285413980484009\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0339112281799316\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.181296706199646\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.19793976843357086\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 220/582 [12:57<13:53,  2.30s/it, loss=0.7777, iou=0.6786, accuracy=0.8085, f1_score=0.8085]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 230/582 [12:57<12:41,  2.16s/it, loss=0.7777, iou=0.6786, accuracy=0.8085, f1_score=0.8085]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2261255979537964\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1609599590301514\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.5482937097549438\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1959810256958008\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2127341032028198\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1414977312088013\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1510874032974243\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5126521587371826\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.45951515436172485\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 230/582 [13:13<12:41,  2.16s/it, loss=0.7601, iou=0.6811, accuracy=0.8103, f1_score=0.8103]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 240/582 [13:13<11:19,  1.99s/it, loss=0.7601, iou=0.6811, accuracy=0.8103, f1_score=0.8103]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1308590173721313\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1016650199890137\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.3141404390335083\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1002521514892578\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1254836320877075\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1799321174621582\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.18981114029884338\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.18978241086006165\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1268049478530884\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.16963984072208405\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 240/582 [13:37<11:19,  1.99s/it, loss=0.7409, iou=0.6847, accuracy=0.8130, f1_score=0.8128]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 250/582 [13:37<11:43,  2.12s/it, loss=0.7409, iou=0.6847, accuracy=0.8130, f1_score=0.8128]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.41185733675956726\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1043806076049805\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1911783516407013\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0225162506103516\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.48113009333610535\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1706482321023941\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.46237608790397644\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0015881061553955\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.05972158908844\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4454818665981293\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 250/582 [13:54<11:43,  2.12s/it, loss=0.7229, iou=0.6880, accuracy=0.8154, f1_score=0.8152]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 260/582 [13:54<10:39,  1.99s/it, loss=0.7229, iou=0.6880, accuracy=0.8154, f1_score=0.8152]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.37748751044273376\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8516839742660522\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.40778931975364685\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8667584657669067\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.750675618648529\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4062354266643524\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.2600799798965454\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1604761779308319\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7922981381416321\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.929160475730896\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 260/582 [14:15<10:39,  1.99s/it, loss=0.7061, iou=0.6919, accuracy=0.8178, f1_score=0.8179]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 270/582 [14:15<10:28,  2.01s/it, loss=0.7061, iou=0.6919, accuracy=0.8178, f1_score=0.8179]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0216748714447021\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8979344367980957\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0958642959594727\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.445904940366745\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7939044833183289\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.38904452323913574\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8537088632583618\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9047777652740479\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4620855748653412\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4148673713207245\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 270/582 [14:31<10:28,  2.01s/it, loss=0.6908, iou=0.6950, accuracy=0.8200, f1_score=0.8201]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 280/582 [14:31<09:31,  1.89s/it, loss=0.6908, iou=0.6950, accuracy=0.8200, f1_score=0.8201]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9209029674530029\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9974843859672546\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9633259773254395\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4134070873260498\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4220457077026367\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.17456704378128052\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.983838677406311\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.4136433601379395\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.37969493865966797\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0110048055648804\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 280/582 [14:46<09:31,  1.89s/it, loss=0.6762, iou=0.6976, accuracy=0.8221, f1_score=0.8219]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 290/582 [14:46<08:39,  1.78s/it, loss=0.6762, iou=0.6976, accuracy=0.8221, f1_score=0.8219]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.42328599095344543\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0954337120056152\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.23971252143383026\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3425455093383789\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9829270839691162\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0695321559906006\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9918563365936279\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9587162733078003\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0238895416259766\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.46315285563468933\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 290/582 [15:08<08:39,  1.78s/it, loss=0.6626, iou=0.6997, accuracy=0.8240, f1_score=0.8233]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 300/582 [15:08<09:00,  1.92s/it, loss=0.6626, iou=0.6997, accuracy=0.8240, f1_score=0.8233]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7339684367179871\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8108724355697632\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.38219165802001953\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9333692789077759\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8024767637252808\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0574201345443726\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8729532361030579\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.40210747718811035\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.43001940846443176\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7605425119400024\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 300/582 [15:30<09:00,  1.92s/it, loss=0.6490, iou=0.7029, accuracy=0.8262, f1_score=0.8256]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 310/582 [15:30<09:06,  2.01s/it, loss=0.6490, iou=0.7029, accuracy=0.8262, f1_score=0.8256]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9035152196884155\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7789148092269897\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8772684335708618\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9599021077156067\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.18494896590709686\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8274154663085938\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9511147141456604\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9711616039276123\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.35410043597221375\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3846229910850525\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 310/582 [15:47<09:06,  2.01s/it, loss=0.6362, iou=0.7057, accuracy=0.8282, f1_score=0.8274]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 320/582 [15:47<08:17,  1.90s/it, loss=0.6362, iou=0.7057, accuracy=0.8282, f1_score=0.8274]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9877704977989197\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.905116856098175\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0004692077636719\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9134716391563416\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9378442168235779\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3657743036746979\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3257637917995453\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.983104407787323\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7417358756065369\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.159888356924057\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 320/582 [16:02<08:17,  1.90s/it, loss=0.6240, iou=0.7084, accuracy=0.8304, f1_score=0.8293]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 330/582 [16:02<07:29,  1.78s/it, loss=0.6240, iou=0.7084, accuracy=0.8304, f1_score=0.8293]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.5495645999908447\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8082453608512878\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.366010457277298\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0977362394332886\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.36500242352485657\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8282409310340881\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.47821253538131714\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8526331186294556\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4621526002883911\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9974638223648071\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 330/582 [16:18<07:29,  1.78s/it, loss=0.6133, iou=0.7101, accuracy=0.8319, f1_score=0.8305]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 340/582 [16:18<06:55,  1.72s/it, loss=0.6133, iou=0.7101, accuracy=0.8319, f1_score=0.8305]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8688609600067139\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.001713514328003\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9124634265899658\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.44109439849853516\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0054328441619873\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9078013896942139\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.903674304485321\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.35530874133110046\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8328070640563965\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3729771077632904\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 340/582 [16:37<06:55,  1.72s/it, loss=0.6030, iou=0.7123, accuracy=0.8335, f1_score=0.8320]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 350/582 [16:37<06:52,  1.78s/it, loss=0.6030, iou=0.7123, accuracy=0.8335, f1_score=0.8320]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3994259238243103\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3910891115665436\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3969379961490631\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8328692317008972\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8522524237632751\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.925168514251709\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8636476993560791\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3911626636981964\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4262285828590393\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3561117947101593\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 350/582 [16:50<06:52,  1.78s/it, loss=0.5931, iou=0.7144, accuracy=0.8350, f1_score=0.8334]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 360/582 [16:50<06:02,  1.63s/it, loss=0.5931, iou=0.7144, accuracy=0.8350, f1_score=0.8334]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4110061824321747\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9356032013893127\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9963974952697754\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8714867234230042\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.30151987075805664\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8495460152626038\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1848280280828476\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8696408867835999\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8774413466453552\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7431470155715942\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 360/582 [17:04<06:02,  1.63s/it, loss=0.5836, iou=0.7166, accuracy=0.8365, f1_score=0.8349]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 370/582 [17:04<05:31,  1.57s/it, loss=0.5836, iou=0.7166, accuracy=0.8365, f1_score=0.8349]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.34284064173698425\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7585737109184265\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.45866385102272034\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9519766569137573\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9275834560394287\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.36084166169166565\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4571601450443268\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7776452302932739\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.14025302231311798\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.6659255027770996\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 370/582 [17:17<05:31,  1.57s/it, loss=0.5749, iou=0.7182, accuracy=0.8379, f1_score=0.8360]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 380/582 [17:17<04:59,  1.48s/it, loss=0.5749, iou=0.7182, accuracy=0.8379, f1_score=0.8360]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.150365948677063\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3781968057155609\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3485601842403412\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8092579245567322\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3515211343765259\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3942842483520508\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9937261939048767\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.40713638067245483\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.2159671038389206\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3826572895050049\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 380/582 [17:35<04:59,  1.48s/it, loss=0.5666, iou=0.7198, accuracy=0.8392, f1_score=0.8370]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 390/582 [17:35<05:04,  1.59s/it, loss=0.5666, iou=0.7198, accuracy=0.8392, f1_score=0.8370]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3379454016685486\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1664741188287735\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9637458324432373\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0144273042678833\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3705102801322937\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0173945426940918\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0652074813842773\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9348658919334412\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8922513723373413\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.36229974031448364\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 390/582 [17:52<05:04,  1.59s/it, loss=0.5587, iou=0.7215, accuracy=0.8404, f1_score=0.8382]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 400/582 [17:52<04:52,  1.61s/it, loss=0.5587, iou=0.7215, accuracy=0.8404, f1_score=0.8382]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9168779253959656\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0450044870376587\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9658253788948059\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.34005534648895264\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7762735486030579\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3384181261062622\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.38806334137916565\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8171786665916443\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7530838251113892\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.31205257773399353\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 400/582 [18:01<04:52,  1.61s/it, loss=0.5504, iou=0.7236, accuracy=0.8419, f1_score=0.8396]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 410/582 [18:01<04:03,  1.41s/it, loss=0.5504, iou=0.7236, accuracy=0.8419, f1_score=0.8396]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.34161576628685\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9078212976455688\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.785147488117218\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7868403196334839\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8522963523864746\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6138095855712891\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1688271313905716\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7297650575637817\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7271866202354431\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.36208096146583557\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 410/582 [18:11<04:03,  1.41s/it, loss=0.5425, iou=0.7258, accuracy=0.8434, f1_score=0.8411]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 420/582 [18:11<03:27,  1.28s/it, loss=0.5425, iou=0.7258, accuracy=0.8434, f1_score=0.8411]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.327515184879303\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3337906002998352\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7609559297561646\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8253686428070068\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7487047910690308\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9478830695152283\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7327883243560791\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4227815568447113\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.13647718727588654\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7496572732925415\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 420/582 [18:26<03:27,  1.28s/it, loss=0.5353, iou=0.7276, accuracy=0.8446, f1_score=0.8423]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 430/582 [18:26<03:22,  1.34s/it, loss=0.5353, iou=0.7276, accuracy=0.8446, f1_score=0.8423]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.769890308380127\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.31997719407081604\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.11516869813203812\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6937756538391113\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.693684458732605\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.11147673428058624\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.743368923664093\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7836596369743347\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.24223271012306213\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.35175755620002747\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 430/582 [18:41<03:22,  1.34s/it, loss=0.5278, iou=0.7302, accuracy=0.8461, f1_score=0.8440]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 440/582 [18:41<03:20,  1.41s/it, loss=0.5278, iou=0.7302, accuracy=0.8461, f1_score=0.8440]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.12495057284832001\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7146511077880859\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8976595401763916\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7533837556838989\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8675659894943237\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7187595367431641\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8486482501029968\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6036758422851562\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.15209947526454926\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.30791574716567993\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 440/582 [18:55<03:20,  1.41s/it, loss=0.5208, iou=0.7322, accuracy=0.8474, f1_score=0.8454]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 450/582 [18:55<03:04,  1.40s/it, loss=0.5208, iou=0.7322, accuracy=0.8474, f1_score=0.8454]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4824899733066559\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.899279773235321\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8083949685096741\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7799088358879089\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8654006719589233\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8670408129692078\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8070108294487\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8854532837867737\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8657657504081726\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3558369576931\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 450/582 [19:05<03:04,  1.40s/it, loss=0.5147, iou=0.7335, accuracy=0.8484, f1_score=0.8463]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 460/582 [19:05<02:34,  1.27s/it, loss=0.5147, iou=0.7335, accuracy=0.8484, f1_score=0.8463]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3626834452152252\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8558107018470764\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.12772493064403534\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1229824349284172\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9930587410926819\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8092957735061646\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8749986886978149\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8256587982177734\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8222872614860535\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.12292521446943283\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 460/582 [19:21<02:34,  1.27s/it, loss=0.5088, iou=0.7351, accuracy=0.8494, f1_score=0.8473]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 470/582 [19:21<02:32,  1.36s/it, loss=0.5088, iou=0.7351, accuracy=0.8494, f1_score=0.8473]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.26682013273239136\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9932398200035095\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9296697974205017\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9436843991279602\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7416939735412598\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3228318989276886\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3365073502063751\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7704924941062927\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9868183732032776\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0588862895965576\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 470/582 [19:31<02:32,  1.36s/it, loss=0.5032, iou=0.7365, accuracy=0.8504, f1_score=0.8483]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 480/582 [19:31<02:09,  1.27s/it, loss=0.5032, iou=0.7365, accuracy=0.8504, f1_score=0.8483]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9100217819213867\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3276633024215698\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.36061719059944153\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.42599090933799744\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8618890047073364\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.33643653988838196\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0384953022003174\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9271703958511353\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8025014400482178\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.065254807472229\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 480/582 [19:41<02:09,  1.27s/it, loss=0.4976, iou=0.7377, accuracy=0.8513, f1_score=0.8491]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 490/582 [19:41<01:48,  1.18s/it, loss=0.4976, iou=0.7377, accuracy=0.8513, f1_score=0.8491]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.13020768761634827\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3673895299434662\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.35437414050102234\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7467079758644104\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8842883706092834\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6554391980171204\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7516540884971619\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4337182939052582\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.2877415120601654\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8518697619438171\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 490/582 [19:51<01:48,  1.18s/it, loss=0.4920, iou=0.7392, accuracy=0.8524, f1_score=0.8501]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 500/582 [19:51<01:32,  1.13s/it, loss=0.4920, iou=0.7392, accuracy=0.8524, f1_score=0.8501]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.068772792816162\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8444749116897583\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8673086762428284\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3070068657398224\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0299776792526245\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9135516881942749\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8522835373878479\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.39851123094558716\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9309647679328918\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3752148449420929\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 500/582 [20:05<01:32,  1.13s/it, loss=0.4876, iou=0.7399, accuracy=0.8529, f1_score=0.8505]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 510/582 [20:05<01:28,  1.23s/it, loss=0.4876, iou=0.7399, accuracy=0.8529, f1_score=0.8505]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.790127694606781\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1210428476333618\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0047688484191895\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.883163332939148\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.37409958243370056\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7575621604919434\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7698014378547668\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7776141166687012\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.928056001663208\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8022065758705139\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 510/582 [20:15<01:28,  1.23s/it, loss=0.4826, iou=0.7413, accuracy=0.8538, f1_score=0.8514]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 520/582 [20:15<01:11,  1.15s/it, loss=0.4826, iou=0.7413, accuracy=0.8538, f1_score=0.8514]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.2984294295310974\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7787240147590637\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9352489113807678\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3407799303531647\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.12344907224178314\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.879024088382721\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6490376591682434\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.807429313659668\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3186790645122528\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4060700535774231\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 520/582 [20:25<01:11,  1.15s/it, loss=0.4774, iou=0.7428, accuracy=0.8548, f1_score=0.8524]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 530/582 [20:25<00:56,  1.09s/it, loss=0.4774, iou=0.7428, accuracy=0.8548, f1_score=0.8524]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6818724274635315\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3031323254108429\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8977358341217041\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9025485515594482\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8092252016067505\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8646227717399597\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8290019035339355\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3611253798007965\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7757257223129272\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9047649502754211\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 530/582 [20:34<00:56,  1.09s/it, loss=0.4728, iou=0.7439, accuracy=0.8556, f1_score=0.8532]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 540/582 [20:34<00:44,  1.05s/it, loss=0.4728, iou=0.7439, accuracy=0.8556, f1_score=0.8532]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.9127346277236938\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.31449729204177856\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3055448830127716\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3583613634109497\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7991484999656677\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.1261892318725586\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7471011281013489\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.29594886302948\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 540/582 [20:48<00:44,  1.05s/it, loss=0.4703, iou=0.7451, accuracy=0.8563, f1_score=0.8540]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 550/582 [20:48<00:37,  1.16s/it, loss=0.4703, iou=0.7451, accuracy=0.8563, f1_score=0.8540]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.39489954710006714\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.4481697976589203\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8802410364151001\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.13233092427253723\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8501062989234924\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.357433021068573\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1997510939836502\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3857426941394806\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 550/582 [20:59<00:37,  1.16s/it, loss=0.4661, iou=0.7462, accuracy=0.8570, f1_score=0.8547]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 560/582 [20:59<00:24,  1.13s/it, loss=0.4661, iou=0.7462, accuracy=0.8570, f1_score=0.8547]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8618055582046509\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8122808933258057\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7969226837158203\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7862086892127991\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.302609384059906\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8647494316101074\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 1.0328999757766724\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7186298966407776\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.32137811183929443\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.1660209447145462\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 560/582 [21:10<00:24,  1.13s/it, loss=0.4617, iou=0.7476, accuracy=0.8579, f1_score=0.8556]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/582 [21:10<00:13,  1.11s/it, loss=0.4617, iou=0.7476, accuracy=0.8579, f1_score=0.8556]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3113909959793091\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.7673858404159546\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.32955655455589294\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.28439295291900635\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3825570046901703\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8484273552894592\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.8093646764755249\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.43823328614234924\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.149175226688385\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.3619448244571686\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 570/582 [21:20<00:13,  1.11s/it, loss=0.4575, iou=0.7486, accuracy=0.8587, f1_score=0.8563]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/582 [21:20<00:02,  1.08s/it, loss=0.4575, iou=0.7486, accuracy=0.8587, f1_score=0.8563]"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.6714792847633362\n"]},{"output_type":"stream","name":"stdout","text":["NaN dÃ©tectÃ© dans la perte - focal_bce: nan, dice_loss: 0.35147660970687866\n"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 580/582 [21:22<00:02,  1.08s/it, loss=0.4566, iou=0.7489, accuracy=0.8589, f1_score=0.8564]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 582/582 [21:22<00:00,  1.08s/it, loss=0.4566, iou=0.7489, accuracy=0.8589, f1_score=0.8564]"]},{"output_type":"stream","name":"stderr","text":["\rEntraÃ®nement: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 582/582 [21:22<00:00,  2.20s/it, loss=0.4566, iou=0.7489, accuracy=0.8589, f1_score=0.8564]"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   0%|                              | 0/124 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   1%|â–                             | 1/124 [00:23<48:11, 23.51s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   2%|â–                             | 2/124 [00:31<29:24, 14.46s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   2%|â–‹                             | 3/124 [00:32<16:24,  8.14s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   3%|â–‰                             | 4/124 [00:32<10:20,  5.17s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   4%|â–ˆâ–                            | 5/124 [00:33<06:59,  3.53s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   5%|â–ˆâ–                            | 6/124 [00:34<04:59,  2.54s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   6%|â–ˆâ–‹                            | 7/124 [00:34<03:43,  1.91s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   6%|â–ˆâ–‰                            | 8/124 [00:35<02:54,  1.50s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   7%|â–ˆâ–ˆâ–                           | 9/124 [00:47<09:22,  4.89s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   8%|â–ˆâ–ˆâ–                           | 10/124 [01:00<13:51,  7.30s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:   9%|â–ˆâ–ˆâ–‹                           | 11/124 [01:00<09:53,  5.25s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  10%|â–ˆâ–ˆâ–‰                           | 12/124 [01:01<07:10,  3.84s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  10%|â–ˆâ–ˆâ–ˆâ–                          | 13/124 [01:02<05:18,  2.87s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  11%|â–ˆâ–ˆâ–ˆâ–                          | 14/124 [01:02<04:00,  2.19s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  12%|â–ˆâ–ˆâ–ˆâ–‹                          | 15/124 [01:03<03:07,  1.72s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  13%|â–ˆâ–ˆâ–ˆâ–Š                          | 16/124 [01:04<02:29,  1.39s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  14%|â–ˆâ–ˆâ–ˆâ–ˆ                          | 17/124 [01:15<07:41,  4.31s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                         | 18/124 [01:29<12:55,  7.32s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 19/124 [01:30<09:17,  5.31s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 20/124 [01:30<06:45,  3.90s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 21/124 [01:31<05:00,  2.92s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 22/124 [01:32<03:47,  2.23s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                        | 23/124 [01:32<02:56,  1.74s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                        | 24/124 [01:33<02:20,  1.41s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 25/124 [01:45<07:43,  4.68s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 26/124 [01:58<11:54,  7.29s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 27/124 [01:59<08:32,  5.29s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 28/124 [02:00<06:13,  3.89s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 29/124 [02:01<04:49,  3.05s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 30/124 [02:01<03:38,  2.32s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 31/124 [02:02<02:48,  1.81s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 32/124 [02:03<02:13,  1.45s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 33/124 [02:17<08:12,  5.42s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 34/124 [02:27<10:00,  6.68s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 35/124 [02:28<07:12,  4.86s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 36/124 [02:30<05:52,  4.01s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 37/124 [02:31<04:29,  3.10s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 38/124 [02:31<03:22,  2.36s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 39/124 [02:32<02:36,  1.84s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 40/124 [02:32<02:03,  1.47s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 41/124 [02:46<07:15,  5.25s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 42/124 [02:47<05:16,  3.86s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 43/124 [02:51<05:13,  3.87s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 44/124 [02:58<06:13,  4.67s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 45/124 [02:59<04:58,  3.78s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 46/124 [03:00<03:41,  2.83s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 47/124 [03:00<02:47,  2.17s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 48/124 [03:01<02:09,  1.70s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 49/124 [03:14<06:19,  5.06s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 50/124 [03:15<04:35,  3.72s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                 | 51/124 [03:18<04:17,  3.53s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 52/124 [03:23<05:01,  4.19s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 53/124 [03:25<03:58,  3.36s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 54/124 [03:25<02:57,  2.54s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 55/124 [03:26<02:15,  1.96s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 56/124 [03:27<01:46,  1.56s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 57/124 [03:42<06:14,  5.59s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 58/124 [03:42<04:30,  4.10s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 59/124 [03:46<04:20,  4.00s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 60/124 [03:52<04:50,  4.54s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š               | 61/124 [03:52<03:31,  3.36s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 62/124 [03:53<02:37,  2.54s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 63/124 [03:54<01:59,  1.96s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 64/124 [03:54<01:33,  1.56s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 65/124 [04:08<05:11,  5.28s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 66/124 [04:10<03:58,  4.11s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 67/124 [04:12<03:30,  3.70s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 68/124 [04:20<04:35,  4.93s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹             | 69/124 [04:21<03:19,  3.64s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 70/124 [04:21<02:27,  2.73s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 71/124 [04:22<01:51,  2.10s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 72/124 [04:23<01:26,  1.66s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 73/124 [04:35<04:09,  4.90s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 74/124 [04:38<03:26,  4.14s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 75/124 [04:39<02:50,  3.48s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 76/124 [04:48<03:56,  4.92s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 77/124 [04:48<02:50,  3.63s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 78/124 [04:49<02:05,  2.73s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 79/124 [04:50<01:34,  2.10s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 80/124 [04:50<01:12,  1.65s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 81/124 [05:02<03:17,  4.60s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 82/124 [05:04<02:40,  3.82s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 83/124 [05:04<01:57,  2.86s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 84/124 [05:15<03:32,  5.30s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 85/124 [05:16<02:32,  3.90s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 86/124 [05:17<01:50,  2.92s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 87/124 [05:17<01:22,  2.23s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 88/124 [05:18<01:02,  1.74s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 89/124 [05:30<02:50,  4.88s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 90/124 [05:32<02:20,  4.15s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 91/124 [05:33<01:41,  3.09s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 92/124 [05:44<02:49,  5.29s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 93/124 [05:44<02:00,  3.89s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 94/124 [05:45<01:27,  2.91s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 95/124 [05:45<01:04,  2.22s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 96/124 [05:46<00:48,  1.74s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 97/124 [05:58<02:10,  4.82s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 98/124 [06:05<02:23,  5.50s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 99/124 [06:06<01:40,  4.04s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 100/124 [06:14<02:08,  5.34s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 101/124 [06:15<01:30,  3.92s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 102/124 [06:15<01:04,  2.93s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 103/124 [06:16<00:47,  2.24s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/124 [06:17<00:35,  1.75s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/124 [06:28<01:28,  4.65s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 106/124 [06:35<01:35,  5.31s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 107/124 [06:35<01:06,  3.90s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 108/124 [06:45<01:27,  5.49s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 109/124 [06:45<01:00,  4.03s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 110/124 [06:46<00:42,  3.00s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 111/124 [06:46<00:29,  2.29s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 112/124 [06:47<00:21,  1.79s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 113/124 [06:59<00:52,  4.79s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 114/124 [07:04<00:48,  4.88s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 115/124 [07:05<00:32,  3.60s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116/124 [07:13<00:40,  5.06s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 117/124 [07:14<00:26,  3.73s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 118/124 [07:14<00:16,  2.80s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 119/124 [07:15<00:10,  2.14s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 120/124 [07:16<00:06,  1.69s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 121/124 [07:28<00:15,  5.06s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 122/124 [07:33<00:09,  4.98s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 123/124 [07:34<00:03,  3.67s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [07:44<00:00,  5.65s/it]"]},{"output_type":"stream","name":"stderr","text":["\rValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [07:44<00:00,  3.75s/it]"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Erreur pendant l'exÃ©cution: \n","Traceback (most recent call last):\n","  File \"<ipython-input-96-8835002ebaf0>\", line 804, in on_run_button_clicked\n","    results = run_workflow_colab(\n","              ^^^^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-96-8835002ebaf0>\", line 577, in run_workflow_colab\n","    model, tracker = train_model(\n","                     ^^^^^^^^^^^^\n","  File \"<ipython-input-92-860793a71868>\", line 348, in train_model\n","    visualize_predictions_tensorboard(\n","  File \"<ipython-input-94-30b17769910e>\", line 654, in visualize_predictions_tensorboard\n","    writer.add_images(f'Metrics/threshold_{threshold_value}m', img_tensor, epoch)\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/writer.py\", line 669, in add_images\n","    image(tag, img_tensor, dataformats=dataformats), global_step, walltime\n","    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/summary.py\", line 572, in image\n","    tensor = convert_to_HWC(tensor, dataformats)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/_utils.py\", line 112, in convert_to_HWC\n","    tensor_CHW = make_grid(tensor_NCHW)\n","                 ^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/tensorboard/_utils.py\", line 78, in make_grid\n","    assert I.ndim == 4 and I.shape[1] == 3\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","AssertionError\n","\n"]}]}},"82200b5472ce4549adfd61b1af9d42a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}