{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMC5OQm2t9i4hsf1uZeSft/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# PrÃ©paration des donnÃ©es pour la dÃ©tection des trouÃ©es forestiÃ¨res par Deep Learning (U-NET et DeepLabV3+)\n","\n","Ce script permet de prÃ©parer les donnÃ©es pour l'analyse des modÃ¨les\n","de deep learning dans la dÃ©tection automatique des trouÃ©es forestiÃ¨res Ã  partir\n","de modÃ¨les numÃ©riques de surface (DSM) et de hauteur de canopÃ©e (CHM).\n","\n","**Auteur :** VANDER LINDEN Arthur\n","**Date :** 28-02-2025\n"],"metadata":{"id":"lNBEGjGedMAb"}},{"cell_type":"markdown","source":["# PARTIE 1 : CONFIGURATION"],"metadata":{"id":"pA73LYyk1NHx"}},{"cell_type":"code","source":["# =====================================================================\n","# SECTION 1: INITIALISATION ET IMPORTATION DES PACKAGES NÃ‰CESSAIRES\n","# =====================================================================\n","\n","# Installation des packages nÃ©cessaires\n","!pip install -q rasterio==1.3.8 geopandas\n","\n","# Importation des bibliothÃ¨ques standard\n","import os\n","import glob\n","import pickle\n","import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import traceback\n","import time\n","import random\n","import re\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","from tensorflow.keras.utils import Sequence\n","\n","# Importation des bibliothÃ¨ques pour le traitement de donnÃ©es gÃ©ospatiales\n","import rasterio\n","from rasterio.windows import Window, from_bounds\n","from rasterio.warp import reproject, Resampling, calculate_default_transform\n","from datetime import datetime\n","\n","# Montage de Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_AwmgShvc6PN","executionInfo":{"status":"ok","timestamp":1741083277092,"user_tz":-60,"elapsed":3714,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"}},"outputId":"3c5d57cc-4032-4409-bf91-26e84564dc23"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# =====================================================================\n","# SECTION 2: CONFIGURATION DES PARAMÃˆTRES ET CHEMINS\n","# =====================================================================\n","\n","class Config:\n","    \"\"\"Classe de configuration centralisÃ©e pour le projet de dÃ©tection de trouÃ©es forestiÃ¨res\"\"\"\n","\n","    def __init__(self):\n","        # RÃ©pertoire de base\n","        self.BASE_DIR = '/content/drive/MyDrive/ForestGaps_DeepLearning_Workflow'\n","\n","        # Sous-rÃ©pertoires principaux\n","        self.DATA_DIR = os.path.join(self.BASE_DIR, 'data')\n","        self.DATA_EXTERNAL_TEST_DIR = os.path.join(self.BASE_DIR, 'data_external_test')\n","        self.PROCESSED_DIR = os.path.join(self.BASE_DIR, 'processed')\n","        self.TILES_DIR = os.path.join(self.PROCESSED_DIR, 'tiles')\n","\n","        # Sous-rÃ©pertoires pour les tuiles\n","        self.TRAIN_TILES_DIR = os.path.join(self.TILES_DIR, 'train')\n","        self.VAL_TILES_DIR = os.path.join(self.TILES_DIR, 'val')\n","        self.TEST_TILES_DIR = os.path.join(self.TILES_DIR, 'test')\n","\n","        # CrÃ©ation des rÃ©pertoires s'ils n'existent pas\n","        for dir_path in [\n","            self.DATA_DIR, self.DATA_EXTERNAL_TEST_DIR, self.PROCESSED_DIR,\n","            self.TILES_DIR, self.TRAIN_TILES_DIR, self.VAL_TILES_DIR, self.TEST_TILES_DIR\n","        ]:\n","            os.makedirs(dir_path, exist_ok=True)\n","\n","        # ParamÃ¨tres de traitement\n","        self.TILE_SIZE = 256  # Taille des tuiles en pixels\n","        self.BATCH_SIZE = 32  # Taille des batchs pour l'entraÃ®nement\n","        self.THRESHOLDS = [10, 15, 20, 25, 30]  # Seuils de hauteur en mÃ¨tres\n","        self.TEST_SPLIT = 0.15  # Proportion de donnÃ©es pour le test\n","        self.VAL_SPLIT = 0.15   # Proportion de donnÃ©es pour la validation\n","        self.OVERLAP = 0.30  # Chevauchement entre les tuiles lors de l'extraction\n","\n","    def save_config(self, filepath=None):\n","        \"\"\"Sauvegarde la configuration actuelle dans un fichier JSON\"\"\"\n","        if filepath is None:\n","            filepath = os.path.join(self.PROCESSED_DIR, 'config.json')\n","\n","        # CrÃ©ation d'un dictionnaire de configuration\n","        config_dict = {k: v for k, v in self.__dict__.items() if not k.startswith('__') and not callable(v)}\n","\n","        # Conversion des chemins en chaÃ®nes de caractÃ¨res\n","        for k, v in config_dict.items():\n","            if isinstance(v, Path):\n","                config_dict[k] = str(v)\n","\n","        # Sauvegarde dans un fichier JSON\n","        with open(filepath, 'w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","        print(f\"Configuration sauvegardÃ©e dans {filepath}\")\n","\n","    def load_config(self, filepath):\n","        \"\"\"Charge une configuration Ã  partir d'un fichier JSON\"\"\"\n","        with open(filepath, 'r') as f:\n","            config_dict = json.load(f)\n","\n","        # Mise Ã  jour des attributs\n","        for k, v in config_dict.items():\n","            setattr(self, k, v)\n","\n","        print(f\"Configuration chargÃ©e depuis {filepath}\")\n","\n","# Initialisation de la configuration\n","config = Config()"],"metadata":{"id":"ghghlDijc-Yz","executionInfo":{"status":"ok","timestamp":1741083277097,"user_tz":-60,"elapsed":3,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CyJfqyZ7bwtd","executionInfo":{"status":"ok","timestamp":1741083277116,"user_tz":-60,"elapsed":5,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"}}},"outputs":[],"source":["# =====================================================================\n","# SECTION 3: FONCTIONS UTILITAIRES POUR LE TRAITEMENT DES DONNÃ‰ES\n","# =====================================================================\n","\n","def find_chm_dsm_pairs(directory):\n","    \"\"\"\n","    Trouve toutes les paires CHM/DSM dans un rÃ©pertoire.\n","    Les paires sont identifiÃ©es par leur prÃ©fixe commun avant les suffixes _CHM.tif et _DSM.tif.\n","\n","    Args:\n","        directory: RÃ©pertoire contenant les donnÃ©es\n","\n","    Returns:\n","        Une liste de tuples (chemin_chm, chemin_dsm, prÃ©fixe)\n","    \"\"\"\n","    # Rechercher tous les fichiers CHM et DSM\n","    chm_files = glob.glob(os.path.join(directory, '*CHM.tif'))\n","    dsm_files = glob.glob(os.path.join(directory, '*DSM.tif'))\n","\n","    # Dictionnaires pour stocker les chemins par prÃ©fixe\n","    chm_dict = {}\n","    dsm_dict = {}\n","\n","    # Extraire les prÃ©fixes des fichiers CHM\n","    for chm_path in chm_files:\n","        filename = os.path.basename(chm_path)\n","        prefix = filename.replace('CHM.tif', '').rstrip('_')\n","        chm_dict[prefix] = chm_path\n","\n","    # Extraire les prÃ©fixes des fichiers DSM\n","    for dsm_path in dsm_files:\n","        filename = os.path.basename(dsm_path)\n","        prefix = filename.replace('DSM.tif', '').rstrip('_')\n","        dsm_dict[prefix] = dsm_path\n","\n","    # Trouver les prÃ©fixes communs (les paires)\n","    pairs = []\n","    for prefix in chm_dict:\n","        if prefix in dsm_dict:\n","            pairs.append((chm_dict[prefix], dsm_dict[prefix], prefix))\n","\n","    # Trier les paires par prÃ©fixe pour une sortie cohÃ©rente\n","    return sorted(pairs, key=lambda x: x[2])\n","\n","def analyze_raster_pair(dsm_path, chm_path, prefix):\n","    \"\"\"\n","    Analyse une paire de rasters DSM/CHM et gÃ©nÃ¨re un rapport dÃ©taillÃ©.\n","\n","    Args:\n","        dsm_path: Chemin vers le fichier DSM\n","        chm_path: Chemin vers le fichier CHM\n","        prefix: PrÃ©fixe du site\n","\n","    Returns:\n","        Dictionnaire contenant le rapport d'analyse\n","    \"\"\"\n","    report = {\n","        'site_id': prefix,\n","        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'valid': True,\n","        'warnings': [],\n","        'errors': [],\n","        'metadata': {'dsm': {}, 'chm': {}},\n","        'statistics': {'dsm': {}, 'chm': {}},\n","        'alignment_status': 'aligned',\n","        'processing_needed': [],\n","        'processing_history': []\n","    }\n","\n","    try:\n","        # Analyse du DSM\n","        with rasterio.open(dsm_path) as dsm:\n","            dsm_data = dsm.read(1)\n","\n","            # MÃ©tadonnÃ©es\n","            report['metadata']['dsm'] = {\n","                'path': dsm_path,\n","                'shape': (dsm.width, dsm.height),\n","                'resolution': dsm.res,\n","                'crs': str(dsm.crs),\n","                'bounds': {\n","                    'left': dsm.bounds.left,\n","                    'bottom': dsm.bounds.bottom,\n","                    'right': dsm.bounds.right,\n","                    'top': dsm.bounds.top\n","                },\n","                'transform': list(dsm.transform),\n","                'nodata': dsm.nodata,\n","                'driver': dsm.driver\n","            }\n","\n","            # Statistiques basiques\n","            valid_mask = ~np.isnan(dsm_data)\n","            if dsm.nodata is not None:\n","                valid_mask &= (dsm_data != dsm.nodata)\n","\n","            if np.any(valid_mask):\n","                report['statistics']['dsm'] = {\n","                    'min': float(np.min(dsm_data[valid_mask])),\n","                    'max': float(np.max(dsm_data[valid_mask])),\n","                    'mean': float(np.mean(dsm_data[valid_mask])),\n","                    'std': float(np.std(dsm_data[valid_mask])),\n","                    'valid_ratio': float(np.sum(valid_mask) / dsm_data.size)\n","                }\n","            else:\n","                report['statistics']['dsm'] = {\n","                    'valid_ratio': 0.0\n","                }\n","                report['errors'].append(\"Le DSM ne contient aucune donnÃ©e valide\")\n","                report['valid'] = False\n","\n","        # Analyse du CHM\n","        with rasterio.open(chm_path) as chm:\n","            chm_data = chm.read(1)\n","\n","            # MÃ©tadonnÃ©es\n","            report['metadata']['chm'] = {\n","                'path': chm_path,\n","                'shape': (chm.width, chm.height),\n","                'resolution': chm.res,\n","                'crs': str(chm.crs),\n","                'bounds': {\n","                    'left': chm.bounds.left,\n","                    'bottom': chm.bounds.bottom,\n","                    'right': chm.bounds.right,\n","                    'top': chm.bounds.top\n","                },\n","                'transform': list(chm.transform),\n","                'nodata': chm.nodata,\n","                'driver': chm.driver\n","            }\n","\n","            # Statistiques basiques\n","            valid_mask = ~np.isnan(chm_data)\n","            if chm.nodata is not None:\n","                valid_mask &= (chm_data != chm.nodata)\n","\n","            if np.any(valid_mask):\n","                report['statistics']['chm'] = {\n","                    'min': float(np.min(chm_data[valid_mask])),\n","                    'max': float(np.max(chm_data[valid_mask])),\n","                    'mean': float(np.mean(chm_data[valid_mask])),\n","                    'std': float(np.std(chm_data[valid_mask])),\n","                    'valid_ratio': float(np.sum(valid_mask) / chm_data.size)\n","                }\n","            else:\n","                report['statistics']['chm'] = {\n","                    'valid_ratio': 0.0\n","                }\n","                report['errors'].append(\"Le CHM ne contient aucune donnÃ©e valide\")\n","                report['valid'] = False\n","\n","        # VÃ©rification des projections\n","        with rasterio.open(dsm_path) as dsm, rasterio.open(chm_path) as chm:\n","            if dsm.crs != chm.crs:\n","                report['warnings'].append(f\"Projections diffÃ©rentes: DSM={dsm.crs}, CHM={chm.crs}\")\n","                report['alignment_status'] = 'misaligned'\n","                report['processing_needed'].append('reproject')\n","\n","            # VÃ©rifier les rÃ©solutions\n","            resolution_threshold = 0.01  # TolÃ©rance pour les diffÃ©rences de rÃ©solution\n","            if (abs(dsm.res[0] - chm.res[0]) > resolution_threshold or\n","                abs(dsm.res[1] - chm.res[1]) > resolution_threshold):\n","                report['warnings'].append(f\"RÃ©solutions diffÃ©rentes: DSM={dsm.res}, CHM={chm.res}\")\n","                report['alignment_status'] = 'misaligned'\n","                report['processing_needed'].append('resample')\n","\n","            # VÃ©rifier les emprises\n","            bounds_threshold = 1.0  # TolÃ©rance pour les diffÃ©rences d'emprise en unitÃ©s de carte\n","            if (abs(dsm.bounds.left - chm.bounds.left) > bounds_threshold or\n","                abs(dsm.bounds.bottom - chm.bounds.bottom) > bounds_threshold or\n","                abs(dsm.bounds.right - chm.bounds.right) > bounds_threshold or\n","                abs(dsm.bounds.top - chm.bounds.top) > bounds_threshold):\n","                report['warnings'].append(\"Emprises diffÃ©rentes\")\n","                report['alignment_status'] = 'misaligned'\n","                report['processing_needed'].append('align_extent')\n","\n","            # VÃ©rifier des dimensions en pixels\n","            if dsm.width != chm.width or dsm.height != chm.height:\n","                report['warnings'].append(f\"Dimensions diffÃ©rentes: DSM={dsm.width}x{dsm.height}, CHM={chm.width}x{chm.height}\")\n","\n","    except Exception as e:\n","        report['valid'] = False\n","        report['errors'].append(f\"Erreur lors de l'analyse: {str(e)}\")\n","\n","    return report\n","\n","def align_rasters(dsm_path, chm_path, report, config):\n","    \"\"\"\n","    Aligne les rasters DSM et CHM pour qu'ils aient la mÃªme projection, rÃ©solution et emprise.\n","    Utilise le DSM comme rÃ©fÃ©rence pour aligner le CHM.\n","\n","    Args:\n","        dsm_path: Chemin vers le fichier DSM\n","        chm_path: Chemin vers le fichier CHM\n","        report: Rapport d'analyse\n","        config: Configuration du projet\n","\n","    Returns:\n","        Tuple (dsm_path_aligned, chm_path_aligned, updated_report)\n","    \"\"\"\n","    # Si les rasters sont dÃ©jÃ  alignÃ©s, rien Ã  faire\n","    if report['alignment_status'] == 'aligned':\n","        return dsm_path, chm_path, report\n","\n","    # CrÃ©er un dossier pour les fichiers traitÃ©s\n","    aligned_dir = os.path.join(config.PROCESSED_DIR, 'aligned_rasters')\n","    os.makedirs(aligned_dir, exist_ok=True)\n","\n","    prefix = report['site_id']\n","\n","    # Chemins des fichiers de sortie\n","    dsm_aligned_path = dsm_path  # Par dÃ©faut, pas de modification\n","    chm_aligned_path = os.path.join(aligned_dir, f\"{prefix}_CHM_aligned.tif\")\n","\n","    # Ouvrir les fichiers source\n","    with rasterio.open(dsm_path) as dsm_src, rasterio.open(chm_path) as chm_src:\n","        # Utiliser le DSM comme rÃ©fÃ©rence pour le nouveau CHM\n","        dst_crs = dsm_src.crs\n","        dst_transform = dsm_src.transform\n","        dst_width = dsm_src.width\n","        dst_height = dsm_src.height\n","        dst_nodata = chm_src.nodata if chm_src.nodata is not None else 0\n","\n","        # Si les projections diffÃ¨rent, recalculer la transformation\n","        if 'reproject' in report['processing_needed'] or 'resample' in report['processing_needed']:\n","            dst_transform, dst_width, dst_height = calculate_default_transform(\n","                chm_src.crs, dst_crs, dsm_src.width, dsm_src.height,\n","                *dsm_src.bounds\n","            )\n","\n","        # PrÃ©parer les mÃ©tadonnÃ©es pour le CHM alignÃ©\n","        dst_profile = chm_src.profile.copy()\n","        dst_profile.update({\n","            'crs': dst_crs,\n","            'transform': dst_transform,\n","            'width': dst_width,\n","            'height': dst_height,\n","            'nodata': dst_nodata\n","        })\n","\n","        # CrÃ©er le fichier CHM alignÃ©\n","        with rasterio.open(chm_aligned_path, 'w', **dst_profile) as dst:\n","            # Reprojeter/RÃ©echantillonner le CHM\n","            reproject(\n","                source=rasterio.band(chm_src, 1),\n","                destination=rasterio.band(dst, 1),\n","                src_transform=chm_src.transform,\n","                src_crs=chm_src.crs,\n","                dst_transform=dst_transform,\n","                dst_crs=dst_crs,\n","                src_nodata=chm_src.nodata,\n","                dst_nodata=dst_nodata,\n","                resampling=Resampling.bilinear\n","            )\n","\n","        # Mettre Ã  jour le rapport\n","        action = \"Alignement du CHM avec le DSM: \"\n","        if 'reproject' in report['processing_needed']:\n","            action += f\"reprojection de {chm_src.crs} vers {dst_crs}, \"\n","        if 'resample' in report['processing_needed']:\n","            action += f\"rÃ©Ã©chantillonnage de {chm_src.res} vers {dsm_src.res}, \"\n","        if 'align_extent' in report['processing_needed']:\n","            action += \"alignement des emprises, \"\n","\n","        report['processing_history'].append(action.rstrip(\", \"))\n","        report['alignment_status'] = 'aligned'\n","        report['warnings'].append(f\"Le CHM a Ã©tÃ© alignÃ© sur le DSM: {chm_aligned_path}\")\n","\n","    return dsm_aligned_path, chm_aligned_path, report\n","\n","def process_raster_pair_robustly(dsm_path, chm_path, prefix, config, save_report=True, force_reanalysis=False):\n","    \"\"\"\n","    Traite une paire de rasters DSM/CHM de maniÃ¨re robuste.\n","    Ã‰vite de rÃ©pÃ©ter l'analyse si elle a dÃ©jÃ  Ã©tÃ© effectuÃ©e.\n","\n","    Args:\n","        dsm_path: Chemin vers le fichier DSM\n","        chm_path: Chemin vers le fichier CHM\n","        prefix: PrÃ©fixe du site\n","        config: Configuration du projet\n","        save_report: Si True, sauvegarde le rapport au format JSON\n","        force_reanalysis: Si True, force la rÃ©analyse mÃªme si un rapport existe dÃ©jÃ \n","\n","    Returns:\n","        Tuple (dsm_path_processed, chm_path_processed, report, success)\n","    \"\"\"\n","    print(f\"Analyse de la paire: {prefix}\")\n","\n","    # VÃ©rifier si l'analyse a dÃ©jÃ  Ã©tÃ© effectuÃ©e\n","    report_dir = os.path.join(config.PROCESSED_DIR, 'reports')\n","    os.makedirs(report_dir, exist_ok=True)\n","    report_path = os.path.join(report_dir, f\"{prefix}_report.json\")\n","\n","    if not force_reanalysis and os.path.exists(report_path):\n","        try:\n","            with open(report_path, 'r') as f:\n","                report = json.load(f)\n","\n","            print(f\"Utilisation de l'analyse existante pour {prefix}\")\n","\n","            # RÃ©cupÃ©rer les chemins traitÃ©s\n","            dsm_processed = dsm_path  # Par dÃ©faut, utiliser le chemin d'origine\n","\n","            # VÃ©rifier si le rapport contient un chemin alignÃ© pour le CHM\n","            if 'alignment_status' in report and report['alignment_status'] == 'aligned':\n","                # Si c'Ã©tait dÃ©jÃ  alignÃ©, utiliser le chemin d'origine\n","                if not report['processing_history']:\n","                    chm_processed = chm_path\n","                else:\n","                    # Sinon, reconstruire le chemin du CHM alignÃ©\n","                    aligned_dir = os.path.join(config.PROCESSED_DIR, 'aligned_rasters')\n","                    chm_processed = os.path.join(aligned_dir, f\"{prefix}_CHM_aligned.tif\")\n","\n","                    # VÃ©rifier que le fichier existe bien\n","                    if not os.path.exists(chm_processed):\n","                        print(f\"âš ï¸ Fichier CHM alignÃ© manquant: {chm_processed}\")\n","                        print(\"RÃ©analyse de la paire...\")\n","                        return process_raster_pair_robustly(dsm_path, chm_path, prefix, config, save_report, True)\n","            else:\n","                # Si pas alignÃ©, utiliser le chemin d'origine\n","                chm_processed = chm_path\n","\n","            return dsm_processed, chm_processed, report, True\n","\n","        except Exception as e:\n","            print(f\"âš ï¸ Erreur lors de la lecture du rapport existant: {str(e)}\")\n","            print(\"RÃ©analyse de la paire...\")\n","\n","    # Si pas de rapport existant ou force_reanalysis, faire l'analyse complÃ¨te\n","    # GÃ©nÃ©rer le rapport d'analyse\n","    report = analyze_raster_pair(dsm_path, chm_path, prefix)\n","\n","    # VÃ©rifier s'il y a des erreurs critiques\n","    if not report['valid']:\n","        print(f\"ERREUR: Paire invalide ({prefix})\")\n","        for error in report['errors']:\n","            print(f\"  - {error}\")\n","\n","        if save_report:\n","            error_report_path = os.path.join(report_dir, f\"{prefix}_error_report.json\")\n","            with open(error_report_path, 'w') as f:\n","                json.dump(report, f, indent=2)\n","\n","        return dsm_path, chm_path, report, False\n","\n","    # Afficher un rÃ©sumÃ© du rapport\n","    print(f\"  DSM: {report['metadata']['dsm']['shape'][0]}x{report['metadata']['dsm']['shape'][1]}, rÃ©solution: {report['metadata']['dsm']['resolution']}\")\n","    print(f\"  CHM: {report['metadata']['chm']['shape'][0]}x{report['metadata']['chm']['shape'][1]}, rÃ©solution: {report['metadata']['chm']['resolution']}\")\n","\n","    if report['alignment_status'] == 'misaligned':\n","        print(f\"  Alignement requis: {', '.join(report['processing_needed'])}\")\n","\n","        # Aligner les rasters si nÃ©cessaire\n","        dsm_path, chm_path, report = align_rasters(dsm_path, chm_path, report, config)\n","        print(f\"  â†’ Alignement effectuÃ©\")\n","    else:\n","        print(f\"  âœ“ Alignement correct\")\n","\n","    # Sauvegarder le rapport\n","    if save_report:\n","        with open(report_path, 'w') as f:\n","            json.dump(report, f, indent=2)\n","\n","    return dsm_path, chm_path, report, True\n","\n","def create_gap_masks(chm_path, thresholds, output_dir, prefix, force_regenerate=False):\n","    \"\"\"\n","    CrÃ©e des masques binaires de trouÃ©es Ã  partir d'un CHM pour diffÃ©rents seuils de hauteur.\n","    VÃ©rifie physiquement si chaque masque existe avant de le crÃ©er.\n","\n","    Args:\n","        chm_path: Chemin vers le fichier CHM\n","        thresholds: Liste des seuils de hauteur (en mÃ¨tres)\n","        output_dir: RÃ©pertoire de sortie pour les masques\n","        prefix: PrÃ©fixe du site pour nommer les fichiers\n","        force_regenerate: Si True, force la rÃ©gÃ©nÃ©ration des masques mÃªme s'ils existent dÃ©jÃ \n","\n","    Returns:\n","        Liste des chemins vers les masques crÃ©Ã©s\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    mask_paths = {}\n","    all_masks_exist = True\n","\n","    # VÃ©rifier pour chaque seuil\n","    for threshold in thresholds:\n","        output_path = os.path.join(output_dir, f'{prefix}_gap_mask_{threshold}m.tif')\n","        mask_paths[threshold] = output_path\n","\n","        # VÃ©rifier si le fichier existe rÃ©ellement\n","        if os.path.isfile(output_path) and not force_regenerate:\n","            try:\n","                # VÃ©rifier que le fichier est valide en l'ouvrant\n","                with rasterio.open(output_path) as src:\n","                    # Lire un petit Ã©chantillon pour vÃ©rifier l'intÃ©gritÃ©\n","                    sample = src.read(1, window=Window(0, 0, 10, 10))\n","                print(f\"Masque pour le seuil {threshold}m - site {prefix} dÃ©jÃ  existant. Utilisation du fichier existant.\")\n","                continue\n","            except Exception as e:\n","                print(f\"âš ï¸ Masque existant corrompu pour le seuil {threshold}m - site {prefix}: {str(e)}\")\n","                all_masks_exist = False\n","        else:\n","            all_masks_exist = False\n","\n","    # Si tous les masques existent et sont valides, retourner les chemins\n","    if all_masks_exist:\n","        return mask_paths\n","\n","    # Pour les masques manquants ou corrompus, crÃ©er les masques\n","    for threshold in thresholds:\n","        output_path = mask_paths[threshold]\n","        if os.path.isfile(output_path) and not force_regenerate:\n","            continue  # Sauter ceux qui existent dÃ©jÃ  et sont valides\n","\n","        print(f\"CrÃ©ation du masque pour le seuil {threshold}m - site {prefix}...\")\n","\n","        # Ouvrir le CHM (seulement si nÃ©cessaire)\n","        with rasterio.open(chm_path) as src:\n","            chm_data = src.read(1)\n","            meta = src.meta.copy()\n","\n","        # Modifier les mÃ©tadonnÃ©es pour les masques binaires\n","        meta.update(dtype='uint8', nodata=255)\n","\n","        # CrÃ©er un masque binaire (1=trouÃ©e, 0=non-trouÃ©e)\n","        gap_mask = np.zeros_like(chm_data, dtype='uint8')\n","\n","        # Les pixels avec hauteur < seuil sont des trouÃ©es (1)\n","        mask_valid = ~np.isnan(chm_data)\n","        gap_mask[mask_valid & (chm_data < threshold)] = 1\n","        gap_mask[~mask_valid] = 255  # nodata value\n","\n","        # Sauvegarder le masque\n","        with rasterio.open(output_path, 'w', **meta) as dst:\n","            dst.write(gap_mask, 1)\n","\n","        # Calcul des statistiques\n","        valid_pixels = np.sum(mask_valid)\n","        gap_pixels = np.sum(gap_mask[mask_valid] == 1)\n","        gap_pct = gap_pixels / valid_pixels * 100 if valid_pixels > 0 else 0\n","\n","        print(f\"  Total pixels valides: {valid_pixels}\")\n","        print(f\"  Pixels de trouÃ©es: {gap_pixels} ({gap_pct:.2f}%)\")\n","\n","    return mask_paths\n","\n","def calculate_and_save_gap_statistics(gap_masks_by_prefix, config):\n","    \"\"\"\n","    Calcule et sauvegarde les statistiques globales de trouÃ©es par seuil.\n","    Convertit les types NumPy en types Python natifs pour Ã©viter l'erreur de sÃ©rialisation JSON.\n","\n","    Args:\n","        gap_masks_by_prefix: Dictionnaire des chemins de masques par prÃ©fixe et par seuil\n","        config: Configuration du projet\n","\n","    Returns:\n","        Dictionnaire des ratios de trouÃ©es par seuil\n","    \"\"\"\n","    # Initialiser les compteurs\n","    total_pixels_by_threshold = {threshold: 0 for threshold in config.THRESHOLDS}\n","    gap_pixels_by_threshold = {threshold: 0 for threshold in config.THRESHOLDS}\n","\n","    # Parcourir tous les sites\n","    for prefix, mask_paths in gap_masks_by_prefix.items():\n","        for threshold, mask_path in mask_paths.items():\n","            try:\n","                with rasterio.open(mask_path) as src:\n","                    mask_data = src.read(1)\n","                    valid_mask = (mask_data != 255)\n","                    total_valid = np.sum(valid_mask)\n","                    gap_pixels = np.sum(mask_data[valid_mask] == 1)\n","\n","                    # Convertir les types NumPy en types Python natifs\n","                    total_pixels_by_threshold[threshold] += int(total_valid)\n","                    gap_pixels_by_threshold[threshold] += int(gap_pixels)\n","            except Exception as e:\n","                print(f\"âš ï¸ Erreur lors de l'analyse du masque {mask_path}: {str(e)}\")\n","                continue\n","\n","    # Calculer les ratios\n","    gap_ratios = {}\n","    for threshold in config.THRESHOLDS:\n","        if total_pixels_by_threshold[threshold] > 0:\n","            gap_ratios[threshold] = float(gap_pixels_by_threshold[threshold]) / float(total_pixels_by_threshold[threshold])\n","        else:\n","            gap_ratios[threshold] = 0.0  # Valeur par dÃ©faut si aucun pixel valide\n","\n","    # PrÃ©parer les donnÃ©es Ã  sauvegarder\n","    stats_data = {\n","        'total_pixels': {str(k): int(v) for k, v in total_pixels_by_threshold.items()},\n","        'gap_pixels': {str(k): int(v) for k, v in gap_pixels_by_threshold.items()},\n","        'gap_ratios': {str(k): float(v) for k, v in gap_ratios.items()},\n","        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","        'prefixes': list(gap_masks_by_prefix.keys())  # Liste des prÃ©fixes inclus dans ces statistiques\n","    }\n","\n","    # Sauvegarde sÃ©curisÃ©e: Ã©crire d'abord dans un fichier temporaire\n","    stats_path = os.path.join(config.PROCESSED_DIR, 'gap_statistics.json')\n","    temp_path = stats_path + \".tmp\"\n","\n","    try:\n","        # Ã‰crire dans un fichier temporaire\n","        with open(temp_path, 'w') as f:\n","            json.dump(stats_data, f, indent=2)\n","\n","        # Si l'Ã©criture a rÃ©ussi, renommer le fichier temporaire\n","        os.replace(temp_path, stats_path)\n","\n","        # CrÃ©er Ã©galement un fichier de backup\n","        backup_path = stats_path + \".backup\"\n","        with open(backup_path, 'w') as f:\n","            json.dump(stats_data, f, indent=2)\n","\n","    except Exception as e:\n","        print(f\"âš ï¸ Erreur lors de la sauvegarde des statistiques: {str(e)}\")\n","        # Supprimer le fichier temporaire si l'opÃ©ration a Ã©chouÃ©\n","        if os.path.exists(temp_path):\n","            os.remove(temp_path)\n","\n","    print(\"\\nStatistiques globales des trouÃ©es:\")\n","    for threshold, ratio in gap_ratios.items():\n","        print(f\"  Seuil {threshold}m: {ratio:.2%} de pixels sont des trouÃ©es\")\n","\n","    return gap_ratios\n","\n","def safe_load_json(file_path, backup_suffix=\"_backup\"):\n","    \"\"\"\n","    Charge un fichier JSON de maniÃ¨re sÃ©curisÃ©e, avec gestion des erreurs et fichier de backup.\n","\n","    Args:\n","        file_path: Chemin vers le fichier JSON Ã  charger\n","        backup_suffix: Suffixe pour le fichier de backup\n","\n","    Returns:\n","        Dictionnaire chargÃ© ou None si Ã©chec\n","    \"\"\"\n","    if not os.path.exists(file_path):\n","        return None\n","\n","    # Essayer de charger le fichier principal\n","    try:\n","        with open(file_path, 'r') as f:\n","            data = json.load(f)\n","        return data\n","    except Exception as main_error:\n","        print(f\"âš ï¸ Erreur lors du chargement de {file_path}: {str(main_error)}\")\n","\n","        # Essayer de charger le backup si disponible\n","        backup_path = file_path + backup_suffix\n","        if os.path.exists(backup_path):\n","            try:\n","                print(f\"Tentative de restauration depuis le backup...\")\n","                with open(backup_path, 'r') as f:\n","                    data = json.load(f)\n","\n","                # Si le backup fonctionne, l'utiliser pour rÃ©parer le fichier principal\n","                with open(file_path, 'w') as f:\n","                    json.dump(data, f, indent=2)\n","                print(f\"âœ… Restauration rÃ©ussie depuis le backup.\")\n","\n","                return data\n","            except Exception as backup_error:\n","                print(f\"âŒ Ã‰chec de la restauration depuis le backup: {str(backup_error)}\")\n","\n","        return None\n","\n","def split_sites_by_percentage(data_pairs, train_percent=0.7, val_percent=0.15, test_percent=0.15):\n","    \"\"\"\n","    Divise les sites en ensembles d'entraÃ®nement, validation et test\n","    selon les pourcentages spÃ©cifiÃ©s.\n","\n","    Args:\n","        data_pairs: Liste des tuples (chm_path, dsm_path, prefix)\n","        train_percent, val_percent, test_percent: Pourcentages pour chaque ensemble\n","\n","    Returns:\n","        Dictionnaire contenant les ensembles d'entraÃ®nement, validation et test\n","    \"\"\"\n","    # VÃ©rification des pourcentages\n","    assert abs(train_percent + val_percent + test_percent - 1.0) < 1e-10, \"Les pourcentages doivent faire 100%\"\n","\n","    # MÃ©langer les donnÃ©es pour une division alÃ©atoire\n","    np.random.seed(42)  # Pour la reproductibilitÃ©\n","    indices = np.random.permutation(len(data_pairs))\n","\n","    # Calculer les indices de sÃ©paration\n","    train_end = int(train_percent * len(data_pairs))\n","    val_end = train_end + int(val_percent * len(data_pairs))\n","\n","    # Diviser les donnÃ©es\n","    train_indices = indices[:train_end]\n","    val_indices = indices[train_end:val_end]\n","    test_indices = indices[val_end:]\n","\n","    # CrÃ©er les sous-ensembles\n","    train_pairs = [data_pairs[i] for i in train_indices]\n","    val_pairs = [data_pairs[i] for i in val_indices]\n","    test_pairs = [data_pairs[i] for i in test_indices]\n","\n","    return {\n","        'train': train_pairs,\n","        'val': val_pairs,\n","        'test': test_pairs\n","    }\n","\n","def update_site_split(existing_split, all_data_pairs, train_percent=0.7, val_percent=0.15, test_percent=0.15):\n","    \"\"\"\n","    Met Ã  jour la division existante avec de nouveaux sites tout en respectant les proportions.\n","\n","    Args:\n","        existing_split: Dictionnaire de la division existante\n","        all_data_pairs: Liste de tous les tuples (chm_path, dsm_path, prefix) actuels\n","        config: Instance de la classe Config\n","\n","    Returns:\n","        Dictionnaire mis Ã  jour avec la nouvelle division\n","    \"\"\"\n","    # Extraire les prÃ©fixes des sites existants\n","    existing_prefixes = set()\n","    for subset in ['train', 'val', 'test']:\n","        existing_prefixes.update(p[2] for p in existing_split[subset])\n","\n","    # Identifier les nouveaux sites\n","    new_sites = [pair for pair in all_data_pairs if pair[2] not in existing_prefixes]\n","\n","    if not new_sites:\n","        print(\"Aucun nouveau site Ã  ajouter Ã  la division existante.\")\n","        return existing_split\n","\n","    print(f\"Ajout de {len(new_sites)} nouveaux sites Ã  la division existante:\")\n","    for pair in new_sites:\n","        print(f\"  - {pair[2]}\")\n","\n","    # Calculer les nombres actuels dans chaque catÃ©gorie\n","    current_counts = {subset: len(existing_split[subset]) for subset in ['train', 'val', 'test']}\n","    current_total = sum(current_counts.values())\n","\n","    # Calculer les nombres cibles aprÃ¨s ajout des nouveaux sites\n","    new_total = current_total + len(new_sites)\n","    target_counts = {\n","        'train': int(new_total * train_percent),\n","        'val': int(new_total * val_percent),\n","        'test': new_total - int(new_total * train_percent) - int(new_total * val_percent)\n","    }\n","\n","    # Calculer combien de sites doivent Ãªtre ajoutÃ©s Ã  chaque catÃ©gorie\n","    to_add = {\n","        subset: max(0, target_counts[subset] - current_counts[subset])\n","        for subset in ['train', 'val', 'test']\n","    }\n","\n","    print(\"RÃ©partition cible:\")\n","    print(f\"  Train: {target_counts['train']} sites ({train_percent*100:.1f}%)\")\n","    print(f\"  Validation: {target_counts['val']} sites ({val_percent*100:.1f}%)\")\n","    print(f\"  Test: {target_counts['test']} sites ({test_percent*100:.1f}%)\")\n","\n","    print(\"Nombre de nouveaux sites Ã  ajouter dans chaque ensemble:\")\n","    print(f\"  Train: +{to_add['train']} sites\")\n","    print(f\"  Validation: +{to_add['val']} sites\")\n","    print(f\"  Test: +{to_add['test']} sites\")\n","\n","    # MÃ©langer les nouveaux sites pour une rÃ©partition alÃ©atoire\n","    np.random.seed(42)  # Pour la reproductibilitÃ©\n","    np.random.shuffle(new_sites)\n","\n","    # RÃ©partir les nouveaux sites\n","    added_count = 0\n","    for subset in ['train', 'val', 'test']:\n","        if added_count >= len(new_sites):\n","            break\n","\n","        # Prendre le nombre requis de sites pour cette catÃ©gorie\n","        sites_to_add = new_sites[added_count:added_count + to_add[subset]]\n","        existing_split[subset].extend(sites_to_add)\n","        added_count += len(sites_to_add)\n","\n","    # Si des sites restent (Ã  cause des arrondis), les ajouter Ã  l'ensemble d'entraÃ®nement\n","    if added_count < len(new_sites):\n","        remaining_sites = new_sites[added_count:]\n","        print(f\"  {len(remaining_sites)} sites supplÃ©mentaires ajoutÃ©s Ã  l'ensemble d'entraÃ®nement (pour Ã©quilibrage)\")\n","        existing_split['train'].extend(remaining_sites)\n","\n","    # Afficher la rÃ©partition finale\n","    print(\"\\nRÃ©partition finale:\")\n","    print(f\"  Train: {len(existing_split['train'])} sites ({len(existing_split['train'])/new_total*100:.1f}%)\")\n","    print(f\"  Validation: {len(existing_split['val'])} sites ({len(existing_split['val'])/new_total*100:.1f}%)\")\n","    print(f\"  Test: {len(existing_split['test'])} sites ({len(existing_split['test'])/new_total*100:.1f}%)\")\n","\n","    return existing_split\n","\n","def update_tile_info(existing_info, site_split, subset_name, output_dir, config):\n","    \"\"\"\n","    Met Ã  jour les informations de tuiles avec les nouveaux sites.\n","\n","    Args:\n","        existing_info: Liste des informations de tuiles existantes\n","        site_split: Dictionnaire de la division des sites\n","        subset_name: Nom du sous-ensemble ('train', 'val', ou 'test')\n","        output_dir: RÃ©pertoire de sortie pour les tuiles\n","        config: Configuration du projet\n","\n","    Returns:\n","        Liste mise Ã  jour des informations de tuiles\n","    \"\"\"\n","    # Extraire les prÃ©fixes des sites dÃ©jÃ  traitÃ©s\n","    existing_prefixes = set(info['site'] for info in existing_info)\n","\n","    # Identifier les sites Ã  traiter\n","    sites_to_process = [pair for pair in site_split[subset_name]\n","                        if pair[2] not in existing_prefixes]\n","\n","    if not sites_to_process:\n","        print(f\"Aucun nouveau site Ã  traiter pour l'ensemble {subset_name}.\")\n","        return existing_info\n","\n","    print(f\"\\nExtraction des tuiles pour {len(sites_to_process)} nouveaux sites dans l'ensemble {subset_name}:\")\n","    for pair in sites_to_process:\n","        print(f\"  - {pair[2]}\")\n","\n","    # Traiter chaque nouveau site\n","    new_info = []\n","    gap_masks_dir = os.path.join(config.PROCESSED_DIR, 'gap_masks')\n","\n","    for chm_path, dsm_path, prefix in sites_to_process:\n","        # Construire les chemins vers les masques\n","        mask_paths = {\n","            threshold: os.path.join(gap_masks_dir, f'{prefix}_gap_mask_{threshold}m.tif')\n","            for threshold in config.THRESHOLDS\n","        }\n","\n","        # Extraire les tuiles pour ce site\n","        site_info = create_dataset_from_site(\n","            dsm_path, mask_paths, config.THRESHOLDS, config.TILE_SIZE,\n","            overlap=config.OVERLAP, output_dir=output_dir, site_prefix=prefix\n","        )\n","\n","        new_info.extend(site_info)\n","\n","    # Combiner les informations existantes et nouvelles\n","    combined_info = existing_info + new_info\n","    print(f\"Nombre total de tuiles pour {subset_name}: {len(combined_info)} \"\n","          f\"({len(existing_info)} existantes + {len(new_info)} nouvelles)\")\n","\n","    # Si c'est l'ensemble d'entraÃ®nement, mettre Ã  jour les indices Ã©quilibrÃ©s\n","    if subset_name == 'train' and len(new_info) > 0:\n","        # Charger les ratios de trouÃ©es\n","        gap_ratios_path = os.path.join(config.PROCESSED_DIR, 'gap_ratios.json')\n","        if os.path.exists(gap_ratios_path):\n","            try:\n","                with open(gap_ratios_path, 'r') as f:\n","                    gap_ratios_data = json.load(f)\n","                    gap_ratios = {int(k): float(v) for k, v in gap_ratios_data.items()}\n","\n","                # Charger les indices existants s'ils existent\n","                train_indices_path = os.path.join(config.PROCESSED_DIR, 'balanced_train_indices.pkl')\n","                if os.path.exists(train_indices_path):\n","                    try:\n","                        with open(train_indices_path, 'rb') as f:\n","                            existing_indices = pickle.load(f)\n","\n","                        # Mettre Ã  jour les indices avec les nouvelles tuiles\n","                        updated_indices = update_balanced_indices(\n","                            existing_indices, new_info, existing_info,\n","                            config.THRESHOLDS, gap_ratios\n","                        )\n","\n","                        # Sauvegarder les indices mis Ã  jour\n","                        with open(train_indices_path, 'wb') as f:\n","                            pickle.dump(updated_indices, f)\n","\n","                        print(f\"Indices Ã©quilibrÃ©s mis Ã  jour: {len(updated_indices)} entrÃ©es\")\n","                    except Exception as e:\n","                        print(f\"âš ï¸ Erreur lors de la mise Ã  jour des indices Ã©quilibrÃ©s: {str(e)}\")\n","                        import traceback\n","                        traceback.print_exc()\n","                else:\n","                    # Si pas d'indices existants mais nouvelles tuiles, crÃ©er des indices\n","                    print(\"CrÃ©ation de nouveaux indices Ã©quilibrÃ©s...\")\n","                    all_indices = create_balanced_indices(combined_info, config.THRESHOLDS, gap_ratios)\n","\n","                    # Sauvegarder les indices\n","                    with open(train_indices_path, 'wb') as f:\n","                        pickle.dump(all_indices, f)\n","\n","                    print(f\"Nouveaux indices Ã©quilibrÃ©s crÃ©Ã©s: {len(all_indices)} entrÃ©es\")\n","            except Exception as e:\n","                print(f\"âš ï¸ Erreur lors de la crÃ©ation/mise Ã  jour des indices Ã©quilibrÃ©s: {str(e)}\")\n","                import traceback\n","                traceback.print_exc()\n","\n","    return combined_info\n","\n","def verify_tiles_integrity(tile_info):\n","    \"\"\"\n","    VÃ©rifie si toutes les tuiles rÃ©fÃ©rencÃ©es dans les mÃ©tadonnÃ©es existent physiquement.\n","\n","    Args:\n","        tile_info: Liste d'informations sur les tuiles\n","\n","    Returns:\n","        Tuple (valide, tuiles_valides, total_tuiles)\n","    \"\"\"\n","    missing = 0\n","    total = len(tile_info)\n","    valid_info = []\n","\n","    for info in tile_info:\n","        is_valid = True\n","\n","        # VÃ©rifier le fichier DSM\n","        if not os.path.isfile(info['dsm_path']):\n","            missing += 1\n","            is_valid = False\n","            continue\n","\n","        # VÃ©rifier les fichiers de masque\n","        for threshold, mask_path in info['mask_paths'].items():\n","            if not os.path.isfile(mask_path):\n","                missing += 1\n","                is_valid = False\n","                break\n","\n","        if is_valid:\n","            valid_info.append(info)\n","\n","    if missing > 0:\n","        print(f\"Attention: {missing}/{total} tuiles manquantes ou corrompues.\")\n","        print(f\"Nombre de tuiles valides: {len(valid_info)}/{total}\")\n","\n","    return len(valid_info) == total, valid_info, total\n","\n","def create_dataset_from_site(dsm_path, mask_paths, thresholds, tile_size, overlap=0.0,\n","                             output_dir=None, site_prefix=None, min_valid_ratio=0.7,\n","                             force_regenerate=False):\n","    \"\"\"\n","    CrÃ©e un ensemble de tuiles Ã  partir d'un site (fichier DSM et masques).\n","    Sauvegarde les tuiles sur disque si un rÃ©pertoire de sortie est spÃ©cifiÃ©.\n","\n","    Args:\n","        dsm_path: Chemin vers le fichier DSM\n","        mask_paths: Dictionnaire des chemins vers les masques par seuil\n","        thresholds: Liste des seuils de hauteur\n","        tile_size: Taille des tuiles en pixels\n","        overlap: Chevauchement entre les tuiles (0.0 = pas de chevauchement, 0.5 = 50% de chevauchement)\n","        output_dir: RÃ©pertoire de sortie pour les tuiles (ou None pour ne pas sauvegarder)\n","        site_prefix: PrÃ©fixe du site pour nommer les fichiers\n","        min_valid_ratio: Ratio minimum de pixels valides pour conserver une tuile\n","        force_regenerate: Si True, rÃ©gÃ©nÃ¨re toutes les tuiles mÃªme si elles existent dÃ©jÃ \n","\n","    Returns:\n","        Liste d'informations sur les tuiles crÃ©Ã©es\n","    \"\"\"\n","    # Ouvrir le fichier DSM pour accÃ©der aux mÃ©tadonnÃ©es\n","    with rasterio.open(dsm_path) as src:\n","        width = src.width\n","        height = src.height\n","\n","    # Calculer le pas entre les tuiles (avec chevauchement)\n","    stride = int(tile_size * (1 - overlap))\n","\n","    # Calculer les positions de dÃ©part des tuiles\n","    row_starts = list(range(0, height - tile_size + 1, stride))\n","    col_starts = list(range(0, width - tile_size + 1, stride))\n","\n","    # Assurer qu'il y a au moins une tuile mÃªme pour les petites images\n","    if not row_starts:\n","        row_starts = [0]\n","    if not col_starts:\n","        col_starts = [0]\n","\n","    # CrÃ©er le rÃ©pertoire de sortie si nÃ©cessaire\n","    if output_dir is not None:\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","    # CrÃ©er un fichier de verrouillage pour indiquer que le traitement est en cours\n","    if output_dir is not None:\n","        lock_file = os.path.join(output_dir, f\"{site_prefix}_processing.lock\")\n","        with open(lock_file, 'w') as f:\n","            f.write(f\"Processing started at: {datetime.now().isoformat()}\\n\")\n","\n","    try:\n","        # Liste pour stocker les informations sur les tuiles\n","        tile_info = []\n","\n","        # Compteurs pour le suivi\n","        total_tiles = len(row_starts) * len(col_starts)\n","        valid_tiles = 0\n","\n","        print(f\"Extraction des tuiles pour {site_prefix} (taille: {tile_size}, chevauchement: {overlap*100:.0f}%)...\")\n","\n","        # Parcourir toutes les positions de dÃ©part\n","        for row_idx, row_start in enumerate(row_starts):\n","            for col_idx, col_start in enumerate(col_starts):\n","                # DÃ©finir la fenÃªtre pour cette tuile\n","                window = Window(col_start, row_start, tile_size, tile_size)\n","\n","                # Chemins des fichiers de tuile\n","                if output_dir is not None:\n","                    dsm_tile_path = os.path.join(output_dir, f\"{site_prefix}_r{row_idx}_c{col_idx}_dsm.npy\")\n","                    mask_tile_paths = {\n","                        threshold: os.path.join(output_dir, f\"{site_prefix}_r{row_idx}_c{col_idx}_mask_{threshold}m.npy\")\n","                        for threshold in thresholds\n","                    }\n","\n","                    # VÃ©rifier si toutes les tuiles existent et sont valides\n","                    all_tiles_exist = not force_regenerate and os.path.isfile(dsm_tile_path)\n","\n","                    if all_tiles_exist:\n","                        for threshold, mask_path in mask_tile_paths.items():\n","                            if not os.path.isfile(mask_path):\n","                                all_tiles_exist = False\n","                                break\n","\n","                        # VÃ©rification supplÃ©mentaire de l'intÃ©gritÃ© des fichiers\n","                        if all_tiles_exist:\n","                            try:\n","                                # Essayer de charger un Ã©chantillon pour vÃ©rifier l'intÃ©gritÃ©\n","                                dsm_sample = np.load(dsm_tile_path)\n","                                if dsm_sample.shape != (tile_size, tile_size):\n","                                    all_tiles_exist = False\n","                                else:\n","                                    # VÃ©rifier un masque au hasard\n","                                    test_threshold = random.choice(thresholds)\n","                                    mask_sample = np.load(mask_tile_paths[test_threshold])\n","                                    if mask_sample.shape != (tile_size, tile_size):\n","                                        all_tiles_exist = False\n","                            except Exception:\n","                                # Si le chargement Ã©choue, le fichier est probablement corrompu\n","                                all_tiles_exist = False\n","                                # Supprimer les fichiers corrompus\n","                                if os.path.exists(dsm_tile_path):\n","                                    os.remove(dsm_tile_path)\n","                                for path in mask_tile_paths.values():\n","                                    if os.path.exists(path):\n","                                        os.remove(path)\n","\n","                    if all_tiles_exist:\n","                        # Ajouter les informations sur cette tuile existante\n","                        tile_info.append({\n","                            'site': site_prefix,\n","                            'row_idx': row_idx,\n","                            'col_idx': col_idx,\n","                            'window': window,\n","                            'dsm_path': dsm_tile_path,\n","                            'mask_paths': mask_tile_paths\n","                        })\n","                        valid_tiles += 1\n","                        continue\n","\n","                # Si les tuiles n'existent pas encore, les extraire\n","                # Extraire la tuile DSM\n","                with rasterio.open(dsm_path) as src:\n","                    dsm_tile = src.read(1, window=window)\n","\n","                # VÃ©rifier si la tuile DSM a suffisamment de donnÃ©es valides\n","                dsm_valid_ratio = np.sum(~np.isnan(dsm_tile)) / (tile_size * tile_size)\n","\n","                if dsm_valid_ratio >= min_valid_ratio:\n","                    # Extraire les tuiles des masques aux mÃªmes positions\n","                    mask_tiles = {}\n","                    all_masks_valid = True\n","\n","                    for threshold in thresholds:\n","                        with rasterio.open(mask_paths[threshold]) as src:\n","                            mask_tile = src.read(1, window=window)\n","\n","                        # VÃ©rifier si la tuile de masque a suffisamment de donnÃ©es valides\n","                        mask_valid_ratio = np.sum(mask_tile != 255) / (tile_size * tile_size)\n","\n","                        if mask_valid_ratio >= min_valid_ratio:\n","                            mask_tiles[threshold] = mask_tile\n","                        else:\n","                            all_masks_valid = False\n","                            break\n","\n","                    # Si toutes les tuiles sont valides, les sauvegarder et ajouter aux informations\n","                    if all_masks_valid:\n","                        if output_dir is not None:\n","                            # Sauvegarder la tuile DSM\n","                            dsm_tile_path = os.path.join(output_dir, f\"{site_prefix}_r{row_idx}_c{col_idx}_dsm.npy\")\n","                            np.save(dsm_tile_path, dsm_tile)\n","\n","                            # Sauvegarder les tuiles des masques\n","                            mask_tile_paths = {}\n","                            for threshold in thresholds:\n","                                mask_tile_path = os.path.join(\n","                                    output_dir,\n","                                    f\"{site_prefix}_r{row_idx}_c{col_idx}_mask_{threshold}m.npy\"\n","                                )\n","                                np.save(mask_tile_path, mask_tiles[threshold])\n","                                mask_tile_paths[threshold] = mask_tile_path\n","                        else:\n","                            # Si pas de sauvegarde, utiliser des chemins None\n","                            dsm_tile_path = None\n","                            mask_tile_paths = {threshold: None for threshold in thresholds}\n","\n","                        # Ajouter les informations sur cette tuile\n","                        tile_info.append({\n","                            'site': site_prefix,\n","                            'row_idx': row_idx,\n","                            'col_idx': col_idx,\n","                            'window': window,\n","                            'dsm_path': dsm_tile_path,\n","                            'mask_paths': mask_tile_paths\n","                        })\n","                        valid_tiles += 1\n","\n","        print(f\"Extraction terminÃ©e: {valid_tiles}/{total_tiles} tuiles valides ({valid_tiles/total_tiles*100:.1f}%)\")\n","\n","        # Supprimer le fichier de verrouillage une fois le traitement terminÃ©\n","        if output_dir is not None and os.path.exists(lock_file):\n","            os.remove(lock_file)\n","\n","        return tile_info\n","\n","    except Exception as e:\n","        # En cas d'erreur, enregistrer l'information et supprimer le verrou\n","        if output_dir is not None:\n","            error_log = os.path.join(output_dir, f\"{site_prefix}_error.log\")\n","            with open(error_log, 'w') as f:\n","                f.write(f\"Error at: {datetime.now().isoformat()}\\n\")\n","                f.write(f\"Error: {str(e)}\\n\")\n","\n","            if os.path.exists(lock_file):\n","                os.remove(lock_file)\n","\n","        raise e  # Relancer l'exception pour la gestion en amont\n","\n","\n","\n","def check_for_interrupted_processing(tiles_dir):\n","    \"\"\"\n","    VÃ©rifie s'il existe des fichiers de verrouillage indiquant un traitement interrompu.\n","\n","    Args:\n","        tiles_dir: RÃ©pertoire contenant les tuiles\n","\n","    Returns:\n","        Liste des prÃ©fixes de sites avec des traitements interrompus\n","    \"\"\"\n","    lock_files = glob.glob(os.path.join(tiles_dir, '*_processing.lock'))\n","    interrupted_sites = []\n","\n","    for lock_file in lock_files:\n","        site_prefix = os.path.basename(lock_file).replace('_processing.lock', '')\n","\n","        # VÃ©rifier l'Ã¢ge du fichier de verrouillage\n","        mod_time = os.path.getmtime(lock_file)\n","        age_hours = (time.time() - mod_time) / 3600\n","\n","        if age_hours > 12:  # Si le fichier a plus de 12 heures\n","            interrupted_sites.append(site_prefix)\n","            print(f\"DÃ©tectÃ© traitement interrompu pour {site_prefix} (il y a {age_hours:.1f} heures)\")\n","\n","    return interrupted_sites\n","\n","def clean_incomplete_tiles(tiles_dir, site_prefix):\n","    \"\"\"\n","    Nettoie les tuiles incomplÃ¨tes pour un site spÃ©cifique.\n","\n","    Args:\n","        tiles_dir: RÃ©pertoire contenant les tuiles\n","        site_prefix: PrÃ©fixe du site\n","    \"\"\"\n","    pattern = os.path.join(tiles_dir, f\"{site_prefix}_r*_c*_*.npy\")\n","    files = glob.glob(pattern)\n","\n","    # Grouper les fichiers par tuile (position row, col)\n","    tile_groups = {}\n","    for file_path in files:\n","        base_name = os.path.basename(file_path)\n","        # Extraire r{row_idx}_c{col_idx}\n","        match = re.search(r'(r\\d+_c\\d+)', base_name)\n","        if match:\n","            key = match.group(1)\n","            if key not in tile_groups:\n","                tile_groups[key] = []\n","            tile_groups[key].append(file_path)\n","\n","    # Pour chaque tuile, vÃ©rifier si tous les fichiers existent\n","    for position, tile_files in tile_groups.items():\n","        # Une tuile complÃ¨te a 1 + len(thresholds) fichiers\n","        # (1 DSM + 1 masque par seuil)\n","        if len(tile_files) != 1 + len(config.THRESHOLDS):\n","            print(f\"Suppression de tuile incomplÃ¨te: {position}\")\n","            for file_path in tile_files:\n","                os.remove(file_path)\n","\n","def process_external_test_data(data_pairs, config):\n","    \"\"\"\n","    Traite les donnÃ©es de test externes.\n","\n","    Args:\n","        data_pairs: Liste des tuples (chm_path, dsm_path, prefix) pour les donnÃ©es externes\n","        config: Configuration du projet\n","\n","    Returns:\n","        Liste d'informations sur les tuiles de test externes\n","    \"\"\"\n","    if not data_pairs:\n","        print(\"Aucune paire de donnÃ©es externe Ã  traiter.\")\n","        return []\n","\n","    print(\"\\nTraitement des donnÃ©es de test externes:\")\n","    for i, (chm_path, dsm_path, prefix) in enumerate(data_pairs):\n","        print(f\"Paire {i+1}: {prefix}\")\n","\n","    # CrÃ©er un rÃ©pertoire pour les masques externes\n","    external_masks_dir = os.path.join(config.PROCESSED_DIR, 'external_gap_masks')\n","    os.makedirs(external_masks_dir, exist_ok=True)\n","\n","    # CrÃ©er un rÃ©pertoire pour les tuiles externes\n","    external_tiles_dir = os.path.join(config.PROCESSED_DIR, 'external_tiles')\n","    os.makedirs(external_tiles_dir, exist_ok=True)\n","\n","    # Traiter chaque paire externe\n","    external_tile_info = []\n","\n","    for chm_path, dsm_path, prefix in data_pairs:\n","        # CrÃ©er les masques de trouÃ©es\n","        mask_paths = {\n","            threshold: os.path.join(external_masks_dir, f'{prefix}_gap_mask_{threshold}m.tif')\n","            for threshold in config.THRESHOLDS\n","        }\n","\n","        # VÃ©rifier si les masques doivent Ãªtre crÃ©Ã©s\n","        if not all(os.path.exists(path) for path in mask_paths.values()):\n","            print(f\"\\nCrÃ©ation des masques de trouÃ©es pour le site externe {prefix}...\")\n","            create_gap_masks(chm_path, config.THRESHOLDS, external_masks_dir, prefix)\n","\n","        # Extraire les tuiles\n","        site_output_dir = os.path.join(external_tiles_dir, prefix)\n","        os.makedirs(site_output_dir, exist_ok=True)\n","\n","        site_info = create_dataset_from_site(\n","            dsm_path, mask_paths, config.THRESHOLDS, config.TILE_SIZE,\n","            overlap=config.OVERLAP, output_dir=site_output_dir, site_prefix=prefix\n","        )\n","\n","        external_tile_info.extend(site_info)\n","\n","    # Sauvegarder les informations sur les tuiles externes\n","    external_info_path = os.path.join(config.PROCESSED_DIR, 'external_tile_info.pkl')\n","    with open(external_info_path, 'wb') as f:\n","        pickle.dump(external_tile_info, f)\n","\n","    print(f\"\\nTraitement des donnÃ©es externes terminÃ©: {len(external_tile_info)} tuiles crÃ©Ã©es.\")\n","    return external_tile_info\n","\n","class GapDataGenerator(Sequence):\n","    \"\"\"\n","    GÃ©nÃ©rateur de donnÃ©es pour l'entraÃ®nement des modÃ¨les.\n","    Prend en entrÃ©e les informations sur les tuiles et gÃ©nÃ¨re des batchs\n","    avec les entrÃ©es (DSM, seuil) et les sorties (masque).\n","    \"\"\"\n","    def __init__(self, tile_info, thresholds, batch_size=32, shuffle=True, augment=False):\n","        \"\"\"\n","        Initialise le gÃ©nÃ©rateur de donnÃ©es.\n","\n","        Args:\n","            tile_info: Liste d'informations sur les tuiles\n","            thresholds: Liste des seuils de hauteur\n","            batch_size: Taille des batchs\n","            shuffle: Si True, mÃ©lange les donnÃ©es Ã  chaque Ã©poque\n","            augment: Si True, applique des augmentations de donnÃ©es Ã  la volÃ©e\n","        \"\"\"\n","        self.tile_info = tile_info\n","        self.thresholds = thresholds\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.augment = augment\n","\n","        # CrÃ©er une liste des indices et des seuils associÃ©s\n","        self.indices = []\n","        for i, info in enumerate(tile_info):\n","            for threshold in thresholds:\n","                self.indices.append((i, threshold))\n","\n","        # MÃ©langer les indices si demandÃ©\n","        if shuffle:\n","            np.random.shuffle(self.indices)\n","\n","    def __len__(self):\n","        \"\"\"Retourne le nombre de batchs par Ã©poque\"\"\"\n","        return int(np.ceil(len(self.indices) / self.batch_size))\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        GÃ©nÃ¨re un batch de donnÃ©es.\n","\n","        Args:\n","            idx: Indice du batch\n","\n","        Returns:\n","            Tuple ([dsm_batch, threshold_batch], mask_batch)\n","        \"\"\"\n","        # Indices pour ce batch\n","        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n","        batch_size = len(batch_indices)\n","\n","        # DÃ©terminer la taille des tuiles Ã  partir de la premiÃ¨re tuile\n","        first_info = self.tile_info[batch_indices[0][0]]\n","        if first_info['dsm_path'] is not None:\n","            dsm_tile = np.load(first_info['dsm_path'])\n","            tile_size = dsm_tile.shape[0]\n","        else:\n","            # Valeur par dÃ©faut si les chemins sont None\n","            tile_size = 256\n","\n","        # Initialiser les arrays pour le batch\n","        dsm_batch = np.zeros((batch_size, tile_size, tile_size, 1), dtype=np.float32)\n","        threshold_batch = np.zeros((batch_size, 1), dtype=np.float32)\n","        mask_batch = np.zeros((batch_size, tile_size, tile_size, 1), dtype=np.float32)\n","\n","        # Remplir le batch\n","        for i, (info_idx, threshold) in enumerate(batch_indices):\n","            info = self.tile_info[info_idx]\n","\n","            # Charger la tuile DSM\n","            if info['dsm_path'] is not None:\n","                dsm_tile = np.load(info['dsm_path'])\n","            else:\n","                # Cas oÃ¹ les tuiles sont passÃ©es directement (sans sauvegarde)\n","                continue\n","\n","            # Charger la tuile de masque\n","            if info['mask_paths'][threshold] is not None:\n","                mask_tile = np.load(info['mask_paths'][threshold])\n","            else:\n","                continue\n","\n","            # Normaliser la tuile DSM\n","            dsm_valid = ~np.isnan(dsm_tile)\n","            if np.any(dsm_valid):\n","                dsm_min = np.nanmin(dsm_tile)\n","                dsm_max = np.nanmax(dsm_tile)\n","                dsm_range = dsm_max - dsm_min\n","                if dsm_range > 0:\n","                    dsm_tile = np.where(dsm_valid, (dsm_tile - dsm_min) / dsm_range, 0)\n","                else:\n","                    dsm_tile = np.where(dsm_valid, 0, 0)\n","\n","            # PrÃ©parer le masque (1=trouÃ©e, 0=non-trouÃ©e, 255=nodata)\n","            mask_valid = (mask_tile != 255)\n","            mask_binary = np.where(mask_valid, (mask_tile > 0).astype(np.float32), 0)\n","\n","            # Appliquer des augmentations si demandÃ©\n","            if self.augment:\n","                # Rotation alÃ©atoire (0, 90, 180 ou 270 degrÃ©s)\n","                k = np.random.randint(0, 4)\n","                if k > 0:\n","                    dsm_tile = np.rot90(dsm_tile, k=k)\n","                    mask_binary = np.rot90(mask_binary, k=k)\n","\n","                # Miroir horizontal alÃ©atoire\n","                if np.random.random() < 0.5:\n","                    dsm_tile = np.fliplr(dsm_tile)\n","                    mask_binary = np.fliplr(mask_binary)\n","\n","                # Miroir vertical alÃ©atoire\n","                if np.random.random() < 0.5:\n","                    dsm_tile = np.flipud(dsm_tile)\n","                    mask_binary = np.flipud(mask_binary)\n","\n","            # Ajouter au batch\n","            dsm_batch[i, :, :, 0] = dsm_tile\n","            threshold_batch[i, 0] = threshold / max(self.thresholds)  # Normaliser entre 0 et 1\n","            mask_batch[i, :, :, 0] = mask_binary\n","\n","        return [dsm_batch, threshold_batch], mask_batch\n","\n","    def on_epoch_end(self):\n","        \"\"\"AppelÃ© Ã  la fin de chaque Ã©poque\"\"\"\n","        if self.shuffle:\n","            np.random.shuffle(self.indices)\n","\n","def create_balanced_indices(tile_info, thresholds, gap_ratios):\n","    \"\"\"\n","    CrÃ©e une liste d'indices Ã©quilibrÃ©e basÃ©e sur les proportions de trouÃ©es.\n","\n","    Args:\n","        tile_info: Liste d'informations sur les tuiles\n","        thresholds: Liste des seuils de hauteur\n","        gap_ratios: Proportions de trouÃ©es par seuil\n","\n","    Returns:\n","        Liste d'indices (tuile_idx, seuil) rÃ©pÃ©tÃ©s selon leur importance\n","    \"\"\"\n","    # Calculer les poids d'Ã©chantillonnage\n","    weight_multiplier = {}\n","    for threshold in thresholds:\n","        ratio = gap_ratios.get(threshold, 0.5)  # Valeur par dÃ©faut si non disponible\n","\n","        # Calculer le facteur de multiplication\n","        if ratio > 0.5:\n","            # Si plus de 50% des pixels sont des trouÃ©es, donner plus de poids aux non-trouÃ©es\n","            weight_multiplier[threshold] = ratio / (1 - ratio + 1e-6)\n","        else:\n","            # Sinon, donner plus de poids aux trouÃ©es\n","            weight_multiplier[threshold] = (1 - ratio) / (ratio + 1e-6)\n","\n","        # Limiter les poids extrÃªmes\n","        weight_multiplier[threshold] = min(weight_multiplier[threshold], 5.0)\n","\n","    # CrÃ©er des groupes d'indices par seuil\n","    indices_by_threshold = {}\n","    for threshold in thresholds:\n","        indices_by_threshold[threshold] = []\n","\n","    # Classer chaque tuile selon son seuil\n","    for i, info in enumerate(tile_info):\n","        for threshold in thresholds:\n","            indices_by_threshold[threshold].append((i, threshold))\n","\n","    # CrÃ©er une liste Ã©quilibrÃ©e en rÃ©pÃ©tant les classes sous-reprÃ©sentÃ©es\n","    balanced_indices = []\n","    for threshold, indices in indices_by_threshold.items():\n","        # DÃ©terminer le nombre de rÃ©pÃ©titions selon le poids\n","        # Convertir en nombre entier avec un minimum de 1\n","        repetitions = max(1, int(round(weight_multiplier[threshold])))\n","        balanced_indices.extend(indices * repetitions)\n","\n","    # MÃ©langer les indices\n","    np.random.seed(42)  # Pour reproductibilitÃ©\n","    np.random.shuffle(balanced_indices)\n","\n","    # Retourner les indices Ã©quilibrÃ©s\n","    return balanced_indices\n","\n","def update_balanced_indices(existing_indices, new_tile_info, old_tile_info, thresholds, gap_ratios):\n","    \"\"\"\n","    Met Ã  jour les indices Ã©quilibrÃ©s avec de nouvelles tuiles.\n","\n","    Args:\n","        existing_indices: Liste d'indices Ã©quilibrÃ©s existante\n","        new_tile_info: Liste des nouvelles informations de tuiles\n","        old_tile_info: Liste des anciennes informations de tuiles\n","        thresholds: Liste des seuils de hauteur\n","        gap_ratios: Proportions de trouÃ©es par seuil\n","\n","    Returns:\n","        Liste mise Ã  jour d'indices Ã©quilibrÃ©s\n","    \"\"\"\n","    # Si pas de nouvelles tuiles, retourner les indices existants\n","    if not new_tile_info:\n","        return existing_indices\n","\n","    # DÃ©calage d'index pour les nouvelles tuiles\n","    offset = len(old_tile_info)\n","\n","    print(f\"Mise Ã  jour des indices Ã©quilibrÃ©s avec {len(new_tile_info)} nouvelles tuiles\")\n","    print(f\"Offset pour les nouveaux indices: {offset}\")\n","\n","    # CrÃ©er des indices pour les nouvelles tuiles uniquement\n","    new_indices = []\n","    for i in range(len(new_tile_info)):\n","        for threshold in thresholds:\n","            new_indices.append((i + offset, threshold))\n","\n","    # Calculer les poids d'Ã©quilibrage\n","    weight_multiplier = {}\n","    for threshold in thresholds:\n","        ratio = gap_ratios.get(threshold, 0.5)\n","        if ratio > 0.5:\n","            weight_multiplier[threshold] = ratio / (1 - ratio + 1e-6)\n","        else:\n","            weight_multiplier[threshold] = (1 - ratio) / (ratio + 1e-6)\n","        weight_multiplier[threshold] = min(weight_multiplier[threshold], 5.0)\n","\n","    # Grouper les nouveaux indices par seuil\n","    new_indices_by_threshold = {threshold: [] for threshold in thresholds}\n","    for idx, threshold in new_indices:\n","        new_indices_by_threshold[threshold].append((idx, threshold))\n","\n","    # Ã‰quilibrer les nouveaux indices\n","    balanced_new_indices = []\n","    for threshold, indices in new_indices_by_threshold.items():\n","        repetitions = max(1, int(round(weight_multiplier[threshold])))\n","        print(f\"  Seuil {threshold}m: {len(indices)} tuiles x {repetitions} rÃ©pÃ©titions = {len(indices) * repetitions} indices\")\n","        balanced_new_indices.extend(indices * repetitions)\n","\n","    # MÃ©langer les nouveaux indices\n","    np.random.seed(42)\n","    np.random.shuffle(balanced_new_indices)\n","\n","    # Combiner avec les indices existants\n","    combined_indices = existing_indices + balanced_new_indices\n","\n","    # Re-mÃ©langer l'ensemble\n","    np.random.shuffle(combined_indices)\n","\n","    return combined_indices\n"]},{"cell_type":"markdown","source":["# PARTIE 2 : EXÃ‰CUTION"],"metadata":{"id":"Hg7m0XMR1f64"}},{"cell_type":"code","source":["# =====================================================================\n","# SECTION 4: PRÃ‰PARATION DES DONNÃ‰ES\n","# =====================================================================\n","\n","def main():\n","    \"\"\"Fonction principale pour la prÃ©paration des donnÃ©es\"\"\"\n","    print(\"=== PRÃ‰PARATION DES DONNÃ‰ES POUR LA DÃ‰TECTION DES TROUÃ‰ES FORESTIÃˆRES ===\\n\")\n","\n","    # Sauvegarder la configuration\n","    config.save_config()\n","\n","    # Trouver les paires CHM/DSM dans le dossier de donnÃ©es principal\n","    print(\"\\n1. RECHERCHE DES PAIRES CHM/DSM\")\n","    print(\"--------------------------------\")\n","    data_pairs = find_chm_dsm_pairs(config.DATA_DIR)\n","    print(f\"Nombre de paires CHM/DSM trouvÃ©es dans le dossier de donnÃ©es: {len(data_pairs)}\")\n","\n","    for i, (chm_path, dsm_path, prefix) in enumerate(data_pairs):\n","        print(f\"Paire {i+1}: {prefix}\")\n","        print(f\"  CHM: {os.path.basename(chm_path)}\")\n","        print(f\"  DSM: {os.path.basename(dsm_path)}\")\n","\n","    # Trouver les paires CHM/DSM dans le dossier de test externe\n","    external_test_pairs = find_chm_dsm_pairs(config.DATA_EXTERNAL_TEST_DIR)\n","    print(f\"\\nNombre de paires CHM/DSM trouvÃ©es dans le dossier de test externe: {len(external_test_pairs)}\")\n","\n","    for i, (chm_path, dsm_path, prefix) in enumerate(external_test_pairs):\n","        print(f\"Paire {i+1}: {prefix}\")\n","        print(f\"  CHM: {os.path.basename(chm_path)}\")\n","        print(f\"  DSM: {os.path.basename(dsm_path)}\")\n","\n","    # VÃ©rifier que les paires de donnÃ©es existent\n","    if len(data_pairs) == 0:\n","        print(\"\\nATTENTION: Aucune paire CHM/DSM trouvÃ©e dans le dossier de donnÃ©es!\")\n","        print(\"Veuillez placer vos fichiers avec suffixes *CHM.tif et *DSM.tif dans le dossier:\")\n","        print(config.DATA_DIR)\n","        return\n","\n","    # Analyse et traitement robuste des paires\n","    print(\"\\n2. ANALYSE ET TRAITEMENT DES PAIRES CHM/DSM\")\n","    print(\"------------------------------------------\")\n","\n","    # CrÃ©er une liste pour stocker les paires traitÃ©es\n","    processed_pairs = []\n","\n","    for chm_path, dsm_path, prefix in data_pairs:\n","        # Traiter la paire de maniÃ¨re robuste\n","        dsm_processed, chm_processed, report, success = process_raster_pair_robustly(\n","            dsm_path, chm_path, prefix, config\n","        )\n","\n","        if success:\n","            processed_pairs.append((chm_processed, dsm_processed, prefix))\n","\n","    print(f\"\\nNombre de paires traitÃ©es avec succÃ¨s: {len(processed_pairs)}/{len(data_pairs)}\")\n","\n","    # Traiter les paires externes sÃ©parÃ©ment\n","    processed_external_pairs = []\n","\n","    if external_test_pairs:\n","        print(\"\\nTraitement des paires de test externes:\")\n","        for chm_path, dsm_path, prefix in external_test_pairs:\n","            dsm_processed, chm_processed, report, success = process_raster_pair_robustly(\n","                dsm_path, chm_path, prefix, config\n","            )\n","\n","            if success:\n","                processed_external_pairs.append((chm_processed, dsm_processed, prefix))\n","\n","        print(f\"Nombre de paires externes traitÃ©es avec succÃ¨s: {len(processed_external_pairs)}/{len(external_test_pairs)}\")\n","\n","    #  CrÃ©ation des masques de trouÃ©es\n","    print(\"\\n3. CRÃ‰ATION DES MASQUES DE TROUÃ‰ES\")\n","    print(\"----------------------------------\")\n","\n","    gap_masks_dir = os.path.join(config.PROCESSED_DIR, 'gap_masks')\n","    gap_masks_by_prefix = {}\n","\n","    for chm_path, dsm_path, prefix in processed_pairs:\n","        mask_paths = create_gap_masks(chm_path, config.THRESHOLDS, gap_masks_dir, prefix)\n","        gap_masks_by_prefix[prefix] = mask_paths\n","\n","    # Calcul ou chargement des statistiques globales de trouÃ©es\n","    gap_stats_path = os.path.join(config.PROCESSED_DIR, 'gap_statistics.json')\n","    force_recalculate_stats = False  # DÃ©finir Ã  True pour forcer le recalcul\n","\n","    # VÃ©rifier si des nouveaux masques ont Ã©tÃ© crÃ©Ã©s dans cette session\n","    new_prefixes = set(gap_masks_by_prefix.keys())\n","    old_prefixes = set()\n","\n","    # Charger les statistiques prÃ©cÃ©dentes pour comparer les prÃ©fixes\n","    if os.path.exists(gap_stats_path) and not force_recalculate_stats:\n","        gap_stats = safe_load_json(gap_stats_path)\n","        if gap_stats and 'prefixes' in gap_stats:\n","            old_prefixes = set(gap_stats['prefixes'])\n","\n","    # Si de nouveaux prÃ©fixes sont prÃ©sents, recalculer les statistiques\n","    new_prefixes_detected = not old_prefixes.issuperset(new_prefixes)\n","\n","    if new_prefixes_detected:\n","        print(f\"Nouveaux prÃ©fixes dÃ©tectÃ©s: {new_prefixes - old_prefixes}\")\n","        print(\"Recalcul des statistiques pour inclure les nouveaux sites...\")\n","        gap_ratios = calculate_and_save_gap_statistics(gap_masks_by_prefix, config)\n","    elif os.path.exists(gap_stats_path) and not force_recalculate_stats:\n","        gap_stats = safe_load_json(gap_stats_path)\n","\n","        if gap_stats and 'gap_ratios' in gap_stats:\n","            print(\"\\nStatistiques globales des trouÃ©es (chargÃ©es):\")\n","            for threshold_str, ratio in gap_stats['gap_ratios'].items():\n","                threshold = int(threshold_str)  # Convertir la clÃ© string en int\n","                print(f\"  Seuil {threshold}m: {float(ratio):.2%} de pixels sont des trouÃ©es\")\n","            gap_ratios = {int(k): float(v) for k, v in gap_stats['gap_ratios'].items()}\n","        else:\n","            print(\"Fichier de statistiques incomplet ou invalide. Recalcul...\")\n","            gap_ratios = calculate_and_save_gap_statistics(gap_masks_by_prefix, config)\n","    else:\n","        print(\"\\nCalcul des statistiques globales des trouÃ©es...\")\n","        gap_ratios = calculate_and_save_gap_statistics(gap_masks_by_prefix, config)\n","\n","    # Sauvegarder les ratios pour utilisation lors de l'entraÃ®nement\n","    # Cette information sera utilisÃ©e par le sampler Ã©quilibrÃ©\n","    gap_ratios_path = os.path.join(config.PROCESSED_DIR, 'gap_ratios.json')\n","    with open(gap_ratios_path, 'w') as f:\n","        json.dump({str(k): float(v) for k, v in gap_ratios.items()}, f, indent=2)\n","\n","    # Division des sites\n","    print(\"\\n4. DIVISION DES SITES\")\n","    print(\"--------------------\")\n","\n","    # VÃ©rifier si la division des sites a dÃ©jÃ  Ã©tÃ© effectuÃ©e\n","    division_info_path = os.path.join(config.PROCESSED_DIR, 'site_split.pkl')\n","\n","    if os.path.isfile(division_info_path):\n","        print(\"Chargement de la division existante des sites...\")\n","        with open(division_info_path, 'rb') as f:\n","            site_split = pickle.load(f)\n","\n","        # Mettre Ã  jour la division avec les nouveaux sites\n","        site_split = update_site_split(\n","            site_split,\n","            processed_pairs,\n","            train_percent=1-(config.TEST_SPLIT+config.VAL_SPLIT),\n","            val_percent=config.VAL_SPLIT,\n","            test_percent=config.TEST_SPLIT\n","        )\n","    else:\n","        print(\"Division des sites en ensembles d'entraÃ®nement, validation et test...\")\n","        site_split = split_sites_by_percentage(\n","            processed_pairs,\n","            train_percent=1-(config.TEST_SPLIT+config.VAL_SPLIT),\n","            val_percent=config.VAL_SPLIT,\n","            test_percent=config.TEST_SPLIT\n","        )\n","\n","    # Sauvegarder la division mise Ã  jour\n","    with open(division_info_path, 'wb') as f:\n","        pickle.dump(site_split, f)\n","\n","    print(f\"\\nRÃ©partition des sites:\")\n","    print(f\"Sites d'entraÃ®nement ({len(site_split['train'])}):\", [p[2] for p in site_split['train']])\n","    print(f\"Sites de validation ({len(site_split['val'])}):\", [p[2] for p in site_split['val']])\n","    print(f\"Sites de test ({len(site_split['test'])}):\", [p[2] for p in site_split['test']])\n","\n","    # Extraction des tuiles\n","    print(\"\\n5. EXTRACTION DES TUILES\")\n","    print(\"----------------------\")\n","\n","    # VÃ©rifier les traitements interrompus\n","    print(\"\\nVÃ©rification des traitements interrompus...\")\n","    interrupted_sites = check_for_interrupted_processing(config.TRAIN_TILES_DIR)\n","    interrupted_sites.extend(check_for_interrupted_processing(config.VAL_TILES_DIR))\n","    interrupted_sites.extend(check_for_interrupted_processing(config.TEST_TILES_DIR))\n","\n","    # Nettoyer les tuiles incomplÃ¨tes si nÃ©cessaire\n","    if interrupted_sites:\n","        print(f\"Nettoyage des tuiles incomplÃ¨tes pour {len(interrupted_sites)} sites...\")\n","        for site in interrupted_sites:\n","            # DÃ©terminer dans quel ensemble se trouve le site\n","            for subset in ['train', 'val', 'test']:\n","                for _, _, prefix in site_split[subset]:\n","                    if prefix == site:\n","                        tiles_dir = getattr(config, f\"{subset.upper()}_TILES_DIR\")\n","                        clean_incomplete_tiles(tiles_dir, site)\n","                        break\n","\n","    # VÃ©rifier si les informations de tuiles existent dÃ©jÃ \n","    train_info_path = os.path.join(config.TILES_DIR, 'train_tile_info.pkl')\n","    val_info_path = os.path.join(config.TILES_DIR, 'val_tile_info.pkl')\n","    test_info_path = os.path.join(config.TILES_DIR, 'test_tile_info.pkl')\n","\n","    # Traiter les tuiles d'entraÃ®nement\n","    print(\"\\nVÃ©rification des tuiles d'entraÃ®nement...\")\n","    if os.path.isfile(train_info_path):\n","        with open(train_info_path, 'rb') as f:\n","            train_tile_info = pickle.load(f)\n","\n","        # VÃ©rifier l'intÃ©gritÃ© des tuiles existantes\n","        is_valid, valid_info, total = verify_tiles_integrity(train_tile_info)\n","\n","        if is_valid:\n","            print(f\"Chargement rÃ©ussi: {len(valid_info)} tuiles valides.\")\n","        else:\n","            print(f\"Certaines tuiles sont manquantes, conservation de {len(valid_info)}/{total} tuiles valides.\")\n","            train_tile_info = valid_info\n","    else:\n","        print(\"Aucune information sur les tuiles d'entraÃ®nement existante.\")\n","        train_tile_info = []\n","\n","    # Mettre Ã  jour les tuiles d'entraÃ®nement avec les nouveaux sites\n","    train_tile_info = update_tile_info(\n","        train_tile_info, site_split, 'train', config.TRAIN_TILES_DIR, config\n","    )\n","\n","    # Sauvegarder les informations mises Ã  jour\n","    with open(train_info_path, 'wb') as f:\n","        pickle.dump(train_tile_info, f)\n","\n","    # Traiter les tuiles de validation\n","    print(\"\\nVÃ©rification des tuiles de validation...\")\n","    if os.path.isfile(val_info_path):\n","        with open(val_info_path, 'rb') as f:\n","            val_tile_info = pickle.load(f)\n","\n","        # VÃ©rifier l'intÃ©gritÃ© des tuiles existantes\n","        is_valid, valid_info, total = verify_tiles_integrity(val_tile_info)\n","\n","        if is_valid:\n","            print(f\"Chargement rÃ©ussi: {len(valid_info)} tuiles valides.\")\n","        else:\n","            print(f\"Certaines tuiles sont manquantes, conservation de {len(valid_info)}/{total} tuiles valides.\")\n","            val_tile_info = valid_info\n","    else:\n","        print(\"Aucune information sur les tuiles de validation existante.\")\n","        val_tile_info = []\n","\n","    # Mettre Ã  jour les tuiles de validation avec les nouveaux sites\n","    val_tile_info = update_tile_info(\n","        val_tile_info, site_split, 'val', config.VAL_TILES_DIR, config\n","    )\n","\n","    # Sauvegarder les informations mises Ã  jour\n","    with open(val_info_path, 'wb') as f:\n","        pickle.dump(val_tile_info, f)\n","\n","    # Traiter les tuiles de test\n","    print(\"\\nVÃ©rification des tuiles de test...\")\n","    if os.path.isfile(test_info_path):\n","        with open(test_info_path, 'rb') as f:\n","            test_tile_info = pickle.load(f)\n","\n","        # VÃ©rifier l'intÃ©gritÃ© des tuiles existantes\n","        is_valid, valid_info, total = verify_tiles_integrity(test_tile_info)\n","\n","        if is_valid:\n","            print(f\"Chargement rÃ©ussi: {len(valid_info)} tuiles valides.\")\n","        else:\n","            print(f\"Certaines tuiles sont manquantes, conservation de {len(valid_info)}/{total} tuiles valides.\")\n","            test_tile_info = valid_info\n","    else:\n","        print(\"Aucune information sur les tuiles de test existante.\")\n","        test_tile_info = []\n","\n","    # Mettre Ã  jour les tuiles de test avec les nouveaux sites\n","    test_tile_info = update_tile_info(\n","        test_tile_info, site_split, 'test', config.TEST_TILES_DIR, config\n","    )\n","\n","    # Sauvegarder les informations mises Ã  jour\n","    with open(test_info_path, 'wb') as f:\n","        pickle.dump(test_tile_info, f)\n","\n","    # Traitement des donnÃ©es de test externes\n","    if processed_external_pairs:\n","        print(\"\\n5b. TRAITEMENT DES DONNÃ‰ES DE TEST EXTERNES\")\n","        print(\"----------------------------------------\")\n","\n","        external_tile_info_path = os.path.join(config.PROCESSED_DIR, 'external_tile_info.pkl')\n","\n","        # VÃ©rifier si des informations sur les tuiles externes existent dÃ©jÃ \n","        if os.path.isfile(external_tile_info_path):\n","            with open(external_tile_info_path, 'rb') as f:\n","                external_tile_info = pickle.load(f)\n","\n","            # VÃ©rifier l'intÃ©gritÃ© des tuiles existantes\n","            is_valid, valid_info, total = verify_tiles_integrity(external_tile_info)\n","\n","            if is_valid:\n","                print(f\"Chargement rÃ©ussi: {len(valid_info)} tuiles externes valides.\")\n","            else:\n","                print(f\"Certaines tuiles externes sont manquantes, conservation de {len(valid_info)}/{total} tuiles valides.\")\n","                external_tile_info = valid_info\n","        else:\n","            print(\"Aucune information sur les tuiles externes existante.\")\n","            external_tile_info = []\n","\n","        # Traiter chaque paire externe\n","        new_external_info = []\n","        external_gap_masks_dir = os.path.join(config.PROCESSED_DIR, 'external_gap_masks')\n","        os.makedirs(external_gap_masks_dir, exist_ok=True)\n","\n","        external_tiles_dir = os.path.join(config.PROCESSED_DIR, 'external_tiles')\n","        os.makedirs(external_tiles_dir, exist_ok=True)\n","\n","        for chm_path, dsm_path, prefix in processed_external_pairs:\n","            # VÃ©rifier si le site est dÃ©jÃ  traitÃ©\n","            if any(info['site'] == prefix for info in external_tile_info):\n","                print(f\"Site externe {prefix} dÃ©jÃ  traitÃ©. IgnorÃ©.\")\n","                continue\n","\n","            # CrÃ©er les masques pour ce site externe\n","            mask_paths = create_gap_masks(chm_path, config.THRESHOLDS, external_gap_masks_dir, prefix)\n","\n","            # Extraire les tuiles pour ce site\n","            site_output_dir = os.path.join(external_tiles_dir, prefix)\n","            os.makedirs(site_output_dir, exist_ok=True)\n","\n","            site_info = create_dataset_from_site(\n","                dsm_path, mask_paths, config.THRESHOLDS, config.TILE_SIZE,\n","                overlap=config.OVERLAP, output_dir=site_output_dir, site_prefix=prefix\n","            )\n","\n","            new_external_info.extend(site_info)\n","\n","        # Mettre Ã  jour les informations sur les tuiles externes\n","        if new_external_info:\n","            external_tile_info.extend(new_external_info)\n","\n","            with open(external_tile_info_path, 'wb') as f:\n","                pickle.dump(external_tile_info, f)\n","\n","            print(f\"Ajout de {len(new_external_info)} nouvelles tuiles externes, total: {len(external_tile_info)}\")\n","\n","    # CrÃ©er les gÃ©nÃ©rateurs de donnÃ©es\n","    print(\"\\n6. CRÃ‰ATION DES GÃ‰NÃ‰RATEURS DE DONNÃ‰ES\")\n","    print(\"-------------------------------------\")\n","\n","    train_generator = GapDataGenerator(train_tile_info, config.THRESHOLDS, batch_size=config.BATCH_SIZE, shuffle=True, augment=True)\n","    val_generator = GapDataGenerator(val_tile_info, config.THRESHOLDS, batch_size=config.BATCH_SIZE, shuffle=False)\n","    test_generator = GapDataGenerator(test_tile_info, config.THRESHOLDS, batch_size=config.BATCH_SIZE, shuffle=False)\n","\n","\n","    print(f\"\\nStatistiques finales:\")\n","    print(f\"  Nombre de tuiles d'entraÃ®nement: {len(train_tile_info)} ({len(train_generator)} batchs de taille {config.BATCH_SIZE})\")\n","    print(f\"  Nombre de tuiles de validation: {len(val_tile_info)} ({len(val_generator)} batchs)\")\n","    print(f\"  Nombre de tuiles de test: {len(test_tile_info)} ({len(test_generator)} batchs)\")\n","    print(f\"  Nombre d'Ã©chantillons d'entraÃ®nement: {len(train_tile_info) * len(config.THRESHOLDS)} (en comptant tous les seuils)\")\n","    print(f\"  Augmentation Ã  la volÃ©e activÃ©e pour l'entraÃ®nement\")\n","    print(f\"  Total d'Ã©chantillons sur disque: {len(train_tile_info) + len(val_tile_info) + len(test_tile_info)} tuiles\")\n","\n","    print(\"\\n=== PRÃ‰PARATION DES DONNÃ‰ES TERMINÃ‰E ===\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8suiCl2VeIOp","executionInfo":{"status":"ok","timestamp":1741083323396,"user_tz":-60,"elapsed":35876,"user":{"displayName":"Arthur Vander Linden","userId":"07794870641930557243"}},"outputId":"c91f2308-e943-4ccb-92f7-446971d7ea13"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["=== PRÃ‰PARATION DES DONNÃ‰ES POUR LA DÃ‰TECTION DES TROUÃ‰ES FORESTIÃˆRES ===\n","\n","Configuration sauvegardÃ©e dans /content/drive/MyDrive/ForestGaps_DeepLearning_Workflow/processed/config.json\n","\n","1. RECHERCHE DES PAIRES CHM/DSM\n","--------------------------------\n","Nombre de paires CHM/DSM trouvÃ©es dans le dossier de donnÃ©es: 20\n","Paire 1: UTM33S_Plot137\n","  CHM: UTM33S_Plot137_CHM.tif\n","  DSM: UTM33S_Plot137_DSM.tif\n","Paire 2: UTM33S_Plot26\n","  CHM: UTM33S_Plot26_CHM.tif\n","  DSM: UTM33S_Plot26_DSM.tif\n","Paire 3: UTM34N_Plot119\n","  CHM: UTM34N_Plot119_CHM.tif\n","  DSM: UTM34N_Plot119_DSM.tif\n","Paire 4: UTM34N_Plot162\n","  CHM: UTM34N_Plot162_CHM.tif\n","  DSM: UTM34N_Plot162_DSM.tif\n","Paire 5: UTM34N_Plot192\n","  CHM: UTM34N_Plot192_CHM.tif\n","  DSM: UTM34N_Plot192_DSM.tif\n","Paire 6: UTM34N_Plot80\n","  CHM: UTM34N_Plot80_CHM.tif\n","  DSM: UTM34N_Plot80_DSM.tif\n","Paire 7: UTM34N_Plot94\n","  CHM: UTM34N_Plot94_CHM.tif\n","  DSM: UTM34N_Plot94_DSM.tif\n","Paire 8: UTM34S_Plot1\n","  CHM: UTM34S_Plot1_CHM.tif\n","  DSM: UTM34S_Plot1_DSM.tif\n","Paire 9: UTM34S_Plot181\n","  CHM: UTM34S_Plot181_CHM.tif\n","  DSM: UTM34S_Plot181_DSM.tif\n","Paire 10: UTM34S_Plot19\n","  CHM: UTM34S_Plot19_CHM.tif\n","  DSM: UTM34S_Plot19_DSM.tif\n","Paire 11: UTM34S_Plot55\n","  CHM: UTM34S_Plot55_CHM.tif\n","  DSM: UTM34S_Plot55_DSM.tif\n","Paire 12: UTM35N_Plot145\n","  CHM: UTM35N_Plot145_CHM.tif\n","  DSM: UTM35N_Plot145_DSM.tif\n","Paire 13: UTM35N_Plot180\n","  CHM: UTM35N_Plot180_CHM.tif\n","  DSM: UTM35N_Plot180_DSM.tif\n","Paire 14: UTM35N_Plot86\n","  CHM: UTM35N_Plot86_CHM.tif\n","  DSM: UTM35N_Plot86_DSM.tif\n","Paire 15: UTM35N_Plot88\n","  CHM: UTM35N_Plot88_CHM.tif\n","  DSM: UTM35N_Plot88_DSM.tif\n","Paire 16: UTM35N_Plot99\n","  CHM: UTM35N_Plot99_CHM.tif\n","  DSM: UTM35N_Plot99_DSM.tif\n","Paire 17: UTM35S_Plot12\n","  CHM: UTM35S_Plot12_CHM.tif\n","  DSM: UTM35S_Plot12_DSM.tif\n","Paire 18: UTM35S_Plot196\n","  CHM: UTM35S_Plot196_CHM.tif\n","  DSM: UTM35S_Plot196_DSM.tif\n","Paire 19: UTM35S_Plot76\n","  CHM: UTM35S_Plot76_CHM.tif\n","  DSM: UTM35S_Plot76_DSM.tif\n","Paire 20: UTM35S_Plot79\n","  CHM: UTM35S_Plot79_CHM.tif\n","  DSM: UTM35S_Plot79_DSM.tif\n","\n","Nombre de paires CHM/DSM trouvÃ©es dans le dossier de test externe: 1\n","Paire 1: SODEFOR_Mini2\n","  CHM: SODEFOR_Mini2_CHM.tif\n","  DSM: SODEFOR_Mini2_DSM.tif\n","\n","2. ANALYSE ET TRAITEMENT DES PAIRES CHM/DSM\n","------------------------------------------\n","Analyse de la paire: UTM33S_Plot137\n","Utilisation de l'analyse existante pour UTM33S_Plot137\n","Analyse de la paire: UTM33S_Plot26\n","Utilisation de l'analyse existante pour UTM33S_Plot26\n","Analyse de la paire: UTM34N_Plot119\n","Utilisation de l'analyse existante pour UTM34N_Plot119\n","Analyse de la paire: UTM34N_Plot162\n","Utilisation de l'analyse existante pour UTM34N_Plot162\n","Analyse de la paire: UTM34N_Plot192\n","Utilisation de l'analyse existante pour UTM34N_Plot192\n","Analyse de la paire: UTM34N_Plot80\n","Utilisation de l'analyse existante pour UTM34N_Plot80\n","Analyse de la paire: UTM34N_Plot94\n","Utilisation de l'analyse existante pour UTM34N_Plot94\n","Analyse de la paire: UTM34S_Plot1\n","Utilisation de l'analyse existante pour UTM34S_Plot1\n","Analyse de la paire: UTM34S_Plot181\n","Utilisation de l'analyse existante pour UTM34S_Plot181\n","Analyse de la paire: UTM34S_Plot19\n","Utilisation de l'analyse existante pour UTM34S_Plot19\n","Analyse de la paire: UTM34S_Plot55\n","Utilisation de l'analyse existante pour UTM34S_Plot55\n","Analyse de la paire: UTM35N_Plot145\n","Utilisation de l'analyse existante pour UTM35N_Plot145\n","Analyse de la paire: UTM35N_Plot180\n","Utilisation de l'analyse existante pour UTM35N_Plot180\n","Analyse de la paire: UTM35N_Plot86\n","Utilisation de l'analyse existante pour UTM35N_Plot86\n","Analyse de la paire: UTM35N_Plot88\n","Utilisation de l'analyse existante pour UTM35N_Plot88\n","Analyse de la paire: UTM35N_Plot99\n","Utilisation de l'analyse existante pour UTM35N_Plot99\n","Analyse de la paire: UTM35S_Plot12\n","Utilisation de l'analyse existante pour UTM35S_Plot12\n","Analyse de la paire: UTM35S_Plot196\n","Utilisation de l'analyse existante pour UTM35S_Plot196\n","Analyse de la paire: UTM35S_Plot76\n","Utilisation de l'analyse existante pour UTM35S_Plot76\n","Analyse de la paire: UTM35S_Plot79\n","Utilisation de l'analyse existante pour UTM35S_Plot79\n","\n","Nombre de paires traitÃ©es avec succÃ¨s: 20/20\n","\n","Traitement des paires de test externes:\n","Analyse de la paire: SODEFOR_Mini2\n","Utilisation de l'analyse existante pour SODEFOR_Mini2\n","Nombre de paires externes traitÃ©es avec succÃ¨s: 1/1\n","\n","3. CRÃ‰ATION DES MASQUES DE TROUÃ‰ES\n","----------------------------------\n","Masque pour le seuil 10m - site UTM33S_Plot137 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM33S_Plot137 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM33S_Plot137 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM33S_Plot137 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM33S_Plot137 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM33S_Plot26 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM33S_Plot26 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM33S_Plot26 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM33S_Plot26 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM33S_Plot26 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34N_Plot119 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34N_Plot119 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34N_Plot119 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34N_Plot119 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34N_Plot119 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34N_Plot162 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34N_Plot162 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34N_Plot162 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34N_Plot162 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34N_Plot162 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34N_Plot192 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34N_Plot192 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34N_Plot192 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34N_Plot192 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34N_Plot192 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34N_Plot80 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34N_Plot80 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34N_Plot80 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34N_Plot80 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34N_Plot80 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34N_Plot94 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34N_Plot94 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34N_Plot94 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34N_Plot94 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34N_Plot94 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34S_Plot1 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34S_Plot1 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34S_Plot1 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34S_Plot1 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34S_Plot1 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34S_Plot181 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34S_Plot181 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34S_Plot181 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34S_Plot181 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34S_Plot181 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34S_Plot19 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34S_Plot19 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34S_Plot19 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34S_Plot19 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34S_Plot19 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM34S_Plot55 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM34S_Plot55 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM34S_Plot55 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM34S_Plot55 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM34S_Plot55 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35N_Plot145 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35N_Plot145 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35N_Plot145 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35N_Plot145 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35N_Plot145 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35N_Plot180 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35N_Plot180 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35N_Plot180 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35N_Plot180 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35N_Plot180 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35N_Plot86 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35N_Plot86 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35N_Plot86 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35N_Plot86 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35N_Plot86 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35N_Plot88 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35N_Plot88 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35N_Plot88 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35N_Plot88 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35N_Plot88 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35N_Plot99 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35N_Plot99 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35N_Plot99 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35N_Plot99 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35N_Plot99 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35S_Plot12 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35S_Plot12 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35S_Plot12 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35S_Plot12 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35S_Plot12 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35S_Plot196 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35S_Plot196 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35S_Plot196 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35S_Plot196 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35S_Plot196 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35S_Plot76 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35S_Plot76 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35S_Plot76 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35S_Plot76 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35S_Plot76 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 10m - site UTM35S_Plot79 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 15m - site UTM35S_Plot79 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 20m - site UTM35S_Plot79 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 25m - site UTM35S_Plot79 dÃ©jÃ  existant. Utilisation du fichier existant.\n","Masque pour le seuil 30m - site UTM35S_Plot79 dÃ©jÃ  existant. Utilisation du fichier existant.\n","\n","Statistiques globales des trouÃ©es (chargÃ©es):\n","  Seuil 10m: 26.86% de pixels sont des trouÃ©es\n","  Seuil 15m: 35.61% de pixels sont des trouÃ©es\n","  Seuil 20m: 48.57% de pixels sont des trouÃ©es\n","  Seuil 25m: 64.29% de pixels sont des trouÃ©es\n","  Seuil 30m: 80.42% de pixels sont des trouÃ©es\n","\n","4. DIVISION DES SITES\n","--------------------\n","Chargement de la division existante des sites...\n","Aucun nouveau site Ã  ajouter Ã  la division existante.\n","\n","RÃ©partition des sites:\n","Sites d'entraÃ®nement (14): ['UTM33S_Plot137', 'UTM35S_Plot196', 'UTM35N_Plot99', 'UTM33S_Plot26', 'UTM34S_Plot181', 'UTM34N_Plot80', 'UTM35N_Plot145', 'UTM34N_Plot162', 'UTM35S_Plot76', 'UTM35S_Plot12', 'UTM35N_Plot86', 'UTM34N_Plot119', 'UTM34S_Plot19', 'UTM35S_Plot79']\n","Sites de validation (3): ['UTM34N_Plot192', 'UTM35N_Plot180', 'UTM34S_Plot1']\n","Sites de test (3): ['UTM34S_Plot55', 'UTM35N_Plot88', 'UTM34N_Plot94']\n","\n","5. EXTRACTION DES TUILES\n","----------------------\n","\n","VÃ©rification des traitements interrompus...\n","\n","VÃ©rification des tuiles d'entraÃ®nement...\n","Chargement rÃ©ussi: 3726 tuiles valides.\n","Aucun nouveau site Ã  traiter pour l'ensemble train.\n","\n","VÃ©rification des tuiles de validation...\n","Chargement rÃ©ussi: 792 tuiles valides.\n","Aucun nouveau site Ã  traiter pour l'ensemble val.\n","\n","VÃ©rification des tuiles de test...\n","Chargement rÃ©ussi: 811 tuiles valides.\n","Aucun nouveau site Ã  traiter pour l'ensemble test.\n","\n","5b. TRAITEMENT DES DONNÃ‰ES DE TEST EXTERNES\n","----------------------------------------\n","Chargement rÃ©ussi: 135 tuiles externes valides.\n","Site externe SODEFOR_Mini2 dÃ©jÃ  traitÃ©. IgnorÃ©.\n","\n","6. CRÃ‰ATION DES GÃ‰NÃ‰RATEURS DE DONNÃ‰ES\n","-------------------------------------\n","\n","Statistiques finales:\n","  Nombre de tuiles d'entraÃ®nement: 3726 (583 batchs de taille 32)\n","  Nombre de tuiles de validation: 792 (124 batchs)\n","  Nombre de tuiles de test: 811 (127 batchs)\n","  Nombre d'Ã©chantillons d'entraÃ®nement: 18630 (en comptant tous les seuils)\n","  Augmentation Ã  la volÃ©e activÃ©e pour l'entraÃ®nement\n","  Total d'Ã©chantillons sur disque: 5329 tuiles\n","\n","=== PRÃ‰PARATION DES DONNÃ‰ES TERMINÃ‰E ===\n"]}]}]}