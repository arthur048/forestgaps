# Guide d'organisation pour le Benchmarking ForestGaps

## ğŸ“‹ Vue d'ensemble

Ce guide dÃ©crit l'organisation des fichiers, logs et outputs pour le benchmarking des modÃ¨les de dÃ©tection de trouÃ©es forestiÃ¨res selon les meilleures pratiques du deep learning.

## ğŸ—‚ï¸ Structure des rÃ©pertoires

```
forestgaps-dl/
â”œâ”€â”€ data/                              # DonnÃ©es d'entraÃ®nement
â”‚   â”œâ”€â”€ UTM33S_Plot137_{DSM,CHM}.tif
â”‚   â”œâ”€â”€ UTM34N_Plot119_{DSM,CHM}.tif
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ data_external_test/            # DonnÃ©es de test indÃ©pendantes
â”‚       â”œâ”€â”€ SODEFOR_Mini2_DSM.tif
â”‚       â””â”€â”€ SODEFOR_Mini2_CHM.tif
â”‚
â”œâ”€â”€ outputs/                           # Tous les outputs d'expÃ©riences
â”‚   â””â”€â”€ benchmarks/                    # Benchmarks des modÃ¨les
â”‚       â””â”€â”€ YYYYMMDD_HHMMSS_<name>/   # Timestamp + nom de l'expÃ©rience
â”‚           â”œâ”€â”€ config.yaml           # Configuration complÃ¨te utilisÃ©e
â”‚           â”œâ”€â”€ benchmark_results.json # RÃ©sultats agrÃ©gÃ©s
â”‚           â”œâ”€â”€ models/               # Checkpoints par modÃ¨le
â”‚           â”‚   â”œâ”€â”€ UNet_Base/
â”‚           â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚           â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt
â”‚           â”‚   â”‚   â”‚   â”œâ”€â”€ epoch_10.pt
â”‚           â”‚   â”‚   â”‚   â””â”€â”€ last.pt
â”‚           â”‚   â”‚   â”œâ”€â”€ metrics.json
â”‚           â”‚   â”‚   â”œâ”€â”€ model_config.json
â”‚           â”‚   â”‚   â””â”€â”€ prediction_examples/
â”‚           â”‚   â”‚       â”œâ”€â”€ example_0.npy
â”‚           â”‚   â”‚       â””â”€â”€ ...
â”‚           â”‚   â”œâ”€â”€ UNet_FiLM/
â”‚           â”‚   â””â”€â”€ DeepLabV3+/
â”‚           â”œâ”€â”€ visualizations/       # Graphiques comparatifs
â”‚           â”‚   â”œâ”€â”€ metric_comparison_iou.png
â”‚           â”‚   â”œâ”€â”€ threshold_comparison_iou.png
â”‚           â”‚   â”œâ”€â”€ training_curves_iou.png
â”‚           â”‚   â”œâ”€â”€ training_time_comparison.png
â”‚           â”‚   â”œâ”€â”€ convergence_speed_iou.png
â”‚           â”‚   â””â”€â”€ radar_chart.png
â”‚           â””â”€â”€ reports/              # Rapports dÃ©taillÃ©s
â”‚               â”œâ”€â”€ benchmark_report.html
â”‚               â”œâ”€â”€ benchmark_report.md
â”‚               â””â”€â”€ benchmark_report.txt
â”‚
â”œâ”€â”€ logs/                              # Logs TensorBoard et training
â”‚   â””â”€â”€ benchmarks/                    # Logs de benchmarks
â”‚       â””â”€â”€ YYYYMMDD_HHMMSS_<name>/   # Correspondance avec outputs
â”‚           â”œâ”€â”€ UNet_Base/
â”‚           â”‚   â”œâ”€â”€ train/
â”‚           â”‚   â”œâ”€â”€ val/
â”‚           â”‚   â””â”€â”€ test/
â”‚           â”œâ”€â”€ UNet_FiLM/
â”‚           â””â”€â”€ DeepLabV3+/
â”‚
â”œâ”€â”€ models/                            # ModÃ¨les finaux sauvegardÃ©s
â”‚   â”œâ”€â”€ production/                    # ModÃ¨les en production
â”‚   â”‚   â”œâ”€â”€ best_unet_film_v1.pt
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â””â”€â”€ archive/                       # Anciens modÃ¨les archivÃ©s
â”‚       â””â”€â”€ YYYYMMDD/
â”‚
â”œâ”€â”€ examples/                          # Scripts d'exemples
â”‚   â”œâ”€â”€ run_benchmark.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ scripts/                           # Scripts utilitaires
    â”œâ”€â”€ benchmark_quick_test.py        # Test rapide
    â””â”€â”€ benchmark_full.py              # Benchmark complet
```

## ğŸš€ Workflow de benchmarking

### 1. PrÃ©paration des donnÃ©es

```bash
# VÃ©rifier la structure des donnÃ©es
ls -lh data/*.tif | head -10
ls -lh data/data_external_test/*.tif
```

**DonnÃ©es disponibles :**
- **Training/Val/Test** : Plots UTM33S et UTM34N dans `data/`
- **Ã‰valuation externe** : SODEFOR_Mini2 dans `data/data_external_test/`

### 2. Lancement du Docker avec TensorBoard

```bash
cd docker/
docker-compose up -d tensorboard

# VÃ©rifier que TensorBoard tourne
# AccÃ¨s via : http://localhost:6006
```

### 3. ExÃ©cution d'un benchmark

#### Option A : Test rapide (recommandÃ© pour dÃ©buter)

```bash
# Dans le container Docker
docker-compose run --rm forestgaps python scripts/benchmark_quick_test.py \
  --experiment-name "test_3models" \
  --epochs 5 \
  --quick-mode
```

#### Option B : Benchmark complet

```bash
# Dans le container Docker
docker-compose run --rm forestgaps python scripts/benchmark_full.py \
  --experiment-name "comparison_all_models" \
  --epochs 50 \
  --batch-size 8 \
  --thresholds 2.0 5.0 10.0 15.0
```

### 4. Surveillance de l'entraÃ®nement

#### Via TensorBoard
```
Ouvrir : http://localhost:6006
- Logs en temps rÃ©el
- Courbes de mÃ©triques
- Comparaison entre modÃ¨les
```

#### Via logs Docker
```bash
# Suivre les logs
docker-compose logs -f forestgaps

# Logs d'un benchmark spÃ©cifique
tail -f logs/benchmarks/20241203_105530_test_3models/UNet_Base/train.log
```

### 5. Analyse des rÃ©sultats

```bash
# Lister les benchmarks disponibles
ls -lhtr outputs/benchmarks/

# Examiner les rÃ©sultats d'un benchmark
cd outputs/benchmarks/20241203_105530_test_3models/

# Voir les rÃ©sultats agrÃ©gÃ©s
cat benchmark_results.json | jq '.summary'

# Voir le rapport HTML
firefox reports/benchmark_report.html  # ou chrome, edge, etc.
```

## ğŸ“Š Convention de nommage

### ExpÃ©riences
```
YYYYMMDD_HHMMSS_<experiment_name>
Exemple : 20241203_105530_comparison_all_models
```

### ModÃ¨les
```
<architecture>_<variant>
Exemples :
  - UNet_Base
  - UNet_FiLM
  - DeepLabV3+_Base
  - DeepLabV3+_Threshold
```

### Checkpoints
```
best.pt              # Meilleur modÃ¨le (selon val_iou)
last.pt              # Dernier checkpoint
epoch_<N>.pt         # Checkpoint Ã  l'Ã©poque N
```

## ğŸ” MÃ©triques suivies

### MÃ©triques principales
- **IoU** (Intersection over Union) : MÃ©trique principale de segmentation
- **F1-Score** : Harmonic mean de prÃ©cision et recall
- **Precision** : Taux de vrais positifs
- **Recall** : Taux de dÃ©tection

### MÃ©triques secondaires
- **Training time** : Temps d'entraÃ®nement total
- **Convergence speed** : Nombre d'Ã©poques pour atteindre 90% de la meilleure performance
- **Inference time** : Temps de prÃ©diction (Ã  ajouter)

### Seuils de hauteur analysÃ©s
- **2.0m** : Petites trouÃ©es
- **5.0m** : TrouÃ©es moyennes
- **10.0m** : Grandes trouÃ©es
- **15.0m** : TrÃ¨s grandes trouÃ©es

## ğŸ› Debugging et monitoring

### VÃ©rifier l'utilisation GPU
```bash
# Dans le container
docker-compose exec forestgaps nvidia-smi

# En continu
docker-compose exec forestgaps watch -n 1 nvidia-smi
```

### VÃ©rifier la mÃ©moire
```bash
docker stats forestgaps-main
```

### Logs d'erreurs
```bash
# Erreurs Python
docker-compose logs forestgaps | grep -i error

# Erreurs CUDA
docker-compose logs forestgaps | grep -i cuda
```

## ğŸ“ˆ Bonnes pratiques

### 1. **Toujours nommer ses expÃ©riences**
```python
# BON
benchmark = ModelComparison(..., output_dir="outputs/benchmarks/20241203_comparison_film_variants")

# MAUVAIS
benchmark = ModelComparison(...)  # Nom auto-gÃ©nÃ©rÃ© illisible
```

### 2. **Sauvegarder la configuration**
- La config complÃ¨te est automatiquement sauvegardÃ©e dans `config.yaml`
- Permet de reproduire exactement l'expÃ©rience

### 3. **Utiliser le mode quick pour tester**
```python
# Test rapide avant un long entraÃ®nement
config.data.max_train_tiles = 20
config.data.max_val_tiles = 5
config.training.epochs = 5
```

### 4. **Suivre l'entraÃ®nement en temps rÃ©el**
- TensorBoard : mÃ©triques et courbes
- Docker logs : progression dÃ©taillÃ©e
- `outputs/<experiment>/models/<model>/metrics.json` : mÃ©triques finales

### 5. **Ã‰valuation externe systÃ©matique**
AprÃ¨s chaque benchmark, Ã©valuer sur les donnÃ©es externes :
```python
from forestgaps.evaluation import ExternalEvaluator

evaluator = ExternalEvaluator(
    model_path="outputs/benchmarks/.../models/UNet_FiLM/checkpoints/best.pt"
)

results = evaluator.evaluate(
    dsm_path="data/data_external_test/SODEFER_Mini2_DSM.tif",
    chm_path="data/data_external_test/SODEFER_Mini2_CHM.tif",
    output_dir="outputs/external_eval/UNet_FiLM",
    visualize=True
)
```

## ğŸ”„ Workflow complet recommandÃ©

### Phase 1 : Test rapide
1. Lancer `benchmark_quick_test.py` avec 2-3 modÃ¨les
2. VÃ©rifier que tout fonctionne (5-10 min)
3. Analyser les premiers rÃ©sultats

### Phase 2 : Benchmark complet
1. Lancer `benchmark_full.py` avec tous les modÃ¨les (plusieurs heures)
2. Surveiller via TensorBoard
3. Sauvegarder les rÃ©sultats

### Phase 3 : Ã‰valuation externe
1. Ã‰valuer le meilleur modÃ¨le sur donnÃ©es externes
2. GÃ©nÃ©rer les visualisations
3. CrÃ©er le rapport final

### Phase 4 : Production
1. Copier le meilleur modÃ¨le dans `models/production/`
2. Documenter les performances dans `metadata.json`
3. Archiver l'expÃ©rience complÃ¨te

## ğŸ“ Notes importantes

- **Logs TensorBoard** : Partagent le mÃªme timestamp que outputs
- **Auto-cleanup** : Les checkpoints intermÃ©diaires peuvent Ãªtre nettoyÃ©s manuellement
- **ReproductibilitÃ©** : Seed fixÃ© dans config pour rÃ©sultats reproductibles
- **Backup** : Sauvegarder rÃ©guliÃ¨rement `outputs/` et `models/production/`

## ğŸ†˜ Troubleshooting

### ProblÃ¨me : Out of memory
```python
# RÃ©duire batch_size
config.training.batch_size = 4  # au lieu de 8

# RÃ©duire taille des features
model_params["init_features"] = 16  # au lieu de 32
```

### ProblÃ¨me : TensorBoard ne s'affiche pas
```bash
# RedÃ©marrer le service
docker-compose restart tensorboard

# VÃ©rifier les logs
docker-compose logs tensorboard
```

### ProblÃ¨me : Checkpoint corrompu
```python
# Charger le dernier checkpoint valide
trainer.load_checkpoint("outputs/.../models/<model>/checkpoints/epoch_N.pt")
```
