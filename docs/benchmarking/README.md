# Guide Benchmarking ForestGaps

Guide unique et complet pour lancer des benchmarks de modÃ¨les.

## ğŸ“‹ Vue d'ensemble

Le systÃ¨me de benchmarking permet de comparer automatiquement plusieurs architectures de deep learning (U-Net, DeepLabV3+) sur la dÃ©tection de trouÃ©es forestiÃ¨res.

**Outputs automatiques :**
- MÃ©triques dÃ©taillÃ©es (IoU, F1, Precision, Recall)
- Visualisations comparatives (PNG)
- Rapports HTML/Markdown
- Logs TensorBoard temps rÃ©el
- Sauvegarde meilleurs modÃ¨les

## ğŸš€ DÃ©marrage rapide

### 1. Lancer Docker
```bash
cd docker/
docker-compose up -d
```

### 2. Entrer dans le container
```bash
docker exec -it forestgaps-main bash
```

Tu verras : `root@xxxxxx:/app#`

### 3. Test rapide (5-10 min)
```bash
python scripts/benchmark_quick_test.py --experiment-name "test_$(date +%Y%m%d)"
```

### 4. Voir les rÃ©sultats
```bash
# Dans le container
ls -lhtr outputs/benchmarks/

# Depuis Windows : ouvrir
# outputs\benchmarks\<timestamp>_test\reports\benchmark_report.html
```

## ğŸ“Š Scripts disponibles

### `benchmark_quick_test.py` - Test rapide
```bash
python scripts/benchmark_quick_test.py \
  --experiment-name "mon_test" \
  --epochs 5 \
  --models "unet,unet_film" \
  --batch-size 4
```

**Usage :** Valider le setup, tester une config
**DurÃ©e :** 5-10 minutes

### `benchmark_full.py` - Benchmark complet
```bash
python scripts/benchmark_full.py \
  --experiment-name "production_v1" \
  --epochs 50 \
  --models "unet,unet_film,deeplabv3_plus,deeplabv3_plus_threshold" \
  --batch-size 8
```

**Usage :** Comparaison complÃ¨te pour publication
**DurÃ©e :** 4-8 heures

## ğŸ¯ ModÃ¨les disponibles

| ModÃ¨le | Code | Description |
|--------|------|-------------|
| U-Net Base | `unet` | Architecture classique |
| U-Net FiLM | `unet_film` | Avec FiLM conditioning |
| DeepLabV3+ | `deeplabv3_plus` | State-of-the-art seg |
| DeepLabV3+ Threshold | `deeplabv3_plus_threshold` | Avec seuil encoding |

## ğŸ“ Structure des outputs

```
outputs/benchmarks/20241203_105530_mon_test/
â”œâ”€â”€ benchmark_results.json         # RÃ©sultats agrÃ©gÃ©s
â”œâ”€â”€ best_model.pt                  # Meilleur modÃ¨le
â”œâ”€â”€ config.yaml                    # Configuration
â”œâ”€â”€ models/                        # Par modÃ¨le
â”‚   â”œâ”€â”€ UNet_Base/
â”‚   â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ best.pt           â­ Utiliser celui-ci
â”‚   â”‚   â”‚   â””â”€â”€ last.pt
â”‚   â”‚   â””â”€â”€ metrics.json
â”‚   â””â”€â”€ UNet_FiLM/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ visualizations/                # Graphiques PNG
â”‚   â”œâ”€â”€ metric_comparison_iou.png
â”‚   â”œâ”€â”€ training_curves_iou.png
â”‚   â””â”€â”€ radar_chart.png
â””â”€â”€ reports/
    â”œâ”€â”€ benchmark_report.html     ğŸ“„ Ouvrir en premier
    â”œâ”€â”€ benchmark_report.md
    â””â”€â”€ benchmark_report.txt
```

## ğŸ“ˆ Monitoring

### TensorBoard (temps rÃ©el)
```bash
# AccÃ¨s depuis Windows
http://localhost:6006
```

MÃ©triques visibles :
- Train/Val Loss
- Train/Val IoU, F1, Precision, Recall
- Learning rate schedule
- Distributions poids

### Logs Docker
```bash
# Suivre les logs
docker-compose logs -f forestgaps
```

## ğŸ”§ ParamÃ¨tres communs

```bash
--experiment-name "nom"      # Nom de l'expÃ©rience (REQUIS pour full)
--epochs N                   # Nombre d'Ã©poques (dÃ©faut: 5 ou 50)
--batch-size N               # Taille batch (dÃ©faut: 4 ou 8)
--models "m1,m2,m3"         # ModÃ¨les Ã  comparer
--thresholds "2,5,10,15"    # Seuils hauteur (mÃ¨tres)
--max-train-tiles N         # Limite tuiles (quick test only)
```

## ğŸ’¡ Exemples pratiques

### Test ultra-rapide (2 min)
```bash
python scripts/benchmark_quick_test.py \
  --experiment-name "ultra_rapide" \
  --epochs 2 \
  --max-train-tiles 10 \
  --models "unet"
```

### Comparer U-Net vs U-Net+FiLM
```bash
python scripts/benchmark_quick_test.py \
  --experiment-name "unet_comparison" \
  --epochs 10 \
  --models "unet,unet_film"
```

### Benchmark production complet
```bash
python scripts/benchmark_full.py \
  --experiment-name "production_$(date +%Y%m%d)" \
  --epochs 50 \
  --batch-size 8
```

## ğŸ› Troubleshooting

| ProblÃ¨me | Solution |
|----------|----------|
| `No module named 'forestgaps'` | `pip install -e .` dans container |
| CUDA out of memory | `--batch-size 2` |
| Container crash | `docker-compose logs forestgaps` |
| TensorBoard vide | Attendre 1-2 min, refresh |
| Pas de GPU visible | `nvidia-smi` dans container |

## ğŸ“š Workflow recommandÃ©

```
1. Test rapide (AUJOURD'HUI)
   â””â”€> Valider setup (5-10 min)

2. Benchmark complet (DEMAIN)
   â””â”€> Lancer pendant la nuit (4-8h)

3. Analyser rÃ©sultats
   â””â”€> Rapport HTML + TensorBoard

4. Ã‰valuation externe
   â””â”€> Tester sur SODEFOR_Mini2

5. Production
   â””â”€> Sauvegarder dans models/production/
```

## ğŸ”— Ressources

- **TensorBoard** : http://localhost:6006
- **Jupyter Lab** : http://localhost:8888
- **Outputs** : `outputs/benchmarks/`
- **Logs** : `logs/benchmarks/`
- **API Docs** : `forestgaps/benchmarking/README.md`

## ğŸ“ Notes importantes

- **Timestamp automatique** : Chaque expÃ©rience a un ID unique
- **Config sauvegardÃ©e** : Permet de reproduire exactement
- **Meilleur modÃ¨le** : SauvegardÃ© automatiquement
- **Rapports multi-formats** : HTML (principal), MD, TXT
- **MÃ©triques par seuil** : 2m, 5m, 10m, 15m analysÃ©s sÃ©parÃ©ment

## âš¡ Commande complÃ¨te (copier-coller)

```bash
# Tout en un : Lancer Docker + Test rapide
cd docker/ && \
docker-compose up -d && \
sleep 5 && \
docker exec -it forestgaps-main bash -c \
  "python scripts/benchmark_quick_test.py --experiment-name test_rapide" && \
echo "âœ… TerminÃ© ! Voir outputs/benchmarks/"
```

## ğŸ†˜ Besoin d'aide ?

1. VÃ©rifier les logs : `docker-compose logs forestgaps`
2. Consulter la section Troubleshooting ci-dessus
3. VÃ©rifier GPU : `docker exec forestgaps-main nvidia-smi`
4. VÃ©rifier donnÃ©es : `docker exec forestgaps-main ls /app/data/*.tif | wc -l`

---

**PrÃªt Ã  lancer ton premier benchmark ? Commence par le test rapide ci-dessus ! ğŸš€**
